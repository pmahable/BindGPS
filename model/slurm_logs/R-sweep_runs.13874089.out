## SLURM PROLOG ###############################################################
##    Job ID : 13874089
##  Job Name : sweep_runs
##  Nodelist : gpu2001
##      CPUs : 1
##  Mem/Node : 32000 MB
## Directory : /oscar/data/larschan/shared_data/BindGPS/model
##   Job Started : Sat Nov  8 17:43:37 EST 2025
###############################################################################
Running model parameter sweep with GAT (NO SVM)
Create sweep with ID: fsw6jcl0
Sweep URL: https://wandb.ai/bind-gps/gps-gat-model-no-svm-parameter-test/sweeps/fsw6jcl0
Created sweep: fsw6jcl0
Starting 24 runs...
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=16.1890 | train_acc=0.5082 | val_loss=9.8084 | val_acc=0.1529
Epoch 001 | loss=7.6928 | train_acc=0.4617 | val_loss=1.1114 | val_acc=0.3228
Epoch 002 | loss=1.6784 | train_acc=0.4730 | val_loss=1.0720 | val_acc=0.6179
Epoch 003 | loss=1.2457 | train_acc=0.5597 | val_loss=1.0591 | val_acc=0.6187
Epoch 004 | loss=1.1458 | train_acc=0.5823 | val_loss=1.0474 | val_acc=0.6187
Epoch 005 | loss=1.0963 | train_acc=0.5922 | val_loss=1.0380 | val_acc=0.6187
Epoch 006 | loss=1.0603 | train_acc=0.5967 | val_loss=1.0309 | val_acc=0.6187
Epoch 007 | loss=1.0298 | train_acc=0.5995 | val_loss=1.0255 | val_acc=0.6187
Epoch 008 | loss=1.0217 | train_acc=0.6020 | val_loss=1.0213 | val_acc=0.6187
Epoch 009 | loss=1.0075 | train_acc=0.6023 | val_loss=1.0182 | val_acc=0.6187
Epoch 010 | loss=0.9998 | train_acc=0.6040 | val_loss=1.0158 | val_acc=0.6187
Epoch 011 | loss=0.9940 | train_acc=0.6041 | val_loss=1.0138 | val_acc=0.6187
Epoch 012 | loss=0.9889 | train_acc=0.6046 | val_loss=1.0122 | val_acc=0.6187
Epoch 013 | loss=0.9829 | train_acc=0.6051 | val_loss=1.0110 | val_acc=0.6187
Epoch 014 | loss=0.9786 | train_acc=0.6057 | val_loss=1.0100 | val_acc=0.6187
Epoch 015 | loss=0.9727 | train_acc=0.6060 | val_loss=1.0091 | val_acc=0.6187
Epoch 016 | loss=0.9729 | train_acc=0.6063 | val_loss=1.0085 | val_acc=0.6187
Epoch 017 | loss=0.9690 | train_acc=0.6064 | val_loss=1.0079 | val_acc=0.6187
Epoch 018 | loss=0.9656 | train_acc=0.6068 | val_loss=1.0075 | val_acc=0.6187
Epoch 019 | loss=0.9639 | train_acc=0.6065 | val_loss=1.0073 | val_acc=0.6187
Epoch 020 | loss=0.9627 | train_acc=0.6076 | val_loss=1.0071 | val_acc=0.6187
Epoch 021 | loss=0.9597 | train_acc=0.6072 | val_loss=1.0070 | val_acc=0.6187
Epoch 022 | loss=0.9590 | train_acc=0.6073 | val_loss=1.0070 | val_acc=0.6187
Epoch 023 | loss=0.9568 | train_acc=0.6070 | val_loss=1.0071 | val_acc=0.6187
Epoch 024 | loss=0.9554 | train_acc=0.6074 | val_loss=1.0072 | val_acc=0.6187
Epoch 025 | loss=0.9549 | train_acc=0.6079 | val_loss=1.0073 | val_acc=0.6187
Epoch 026 | loss=0.9526 | train_acc=0.6078 | val_loss=1.0074 | val_acc=0.6187
Epoch 027 | loss=0.9537 | train_acc=0.6069 | val_loss=1.0076 | val_acc=0.6187
Epoch 028 | loss=0.9499 | train_acc=0.6076 | val_loss=1.0078 | val_acc=0.6187
Epoch 029 | loss=0.9484 | train_acc=0.6080 | val_loss=1.0081 | val_acc=0.6187
Epoch 030 | loss=0.9467 | train_acc=0.6081 | val_loss=1.0083 | val_acc=0.6187
Epoch 031 | loss=0.9479 | train_acc=0.6074 | val_loss=1.0086 | val_acc=0.6187
Epoch 032 | loss=0.9450 | train_acc=0.6083 | val_loss=1.0090 | val_acc=0.6187
Epoch 033 | loss=0.9445 | train_acc=0.6068 | val_loss=1.0093 | val_acc=0.6187
Epoch 034 | loss=0.9429 | train_acc=0.6079 | val_loss=1.0096 | val_acc=0.6187
Epoch 035 | loss=0.9418 | train_acc=0.6076 | val_loss=1.0099 | val_acc=0.6187
Epoch 036 | loss=0.9393 | train_acc=0.6075 | val_loss=1.0102 | val_acc=0.6187
Epoch 037 | loss=0.9411 | train_acc=0.6074 | val_loss=1.0106 | val_acc=0.6187
Epoch 038 | loss=0.9375 | train_acc=0.6081 | val_loss=1.0110 | val_acc=0.6187
Epoch 039 | loss=0.9377 | train_acc=0.6077 | val_loss=1.0112 | val_acc=0.6187
Epoch 040 | loss=0.9345 | train_acc=0.6078 | val_loss=1.0111 | val_acc=0.6187
Epoch 041 | loss=0.9352 | train_acc=0.6082 | val_loss=1.0119 | val_acc=0.6187
Epoch 042 | loss=0.9339 | train_acc=0.6077 | val_loss=1.0120 | val_acc=0.6187
Epoch 043 | loss=0.9324 | train_acc=0.6079 | val_loss=1.0128 | val_acc=0.6187
Epoch 044 | loss=0.9326 | train_acc=0.6076 | val_loss=1.0130 | val_acc=0.6187
Epoch 045 | loss=0.9331 | train_acc=0.6083 | val_loss=1.0135 | val_acc=0.6187
Epoch 046 | loss=0.9301 | train_acc=0.6072 | val_loss=1.0140 | val_acc=0.6187
Epoch 047 | loss=0.9281 | train_acc=0.6078 | val_loss=1.0142 | val_acc=0.6187
Epoch 048 | loss=0.9298 | train_acc=0.6083 | val_loss=1.0150 | val_acc=0.6187
Epoch 049 | loss=0.9282 | train_acc=0.6079 | val_loss=1.0153 | val_acc=0.6187
Final Test Loss: 0.9246 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=6.5157 | train_acc=0.5808 | val_loss=5.2237 | val_acc=0.2007
Epoch 001 | loss=2.5464 | train_acc=0.5016 | val_loss=1.0516 | val_acc=0.5657
Epoch 002 | loss=1.1506 | train_acc=0.5590 | val_loss=1.0285 | val_acc=0.6187
Epoch 003 | loss=1.0651 | train_acc=0.5862 | val_loss=1.0210 | val_acc=0.6187
Epoch 004 | loss=1.0271 | train_acc=0.5945 | val_loss=1.0154 | val_acc=0.6187
Epoch 005 | loss=1.0018 | train_acc=0.5996 | val_loss=1.0127 | val_acc=0.6187
Epoch 006 | loss=0.9901 | train_acc=0.6026 | val_loss=1.0118 | val_acc=0.6187
Epoch 007 | loss=0.9784 | train_acc=0.6042 | val_loss=1.0120 | val_acc=0.6187
Epoch 008 | loss=0.9711 | train_acc=0.6055 | val_loss=1.0127 | val_acc=0.6187
Epoch 009 | loss=0.9684 | train_acc=0.6054 | val_loss=1.0138 | val_acc=0.6187
Epoch 010 | loss=0.9631 | train_acc=0.6059 | val_loss=1.0151 | val_acc=0.6187
Epoch 011 | loss=0.9590 | train_acc=0.6062 | val_loss=1.0163 | val_acc=0.6187
Epoch 012 | loss=0.9561 | train_acc=0.6073 | val_loss=1.0175 | val_acc=0.6187
Epoch 013 | loss=0.9558 | train_acc=0.6065 | val_loss=1.0185 | val_acc=0.6187
Epoch 014 | loss=0.9545 | train_acc=0.6071 | val_loss=1.0196 | val_acc=0.6187
Epoch 015 | loss=0.9534 | train_acc=0.6075 | val_loss=1.0205 | val_acc=0.6187
Epoch 016 | loss=0.9521 | train_acc=0.6078 | val_loss=1.0212 | val_acc=0.6187
Epoch 017 | loss=0.9519 | train_acc=0.6079 | val_loss=1.0219 | val_acc=0.6187
Epoch 018 | loss=0.9516 | train_acc=0.6074 | val_loss=1.0225 | val_acc=0.6187
Epoch 019 | loss=0.9512 | train_acc=0.6076 | val_loss=1.0230 | val_acc=0.6187
Epoch 020 | loss=0.9502 | train_acc=0.6084 | val_loss=1.0235 | val_acc=0.6187
Epoch 021 | loss=0.9498 | train_acc=0.6083 | val_loss=1.0240 | val_acc=0.6187
Epoch 022 | loss=0.9499 | train_acc=0.6080 | val_loss=1.0244 | val_acc=0.6187
Epoch 023 | loss=0.9495 | train_acc=0.6075 | val_loss=1.0248 | val_acc=0.6187
Epoch 024 | loss=0.9495 | train_acc=0.6084 | val_loss=1.0251 | val_acc=0.6187
Epoch 025 | loss=0.9495 | train_acc=0.6076 | val_loss=1.0253 | val_acc=0.6187
Epoch 026 | loss=0.9494 | train_acc=0.6077 | val_loss=1.0256 | val_acc=0.6187
Epoch 027 | loss=0.9486 | train_acc=0.6084 | val_loss=1.0258 | val_acc=0.6187
Epoch 028 | loss=0.9483 | train_acc=0.6087 | val_loss=1.0260 | val_acc=0.6187
Epoch 029 | loss=0.9486 | train_acc=0.6082 | val_loss=1.0262 | val_acc=0.6187
Epoch 030 | loss=0.9490 | train_acc=0.6074 | val_loss=1.0263 | val_acc=0.6187
Epoch 031 | loss=0.9482 | train_acc=0.6086 | val_loss=1.0265 | val_acc=0.6187
Epoch 032 | loss=0.9491 | train_acc=0.6080 | val_loss=1.0266 | val_acc=0.6187
Epoch 033 | loss=0.9482 | train_acc=0.6082 | val_loss=1.0266 | val_acc=0.6187
Epoch 034 | loss=0.9478 | train_acc=0.6081 | val_loss=1.0268 | val_acc=0.6187
Epoch 035 | loss=0.9488 | train_acc=0.6073 | val_loss=1.0268 | val_acc=0.6187
Epoch 036 | loss=0.9488 | train_acc=0.6078 | val_loss=1.0268 | val_acc=0.6187
Epoch 037 | loss=0.9485 | train_acc=0.6080 | val_loss=1.0268 | val_acc=0.6187
Epoch 038 | loss=0.9477 | train_acc=0.6082 | val_loss=1.0268 | val_acc=0.6187
Epoch 039 | loss=0.9478 | train_acc=0.6085 | val_loss=1.0269 | val_acc=0.6187
Epoch 040 | loss=0.9485 | train_acc=0.6082 | val_loss=1.0268 | val_acc=0.6187
Epoch 041 | loss=0.9476 | train_acc=0.6079 | val_loss=1.0268 | val_acc=0.6187
Epoch 042 | loss=0.9481 | train_acc=0.6079 | val_loss=1.0267 | val_acc=0.6187
Epoch 043 | loss=0.9487 | train_acc=0.6079 | val_loss=1.0266 | val_acc=0.6187
Epoch 044 | loss=0.9481 | train_acc=0.6078 | val_loss=1.0266 | val_acc=0.6187
Epoch 045 | loss=0.9478 | train_acc=0.6085 | val_loss=1.0266 | val_acc=0.6187
Epoch 046 | loss=0.9495 | train_acc=0.6078 | val_loss=1.0265 | val_acc=0.6187
Epoch 047 | loss=0.9468 | train_acc=0.6082 | val_loss=1.0266 | val_acc=0.6187
Epoch 048 | loss=0.9476 | train_acc=0.6082 | val_loss=1.0266 | val_acc=0.6187
Epoch 049 | loss=0.9488 | train_acc=0.6082 | val_loss=1.0264 | val_acc=0.6187
Final Test Loss: 0.9190 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=22.0444 | train_acc=0.6138 | val_loss=36.9491 | val_acc=0.1851
Epoch 001 | loss=17.7410 | train_acc=0.5108 | val_loss=1.2943 | val_acc=0.3906
Epoch 002 | loss=4.2365 | train_acc=0.5055 | val_loss=1.1067 | val_acc=0.5931
Epoch 003 | loss=1.8013 | train_acc=0.5076 | val_loss=1.0607 | val_acc=0.6201
Epoch 004 | loss=1.3346 | train_acc=0.5475 | val_loss=1.0414 | val_acc=0.6187
Epoch 005 | loss=1.1888 | train_acc=0.5709 | val_loss=1.0325 | val_acc=0.6187
Epoch 006 | loss=1.1156 | train_acc=0.5857 | val_loss=1.0253 | val_acc=0.6187
Epoch 007 | loss=1.0714 | train_acc=0.5911 | val_loss=1.0195 | val_acc=0.6187
Epoch 008 | loss=1.0465 | train_acc=0.5955 | val_loss=1.0158 | val_acc=0.6187
Epoch 009 | loss=1.0229 | train_acc=0.5992 | val_loss=1.0117 | val_acc=0.6187
Epoch 010 | loss=1.0128 | train_acc=0.6003 | val_loss=1.0075 | val_acc=0.6187
Epoch 011 | loss=0.9982 | train_acc=0.6028 | val_loss=1.0042 | val_acc=0.6187
Epoch 012 | loss=0.9879 | train_acc=0.6032 | val_loss=1.0036 | val_acc=0.6187
Epoch 013 | loss=0.9885 | train_acc=0.6040 | val_loss=1.0029 | val_acc=0.6187
Epoch 014 | loss=0.9788 | train_acc=0.6050 | val_loss=1.0027 | val_acc=0.6187
Epoch 015 | loss=0.9703 | train_acc=0.6053 | val_loss=1.0025 | val_acc=0.6187
Epoch 016 | loss=0.9688 | train_acc=0.6056 | val_loss=1.0022 | val_acc=0.6187
Epoch 017 | loss=0.9660 | train_acc=0.6061 | val_loss=1.0020 | val_acc=0.6187
Epoch 018 | loss=0.9604 | train_acc=0.6067 | val_loss=1.0004 | val_acc=0.6187
Epoch 019 | loss=0.9609 | train_acc=0.6069 | val_loss=1.0002 | val_acc=0.6187
Epoch 020 | loss=0.9581 | train_acc=0.6065 | val_loss=0.9904 | val_acc=0.6187
Epoch 021 | loss=0.9558 | train_acc=0.6067 | val_loss=0.9826 | val_acc=0.6187
Epoch 022 | loss=0.9541 | train_acc=0.6076 | val_loss=0.9810 | val_acc=0.6187
Epoch 023 | loss=0.9512 | train_acc=0.6071 | val_loss=0.9805 | val_acc=0.6187
Epoch 024 | loss=0.9500 | train_acc=0.6069 | val_loss=0.9822 | val_acc=0.6187
Epoch 025 | loss=0.9482 | train_acc=0.6076 | val_loss=0.9818 | val_acc=0.6187
Epoch 026 | loss=0.9475 | train_acc=0.6074 | val_loss=0.9856 | val_acc=0.6187
Epoch 027 | loss=0.9451 | train_acc=0.6072 | val_loss=0.9857 | val_acc=0.6187
Epoch 028 | loss=0.9445 | train_acc=0.6073 | val_loss=0.9828 | val_acc=0.6187
Epoch 029 | loss=0.9463 | train_acc=0.6074 | val_loss=0.9829 | val_acc=0.6187
Epoch 030 | loss=0.9421 | train_acc=0.6077 | val_loss=0.9855 | val_acc=0.6187
Epoch 031 | loss=0.9411 | train_acc=0.6078 | val_loss=0.9851 | val_acc=0.6187
Epoch 032 | loss=0.9410 | train_acc=0.6078 | val_loss=0.9847 | val_acc=0.6187
Epoch 033 | loss=0.9399 | train_acc=0.6078 | val_loss=1.0022 | val_acc=0.6187
Epoch 034 | loss=0.9374 | train_acc=0.6078 | val_loss=1.0020 | val_acc=0.6187
Epoch 035 | loss=0.9370 | train_acc=0.6080 | val_loss=1.0009 | val_acc=0.6187
Epoch 036 | loss=0.9339 | train_acc=0.6081 | val_loss=0.9987 | val_acc=0.6187
Epoch 037 | loss=0.9331 | train_acc=0.6081 | val_loss=0.9985 | val_acc=0.6187
Epoch 038 | loss=0.9343 | train_acc=0.6081 | val_loss=0.9972 | val_acc=0.6187
Epoch 039 | loss=0.9330 | train_acc=0.6077 | val_loss=0.9985 | val_acc=0.6187
Epoch 040 | loss=0.9327 | train_acc=0.6075 | val_loss=0.9975 | val_acc=0.6187
Epoch 041 | loss=0.9302 | train_acc=0.6076 | val_loss=0.9962 | val_acc=0.6187
Epoch 042 | loss=0.9284 | train_acc=0.6080 | val_loss=0.9985 | val_acc=0.6187
Epoch 043 | loss=0.9278 | train_acc=0.6072 | val_loss=0.9947 | val_acc=0.6187
Epoch 044 | loss=0.9306 | train_acc=0.6077 | val_loss=0.9859 | val_acc=0.6187
Epoch 045 | loss=0.9275 | train_acc=0.6071 | val_loss=0.9869 | val_acc=0.6187
Epoch 046 | loss=0.9269 | train_acc=0.6075 | val_loss=0.9884 | val_acc=0.6187
Epoch 047 | loss=0.9271 | train_acc=0.6074 | val_loss=0.9907 | val_acc=0.6187
Epoch 048 | loss=0.9264 | train_acc=0.6071 | val_loss=0.9911 | val_acc=0.6187
Epoch 049 | loss=0.9271 | train_acc=0.6068 | val_loss=0.9948 | val_acc=0.6187
Final Test Loss: 0.8886 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=9.6069 | train_acc=0.5760 | val_loss=4.3987 | val_acc=0.1870
Epoch 001 | loss=3.0436 | train_acc=0.4707 | val_loss=1.0742 | val_acc=0.5191
Epoch 002 | loss=1.3235 | train_acc=0.5127 | val_loss=1.0595 | val_acc=0.6172
Epoch 003 | loss=1.1566 | train_acc=0.5407 | val_loss=1.0462 | val_acc=0.6187
Epoch 004 | loss=1.0885 | train_acc=0.5656 | val_loss=1.0367 | val_acc=0.6187
Epoch 005 | loss=1.0475 | train_acc=0.5867 | val_loss=1.0265 | val_acc=0.6187
Epoch 006 | loss=1.0249 | train_acc=0.5969 | val_loss=1.0176 | val_acc=0.6187
Epoch 007 | loss=1.0090 | train_acc=0.6006 | val_loss=1.0105 | val_acc=0.6187
Epoch 008 | loss=0.9977 | train_acc=0.6032 | val_loss=1.0050 | val_acc=0.6187
Epoch 009 | loss=0.9860 | train_acc=0.6048 | val_loss=0.9979 | val_acc=0.6187
Epoch 010 | loss=0.9810 | train_acc=0.6051 | val_loss=0.9928 | val_acc=0.6187
Epoch 011 | loss=0.9752 | train_acc=0.6064 | val_loss=0.9872 | val_acc=0.6187
Epoch 012 | loss=0.9680 | train_acc=0.6066 | val_loss=0.9818 | val_acc=0.6187
Epoch 013 | loss=0.9623 | train_acc=0.6073 | val_loss=0.9736 | val_acc=0.6187
Epoch 014 | loss=0.9605 | train_acc=0.6075 | val_loss=0.9676 | val_acc=0.6187
Epoch 015 | loss=0.9547 | train_acc=0.6072 | val_loss=0.9562 | val_acc=0.6187
Epoch 016 | loss=0.9516 | train_acc=0.6071 | val_loss=0.9519 | val_acc=0.6187
Epoch 017 | loss=0.9482 | train_acc=0.6082 | val_loss=0.9455 | val_acc=0.6187
Epoch 018 | loss=0.9481 | train_acc=0.6078 | val_loss=0.9405 | val_acc=0.6187
Epoch 019 | loss=0.9421 | train_acc=0.6083 | val_loss=0.9359 | val_acc=0.6187
Epoch 020 | loss=0.9407 | train_acc=0.6076 | val_loss=0.9316 | val_acc=0.6187
Epoch 021 | loss=0.9337 | train_acc=0.6073 | val_loss=0.9238 | val_acc=0.6187
Epoch 022 | loss=0.9330 | train_acc=0.6075 | val_loss=0.9206 | val_acc=0.6187
Epoch 023 | loss=0.9333 | train_acc=0.6079 | val_loss=0.9238 | val_acc=0.6187
Epoch 024 | loss=0.9309 | train_acc=0.6079 | val_loss=0.9271 | val_acc=0.6187
Epoch 025 | loss=0.9295 | train_acc=0.6081 | val_loss=0.9253 | val_acc=0.6187
Epoch 026 | loss=0.9318 | train_acc=0.6081 | val_loss=0.9213 | val_acc=0.6187
Epoch 027 | loss=0.9249 | train_acc=0.6076 | val_loss=0.9173 | val_acc=0.6187
Epoch 028 | loss=0.9251 | train_acc=0.6081 | val_loss=0.9198 | val_acc=0.6187
Epoch 029 | loss=0.9232 | train_acc=0.6081 | val_loss=0.9248 | val_acc=0.6187
Epoch 030 | loss=0.9224 | train_acc=0.6079 | val_loss=0.9208 | val_acc=0.6187
Epoch 031 | loss=0.9230 | train_acc=0.6081 | val_loss=0.9190 | val_acc=0.6187
Epoch 032 | loss=0.9231 | train_acc=0.6075 | val_loss=0.9135 | val_acc=0.6187
Epoch 033 | loss=0.9204 | train_acc=0.6077 | val_loss=0.9137 | val_acc=0.6187
Epoch 034 | loss=0.9162 | train_acc=0.6083 | val_loss=0.9179 | val_acc=0.6187
Epoch 035 | loss=0.9191 | train_acc=0.6079 | val_loss=0.9163 | val_acc=0.6187
Epoch 036 | loss=0.9125 | train_acc=0.6085 | val_loss=0.9186 | val_acc=0.6187
Epoch 037 | loss=0.9157 | train_acc=0.6077 | val_loss=0.9234 | val_acc=0.6187
Epoch 038 | loss=0.9161 | train_acc=0.6074 | val_loss=0.9151 | val_acc=0.6187
Epoch 039 | loss=0.9148 | train_acc=0.6083 | val_loss=0.9215 | val_acc=0.6187
Epoch 040 | loss=0.9139 | train_acc=0.6072 | val_loss=0.9191 | val_acc=0.6187
Epoch 041 | loss=0.9171 | train_acc=0.6082 | val_loss=0.9238 | val_acc=0.6187
Epoch 042 | loss=0.9118 | train_acc=0.6084 | val_loss=0.9319 | val_acc=0.6187
Epoch 043 | loss=0.9157 | train_acc=0.6074 | val_loss=0.9169 | val_acc=0.6187
Epoch 044 | loss=0.9124 | train_acc=0.6086 | val_loss=0.9230 | val_acc=0.6187
Epoch 045 | loss=0.9121 | train_acc=0.6081 | val_loss=0.9183 | val_acc=0.6187
Epoch 046 | loss=0.9120 | train_acc=0.6082 | val_loss=0.9162 | val_acc=0.6187
Epoch 047 | loss=0.9093 | train_acc=0.6083 | val_loss=0.9172 | val_acc=0.6187
Epoch 048 | loss=0.9111 | train_acc=0.6076 | val_loss=0.9183 | val_acc=0.6187
Epoch 049 | loss=0.9105 | train_acc=0.6075 | val_loss=0.9188 | val_acc=0.6187
Final Test Loss: 0.8262 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=28.1986 | train_acc=0.5890 | val_loss=83.7159 | val_acc=0.1844
Epoch 001 | loss=21.3524 | train_acc=0.5258 | val_loss=5.5282 | val_acc=0.1899
Epoch 002 | loss=7.5035 | train_acc=0.5163 | val_loss=2.1463 | val_acc=0.2940
Epoch 003 | loss=2.8570 | train_acc=0.5190 | val_loss=1.1429 | val_acc=0.5091
Epoch 004 | loss=1.7642 | train_acc=0.5208 | val_loss=1.0619 | val_acc=0.6039
Epoch 005 | loss=1.4075 | train_acc=0.5480 | val_loss=1.0447 | val_acc=0.6172
Epoch 006 | loss=1.2740 | train_acc=0.5638 | val_loss=1.0374 | val_acc=0.6172
Epoch 007 | loss=1.1896 | train_acc=0.5730 | val_loss=1.0308 | val_acc=0.6183
Epoch 008 | loss=1.1183 | train_acc=0.5840 | val_loss=1.0259 | val_acc=0.6187
Epoch 009 | loss=1.0873 | train_acc=0.5870 | val_loss=1.0221 | val_acc=0.6187
Epoch 010 | loss=1.0663 | train_acc=0.5916 | val_loss=1.0193 | val_acc=0.6187
Epoch 011 | loss=1.0416 | train_acc=0.5957 | val_loss=1.0167 | val_acc=0.6187
Epoch 012 | loss=1.0331 | train_acc=0.5970 | val_loss=1.0147 | val_acc=0.6187
Epoch 013 | loss=1.0192 | train_acc=0.5984 | val_loss=1.0129 | val_acc=0.6187
Epoch 014 | loss=1.0051 | train_acc=0.5999 | val_loss=1.0115 | val_acc=0.6187
Epoch 015 | loss=0.9974 | train_acc=0.6018 | val_loss=1.0106 | val_acc=0.6187
Epoch 016 | loss=0.9938 | train_acc=0.6014 | val_loss=1.0097 | val_acc=0.6187
Epoch 017 | loss=0.9912 | train_acc=0.6024 | val_loss=1.0090 | val_acc=0.6187
Epoch 018 | loss=0.9825 | train_acc=0.6030 | val_loss=1.0084 | val_acc=0.6187
Epoch 019 | loss=0.9799 | train_acc=0.6039 | val_loss=1.0080 | val_acc=0.6187
Epoch 020 | loss=0.9786 | train_acc=0.6042 | val_loss=1.0077 | val_acc=0.6187
Epoch 021 | loss=0.9731 | train_acc=0.6039 | val_loss=1.0076 | val_acc=0.6187
Epoch 022 | loss=0.9691 | train_acc=0.6056 | val_loss=1.0074 | val_acc=0.6187
Epoch 023 | loss=0.9677 | train_acc=0.6056 | val_loss=1.0075 | val_acc=0.6187
Epoch 024 | loss=0.9688 | train_acc=0.6056 | val_loss=1.0076 | val_acc=0.6187
Epoch 025 | loss=0.9635 | train_acc=0.6055 | val_loss=1.0076 | val_acc=0.6187
Epoch 026 | loss=0.9617 | train_acc=0.6064 | val_loss=1.0077 | val_acc=0.6187
Epoch 027 | loss=0.9621 | train_acc=0.6060 | val_loss=1.0078 | val_acc=0.6187
Epoch 028 | loss=0.9590 | train_acc=0.6063 | val_loss=1.0080 | val_acc=0.6187
Epoch 029 | loss=0.9589 | train_acc=0.6071 | val_loss=1.0082 | val_acc=0.6187
Epoch 030 | loss=0.9601 | train_acc=0.6066 | val_loss=1.0086 | val_acc=0.6187
Epoch 031 | loss=0.9563 | train_acc=0.6063 | val_loss=1.0089 | val_acc=0.6187
Epoch 032 | loss=0.9528 | train_acc=0.6073 | val_loss=1.0092 | val_acc=0.6187
Epoch 033 | loss=0.9540 | train_acc=0.6072 | val_loss=1.0096 | val_acc=0.6187
Epoch 034 | loss=0.9524 | train_acc=0.6071 | val_loss=1.0099 | val_acc=0.6187
Epoch 035 | loss=0.9532 | train_acc=0.6070 | val_loss=1.0103 | val_acc=0.6187
Epoch 036 | loss=0.9519 | train_acc=0.6074 | val_loss=1.0107 | val_acc=0.6187
Epoch 037 | loss=0.9510 | train_acc=0.6074 | val_loss=1.0111 | val_acc=0.6187
Epoch 038 | loss=0.9502 | train_acc=0.6069 | val_loss=1.0115 | val_acc=0.6187
Epoch 039 | loss=0.9509 | train_acc=0.6073 | val_loss=1.0122 | val_acc=0.6187
Epoch 040 | loss=0.9482 | train_acc=0.6078 | val_loss=1.0123 | val_acc=0.6187
Epoch 041 | loss=0.9495 | train_acc=0.6075 | val_loss=1.0128 | val_acc=0.6187
Epoch 042 | loss=0.9472 | train_acc=0.6076 | val_loss=1.0133 | val_acc=0.6187
Epoch 043 | loss=0.9495 | train_acc=0.6071 | val_loss=1.0138 | val_acc=0.6187
Epoch 044 | loss=0.9453 | train_acc=0.6081 | val_loss=1.0140 | val_acc=0.6187
Epoch 045 | loss=0.9443 | train_acc=0.6077 | val_loss=1.0146 | val_acc=0.6187
Epoch 046 | loss=0.9441 | train_acc=0.6080 | val_loss=1.0148 | val_acc=0.6187
Epoch 047 | loss=0.9433 | train_acc=0.6080 | val_loss=1.0152 | val_acc=0.6187
Epoch 048 | loss=0.9445 | train_acc=0.6078 | val_loss=1.0156 | val_acc=0.6187
Epoch 049 | loss=0.9430 | train_acc=0.6082 | val_loss=1.0161 | val_acc=0.6187
Final Test Loss: 0.9223 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=8.0045 | train_acc=0.6265 | val_loss=1.9012 | val_acc=0.2318
Epoch 001 | loss=2.1279 | train_acc=0.5265 | val_loss=1.0585 | val_acc=0.6187
Epoch 002 | loss=1.0393 | train_acc=0.5875 | val_loss=1.0440 | val_acc=0.6187
Epoch 003 | loss=1.0133 | train_acc=0.6010 | val_loss=1.0338 | val_acc=0.6187
Epoch 004 | loss=0.9977 | train_acc=0.6030 | val_loss=1.0252 | val_acc=0.6187
Epoch 005 | loss=0.9854 | train_acc=0.6051 | val_loss=1.0186 | val_acc=0.6187
Epoch 006 | loss=0.9795 | train_acc=0.6057 | val_loss=1.0143 | val_acc=0.6187
Epoch 007 | loss=0.9732 | train_acc=0.6067 | val_loss=1.0112 | val_acc=0.6187
Epoch 008 | loss=0.9689 | train_acc=0.6074 | val_loss=1.0090 | val_acc=0.6187
Epoch 009 | loss=0.9662 | train_acc=0.6071 | val_loss=1.0092 | val_acc=0.6187
Epoch 010 | loss=0.9639 | train_acc=0.6073 | val_loss=1.0050 | val_acc=0.6187
Epoch 011 | loss=0.9604 | train_acc=0.6078 | val_loss=1.0043 | val_acc=0.6187
Epoch 012 | loss=0.9589 | train_acc=0.6078 | val_loss=1.0033 | val_acc=0.6187
Epoch 013 | loss=0.9562 | train_acc=0.6076 | val_loss=1.0031 | val_acc=0.6187
Epoch 014 | loss=0.9554 | train_acc=0.6075 | val_loss=1.0026 | val_acc=0.6187
Epoch 015 | loss=0.9542 | train_acc=0.6081 | val_loss=1.0019 | val_acc=0.6187
Epoch 016 | loss=0.9514 | train_acc=0.6080 | val_loss=1.0026 | val_acc=0.6187
Epoch 017 | loss=0.9505 | train_acc=0.6080 | val_loss=1.0028 | val_acc=0.6187
Epoch 018 | loss=0.9500 | train_acc=0.6081 | val_loss=1.0034 | val_acc=0.6187
Epoch 019 | loss=0.9500 | train_acc=0.6081 | val_loss=1.0040 | val_acc=0.6187
Epoch 020 | loss=0.9481 | train_acc=0.6084 | val_loss=1.0050 | val_acc=0.6187
Epoch 021 | loss=0.9474 | train_acc=0.6081 | val_loss=1.0047 | val_acc=0.6187
Epoch 022 | loss=0.9463 | train_acc=0.6083 | val_loss=1.0053 | val_acc=0.6187
Epoch 023 | loss=0.9459 | train_acc=0.6080 | val_loss=1.0062 | val_acc=0.6187
Epoch 024 | loss=0.9456 | train_acc=0.6085 | val_loss=1.0069 | val_acc=0.6187
Epoch 025 | loss=0.9447 | train_acc=0.6083 | val_loss=1.0071 | val_acc=0.6187
Epoch 026 | loss=0.9445 | train_acc=0.6077 | val_loss=1.0073 | val_acc=0.6187
Epoch 027 | loss=0.9436 | train_acc=0.6086 | val_loss=1.0076 | val_acc=0.6187
Epoch 028 | loss=0.9431 | train_acc=0.6083 | val_loss=1.0086 | val_acc=0.6187
Epoch 029 | loss=0.9429 | train_acc=0.6080 | val_loss=1.0088 | val_acc=0.6187
Epoch 030 | loss=0.9417 | train_acc=0.6086 | val_loss=1.0085 | val_acc=0.6187
Epoch 031 | loss=0.9396 | train_acc=0.6085 | val_loss=1.0083 | val_acc=0.6187
Epoch 032 | loss=0.9556 | train_acc=0.6083 | val_loss=1.0094 | val_acc=0.6187
Epoch 033 | loss=0.9419 | train_acc=0.6083 | val_loss=1.0102 | val_acc=0.6187
Epoch 034 | loss=0.9418 | train_acc=0.6079 | val_loss=1.0112 | val_acc=0.6187
Epoch 035 | loss=0.9418 | train_acc=0.6083 | val_loss=1.0121 | val_acc=0.6187
Epoch 036 | loss=0.9481 | train_acc=0.6082 | val_loss=1.0118 | val_acc=0.6187
Epoch 037 | loss=0.9412 | train_acc=0.6080 | val_loss=1.0129 | val_acc=0.6187
Epoch 038 | loss=0.9411 | train_acc=0.6082 | val_loss=1.0024 | val_acc=0.6179
Epoch 039 | loss=0.9436 | train_acc=0.6082 | val_loss=1.0140 | val_acc=0.6187
Epoch 040 | loss=0.9503 | train_acc=0.6081 | val_loss=1.0150 | val_acc=0.6187
Epoch 041 | loss=0.9512 | train_acc=0.6084 | val_loss=1.0064 | val_acc=0.6172
Epoch 042 | loss=0.9624 | train_acc=0.6082 | val_loss=1.0142 | val_acc=0.6187
Epoch 043 | loss=0.9419 | train_acc=0.6072 | val_loss=1.0148 | val_acc=0.6187
Epoch 044 | loss=0.9404 | train_acc=0.6078 | val_loss=1.0154 | val_acc=0.6187
Epoch 045 | loss=0.9397 | train_acc=0.6081 | val_loss=1.0162 | val_acc=0.6187
Epoch 046 | loss=0.9397 | train_acc=0.6083 | val_loss=1.0167 | val_acc=0.6187
Epoch 047 | loss=0.9394 | train_acc=0.6077 | val_loss=1.0168 | val_acc=0.6187
Epoch 048 | loss=0.9390 | train_acc=0.6084 | val_loss=1.0172 | val_acc=0.6187
Epoch 049 | loss=0.9388 | train_acc=0.6083 | val_loss=1.0176 | val_acc=0.6187
Final Test Loss: 0.9152 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=9.4974 | train_acc=0.5321 | val_loss=5.7936 | val_acc=0.1655
Epoch 001 | loss=2.6383 | train_acc=0.4359 | val_loss=1.0989 | val_acc=0.2529
Epoch 002 | loss=1.1629 | train_acc=0.4951 | val_loss=1.0735 | val_acc=0.6187
Epoch 003 | loss=1.0952 | train_acc=0.5979 | val_loss=1.0549 | val_acc=0.6187
Epoch 004 | loss=1.0541 | train_acc=0.6023 | val_loss=1.0414 | val_acc=0.6187
Epoch 005 | loss=1.0261 | train_acc=0.6046 | val_loss=1.0316 | val_acc=0.6187
Epoch 006 | loss=1.0130 | train_acc=0.6055 | val_loss=1.0246 | val_acc=0.6187
Epoch 007 | loss=1.0005 | train_acc=0.6069 | val_loss=1.0195 | val_acc=0.6187
Epoch 008 | loss=0.9922 | train_acc=0.6067 | val_loss=1.0156 | val_acc=0.6187
Epoch 009 | loss=0.9845 | train_acc=0.6071 | val_loss=1.0126 | val_acc=0.6187
Epoch 010 | loss=0.9797 | train_acc=0.6072 | val_loss=1.0105 | val_acc=0.6187
Epoch 011 | loss=0.9754 | train_acc=0.6075 | val_loss=1.0090 | val_acc=0.6187
Epoch 012 | loss=0.9702 | train_acc=0.6077 | val_loss=1.0080 | val_acc=0.6187
Epoch 013 | loss=0.9673 | train_acc=0.6077 | val_loss=1.0074 | val_acc=0.6187
Epoch 014 | loss=0.9643 | train_acc=0.6077 | val_loss=1.0069 | val_acc=0.6187
Epoch 015 | loss=0.9614 | train_acc=0.6079 | val_loss=1.0066 | val_acc=0.6187
Epoch 016 | loss=0.9584 | train_acc=0.6078 | val_loss=1.0067 | val_acc=0.6187
Epoch 017 | loss=0.9559 | train_acc=0.6080 | val_loss=1.0069 | val_acc=0.6187
Epoch 018 | loss=0.9539 | train_acc=0.6084 | val_loss=1.0071 | val_acc=0.6187
Epoch 019 | loss=0.9532 | train_acc=0.6083 | val_loss=1.0075 | val_acc=0.6187
Epoch 020 | loss=0.9523 | train_acc=0.6083 | val_loss=1.0079 | val_acc=0.6187
Epoch 021 | loss=0.9517 | train_acc=0.6086 | val_loss=1.0086 | val_acc=0.6187
Epoch 022 | loss=0.9494 | train_acc=0.6078 | val_loss=1.0091 | val_acc=0.6187
Epoch 023 | loss=0.9482 | train_acc=0.6082 | val_loss=1.0097 | val_acc=0.6187
Epoch 024 | loss=0.9471 | train_acc=0.6080 | val_loss=1.0102 | val_acc=0.6187
Epoch 025 | loss=0.9465 | train_acc=0.6081 | val_loss=1.0109 | val_acc=0.6187
Epoch 026 | loss=0.9451 | train_acc=0.6078 | val_loss=1.0116 | val_acc=0.6187
Epoch 027 | loss=0.9446 | train_acc=0.6082 | val_loss=1.0121 | val_acc=0.6187
Epoch 028 | loss=0.9459 | train_acc=0.6080 | val_loss=1.0127 | val_acc=0.6187
Epoch 029 | loss=0.9449 | train_acc=0.6076 | val_loss=1.0133 | val_acc=0.6187
Epoch 030 | loss=0.9438 | train_acc=0.6080 | val_loss=1.0139 | val_acc=0.6187
Epoch 031 | loss=0.9423 | train_acc=0.6080 | val_loss=1.0146 | val_acc=0.6187
Epoch 032 | loss=0.9428 | train_acc=0.6082 | val_loss=1.0152 | val_acc=0.6187
Epoch 033 | loss=0.9422 | train_acc=0.6086 | val_loss=1.0159 | val_acc=0.6187
Epoch 034 | loss=0.9408 | train_acc=0.6083 | val_loss=1.0164 | val_acc=0.6187
Epoch 035 | loss=0.9409 | train_acc=0.6084 | val_loss=1.0170 | val_acc=0.6187
Epoch 036 | loss=0.9406 | train_acc=0.6079 | val_loss=1.0175 | val_acc=0.6187
Epoch 037 | loss=0.9399 | train_acc=0.6085 | val_loss=1.0179 | val_acc=0.6187
Epoch 038 | loss=0.9399 | train_acc=0.6083 | val_loss=1.0184 | val_acc=0.6187
Epoch 039 | loss=0.9397 | train_acc=0.6080 | val_loss=1.0190 | val_acc=0.6187
Epoch 040 | loss=0.9399 | train_acc=0.6082 | val_loss=1.0194 | val_acc=0.6187
Epoch 041 | loss=0.9392 | train_acc=0.6083 | val_loss=1.0199 | val_acc=0.6187
Epoch 042 | loss=0.9401 | train_acc=0.6077 | val_loss=1.0203 | val_acc=0.6187
Epoch 043 | loss=0.9387 | train_acc=0.6086 | val_loss=1.0210 | val_acc=0.6187
Epoch 044 | loss=0.9388 | train_acc=0.6078 | val_loss=1.0217 | val_acc=0.6187
Epoch 045 | loss=0.9384 | train_acc=0.6081 | val_loss=1.0220 | val_acc=0.6187
Epoch 046 | loss=0.9378 | train_acc=0.6082 | val_loss=1.0224 | val_acc=0.6187
Epoch 047 | loss=0.9381 | train_acc=0.6077 | val_loss=1.0218 | val_acc=0.6187
Epoch 048 | loss=0.9382 | train_acc=0.6080 | val_loss=1.0225 | val_acc=0.6187
Epoch 049 | loss=0.9375 | train_acc=0.6079 | val_loss=1.0228 | val_acc=0.6187
Final Test Loss: 0.9178 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=3.0070 | train_acc=0.5742 | val_loss=1.0905 | val_acc=0.5887
Epoch 001 | loss=1.1336 | train_acc=0.5831 | val_loss=1.0295 | val_acc=0.6187
Epoch 002 | loss=0.9965 | train_acc=0.6054 | val_loss=1.0191 | val_acc=0.6187
Epoch 003 | loss=0.9783 | train_acc=0.6068 | val_loss=1.0148 | val_acc=0.6187
Epoch 004 | loss=0.9674 | train_acc=0.6079 | val_loss=1.0142 | val_acc=0.6187
Epoch 005 | loss=0.9600 | train_acc=0.6083 | val_loss=1.0153 | val_acc=0.6187
Epoch 006 | loss=0.9571 | train_acc=0.6076 | val_loss=1.0170 | val_acc=0.6187
Epoch 007 | loss=0.9542 | train_acc=0.6077 | val_loss=1.0188 | val_acc=0.6187
Epoch 008 | loss=0.9523 | train_acc=0.6082 | val_loss=1.0203 | val_acc=0.6187
Epoch 009 | loss=0.9520 | train_acc=0.6082 | val_loss=1.0216 | val_acc=0.6187
Epoch 010 | loss=0.9508 | train_acc=0.6082 | val_loss=1.0226 | val_acc=0.6187
Epoch 011 | loss=0.9512 | train_acc=0.6080 | val_loss=1.0232 | val_acc=0.6187
Epoch 012 | loss=0.9508 | train_acc=0.6080 | val_loss=1.0238 | val_acc=0.6187
Epoch 013 | loss=0.9502 | train_acc=0.6079 | val_loss=1.0243 | val_acc=0.6187
Epoch 014 | loss=0.9505 | train_acc=0.6083 | val_loss=1.0246 | val_acc=0.6187
Epoch 015 | loss=0.9501 | train_acc=0.6082 | val_loss=1.0249 | val_acc=0.6187
Epoch 016 | loss=0.9498 | train_acc=0.6080 | val_loss=1.0253 | val_acc=0.6187
Epoch 017 | loss=0.9496 | train_acc=0.6085 | val_loss=1.0254 | val_acc=0.6187
Epoch 018 | loss=0.9499 | train_acc=0.6078 | val_loss=1.0256 | val_acc=0.6187
Epoch 019 | loss=0.9494 | train_acc=0.6087 | val_loss=1.0258 | val_acc=0.6187
Epoch 020 | loss=0.9500 | train_acc=0.6083 | val_loss=1.0258 | val_acc=0.6187
Epoch 021 | loss=0.9509 | train_acc=0.6085 | val_loss=1.0265 | val_acc=0.6187
Epoch 022 | loss=0.9500 | train_acc=0.6082 | val_loss=1.0264 | val_acc=0.6187
Epoch 023 | loss=0.9495 | train_acc=0.6089 | val_loss=1.0263 | val_acc=0.6187
Epoch 024 | loss=0.9493 | train_acc=0.6084 | val_loss=1.0266 | val_acc=0.6187
Epoch 025 | loss=0.9496 | train_acc=0.6087 | val_loss=1.0262 | val_acc=0.6187
Epoch 026 | loss=0.9505 | train_acc=0.6079 | val_loss=1.0261 | val_acc=0.6187
Epoch 027 | loss=0.9494 | train_acc=0.6089 | val_loss=1.0262 | val_acc=0.6187
Epoch 028 | loss=0.9497 | train_acc=0.6085 | val_loss=1.0261 | val_acc=0.6187
Epoch 029 | loss=0.9496 | train_acc=0.6086 | val_loss=1.0261 | val_acc=0.6187
Epoch 030 | loss=0.9499 | train_acc=0.6083 | val_loss=1.0261 | val_acc=0.6187
Epoch 031 | loss=0.9490 | train_acc=0.6089 | val_loss=1.0262 | val_acc=0.6187
Epoch 032 | loss=0.9499 | train_acc=0.6079 | val_loss=1.0262 | val_acc=0.6187
Epoch 033 | loss=0.9496 | train_acc=0.6083 | val_loss=1.0262 | val_acc=0.6187
Epoch 034 | loss=0.9505 | train_acc=0.6078 | val_loss=1.0264 | val_acc=0.6187
Epoch 035 | loss=0.9497 | train_acc=0.6082 | val_loss=1.0261 | val_acc=0.6187
Epoch 036 | loss=0.9496 | train_acc=0.6083 | val_loss=1.0261 | val_acc=0.6187
Epoch 037 | loss=0.9494 | train_acc=0.6081 | val_loss=1.0261 | val_acc=0.6187
Epoch 038 | loss=0.9498 | train_acc=0.6082 | val_loss=1.0261 | val_acc=0.6187
Epoch 039 | loss=0.9497 | train_acc=0.6087 | val_loss=1.0261 | val_acc=0.6187
Epoch 040 | loss=0.9498 | train_acc=0.6084 | val_loss=1.0261 | val_acc=0.6187
Epoch 041 | loss=0.9491 | train_acc=0.6088 | val_loss=1.0261 | val_acc=0.6187
Epoch 042 | loss=0.9496 | train_acc=0.6084 | val_loss=1.0261 | val_acc=0.6187
Epoch 043 | loss=0.9492 | train_acc=0.6084 | val_loss=1.0261 | val_acc=0.6187
Epoch 044 | loss=0.9493 | train_acc=0.6087 | val_loss=1.0261 | val_acc=0.6187
Epoch 045 | loss=0.9493 | train_acc=0.6083 | val_loss=1.0261 | val_acc=0.6187
Epoch 046 | loss=0.9491 | train_acc=0.6083 | val_loss=1.0262 | val_acc=0.6187
Epoch 047 | loss=0.9496 | train_acc=0.6085 | val_loss=1.0261 | val_acc=0.6187
Epoch 048 | loss=0.9489 | train_acc=0.6079 | val_loss=1.0261 | val_acc=0.6187
Epoch 049 | loss=0.9487 | train_acc=0.6086 | val_loss=1.0261 | val_acc=0.6187
Final Test Loss: 0.9189 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=9.1828 | train_acc=0.6167 | val_loss=13.9500 | val_acc=0.1899
Epoch 001 | loss=3.3228 | train_acc=0.5411 | val_loss=1.0669 | val_acc=0.6187
Epoch 002 | loss=1.0700 | train_acc=0.5867 | val_loss=1.0467 | val_acc=0.6187
Epoch 003 | loss=1.0332 | train_acc=0.6008 | val_loss=1.0333 | val_acc=0.6187
Epoch 004 | loss=1.0112 | train_acc=0.6040 | val_loss=1.0246 | val_acc=0.6187
Epoch 005 | loss=0.9966 | train_acc=0.6050 | val_loss=1.0188 | val_acc=0.6187
Epoch 006 | loss=0.9887 | train_acc=0.6062 | val_loss=1.0148 | val_acc=0.6187
Epoch 007 | loss=0.9822 | train_acc=0.6067 | val_loss=1.0120 | val_acc=0.6187
Epoch 008 | loss=0.9783 | train_acc=0.6074 | val_loss=1.0102 | val_acc=0.6187
Epoch 009 | loss=0.9720 | train_acc=0.6068 | val_loss=1.0086 | val_acc=0.6187
Epoch 010 | loss=0.9690 | train_acc=0.6066 | val_loss=1.0077 | val_acc=0.6187
Epoch 011 | loss=0.9675 | train_acc=0.6071 | val_loss=1.0071 | val_acc=0.6187
Epoch 012 | loss=0.9628 | train_acc=0.6077 | val_loss=1.0066 | val_acc=0.6187
Epoch 013 | loss=0.9581 | train_acc=0.6081 | val_loss=1.0062 | val_acc=0.6187
Epoch 014 | loss=0.9567 | train_acc=0.6075 | val_loss=1.0060 | val_acc=0.6187
Epoch 015 | loss=0.9563 | train_acc=0.6080 | val_loss=1.0062 | val_acc=0.6187
Epoch 016 | loss=0.9549 | train_acc=0.6078 | val_loss=1.0062 | val_acc=0.6187
Epoch 017 | loss=0.9527 | train_acc=0.6075 | val_loss=1.0064 | val_acc=0.6187
Epoch 018 | loss=0.9505 | train_acc=0.6078 | val_loss=1.0063 | val_acc=0.6187
Epoch 019 | loss=0.9497 | train_acc=0.6076 | val_loss=1.0067 | val_acc=0.6187
Epoch 020 | loss=0.9473 | train_acc=0.6080 | val_loss=1.0069 | val_acc=0.6187
Epoch 021 | loss=0.9475 | train_acc=0.6080 | val_loss=1.0074 | val_acc=0.6187
Epoch 022 | loss=0.9459 | train_acc=0.6077 | val_loss=1.0079 | val_acc=0.6187
Epoch 023 | loss=0.9453 | train_acc=0.6083 | val_loss=1.0084 | val_acc=0.6187
Epoch 024 | loss=0.9440 | train_acc=0.6082 | val_loss=1.0089 | val_acc=0.6187
Epoch 025 | loss=0.9417 | train_acc=0.6081 | val_loss=1.0092 | val_acc=0.6187
Epoch 026 | loss=0.9409 | train_acc=0.6083 | val_loss=1.0094 | val_acc=0.6187
Epoch 027 | loss=0.9405 | train_acc=0.6088 | val_loss=1.0098 | val_acc=0.6187
Epoch 028 | loss=0.9403 | train_acc=0.6081 | val_loss=1.0102 | val_acc=0.6187
Epoch 029 | loss=0.9394 | train_acc=0.6082 | val_loss=1.0106 | val_acc=0.6187
Epoch 030 | loss=0.9390 | train_acc=0.6087 | val_loss=1.0111 | val_acc=0.6187
Epoch 031 | loss=0.9389 | train_acc=0.6087 | val_loss=1.0116 | val_acc=0.6187
Epoch 032 | loss=0.9387 | train_acc=0.6077 | val_loss=1.0121 | val_acc=0.6187
Epoch 033 | loss=0.9378 | train_acc=0.6083 | val_loss=1.0125 | val_acc=0.6187
Epoch 034 | loss=0.9373 | train_acc=0.6080 | val_loss=1.0131 | val_acc=0.6187
Epoch 035 | loss=0.9372 | train_acc=0.6082 | val_loss=1.0140 | val_acc=0.6187
Epoch 036 | loss=0.9364 | train_acc=0.6076 | val_loss=1.0147 | val_acc=0.6187
Epoch 037 | loss=0.9367 | train_acc=0.6083 | val_loss=1.0147 | val_acc=0.6187
Epoch 038 | loss=0.9363 | train_acc=0.6085 | val_loss=1.0147 | val_acc=0.6187
Epoch 039 | loss=0.9366 | train_acc=0.6080 | val_loss=1.0150 | val_acc=0.6187
Epoch 040 | loss=0.9381 | train_acc=0.6083 | val_loss=1.0146 | val_acc=0.6187
Epoch 041 | loss=0.9369 | train_acc=0.6081 | val_loss=1.0143 | val_acc=0.6187
Epoch 042 | loss=0.9362 | train_acc=0.6085 | val_loss=1.0149 | val_acc=0.6187
Epoch 043 | loss=0.9361 | train_acc=0.6083 | val_loss=1.0152 | val_acc=0.6187
Epoch 044 | loss=0.9341 | train_acc=0.6082 | val_loss=1.0163 | val_acc=0.6187
Epoch 045 | loss=0.9332 | train_acc=0.6083 | val_loss=1.0178 | val_acc=0.6187
Epoch 046 | loss=0.9380 | train_acc=0.6084 | val_loss=1.0226 | val_acc=0.6187
Epoch 047 | loss=0.9364 | train_acc=0.6080 | val_loss=1.0231 | val_acc=0.6187
Epoch 048 | loss=0.9375 | train_acc=0.6078 | val_loss=1.0230 | val_acc=0.6187
Epoch 049 | loss=0.9359 | train_acc=0.6076 | val_loss=1.0235 | val_acc=0.6187
Final Test Loss: 0.9241 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=4.2165 | train_acc=0.6051 | val_loss=4.5479 | val_acc=0.1866
Epoch 001 | loss=2.1264 | train_acc=0.5349 | val_loss=1.0522 | val_acc=0.6187
Epoch 002 | loss=1.0735 | train_acc=0.5749 | val_loss=1.0288 | val_acc=0.6187
Epoch 003 | loss=1.0124 | train_acc=0.6004 | val_loss=1.0163 | val_acc=0.6187
Epoch 004 | loss=0.9873 | train_acc=0.6056 | val_loss=1.0116 | val_acc=0.6187
Epoch 005 | loss=0.9734 | train_acc=0.6064 | val_loss=1.0106 | val_acc=0.6187
Epoch 006 | loss=0.9659 | train_acc=0.6069 | val_loss=1.0112 | val_acc=0.6187
Epoch 007 | loss=0.9604 | train_acc=0.6077 | val_loss=1.0123 | val_acc=0.6187
Epoch 008 | loss=0.9580 | train_acc=0.6077 | val_loss=1.0135 | val_acc=0.6187
Epoch 009 | loss=0.9575 | train_acc=0.6077 | val_loss=1.0147 | val_acc=0.6187
Epoch 010 | loss=0.9545 | train_acc=0.6075 | val_loss=1.0158 | val_acc=0.6187
Epoch 011 | loss=0.9538 | train_acc=0.6079 | val_loss=1.0168 | val_acc=0.6187
Epoch 012 | loss=0.9537 | train_acc=0.6080 | val_loss=1.0177 | val_acc=0.6187
Epoch 013 | loss=0.9533 | train_acc=0.6076 | val_loss=1.0183 | val_acc=0.6187
Epoch 014 | loss=0.9530 | train_acc=0.6074 | val_loss=1.0189 | val_acc=0.6187
Epoch 015 | loss=0.9528 | train_acc=0.6078 | val_loss=1.0193 | val_acc=0.6187
Epoch 016 | loss=0.9526 | train_acc=0.6077 | val_loss=1.0198 | val_acc=0.6187
Epoch 017 | loss=0.9520 | train_acc=0.6081 | val_loss=1.0202 | val_acc=0.6187
Epoch 018 | loss=0.9524 | train_acc=0.6077 | val_loss=1.0206 | val_acc=0.6187
Epoch 019 | loss=0.9523 | train_acc=0.6082 | val_loss=1.0209 | val_acc=0.6187
Epoch 020 | loss=0.9521 | train_acc=0.6079 | val_loss=1.0212 | val_acc=0.6187
Epoch 021 | loss=0.9517 | train_acc=0.6081 | val_loss=1.0214 | val_acc=0.6187
Epoch 022 | loss=0.9522 | train_acc=0.6083 | val_loss=1.0217 | val_acc=0.6187
Epoch 023 | loss=0.9522 | train_acc=0.6082 | val_loss=1.0219 | val_acc=0.6187
Epoch 024 | loss=0.9523 | train_acc=0.6082 | val_loss=1.0220 | val_acc=0.6187
Epoch 025 | loss=0.9523 | train_acc=0.6086 | val_loss=1.0220 | val_acc=0.6187
Epoch 026 | loss=0.9518 | train_acc=0.6085 | val_loss=1.0222 | val_acc=0.6187
Epoch 027 | loss=0.9525 | train_acc=0.6081 | val_loss=1.0222 | val_acc=0.6187
Epoch 028 | loss=0.9520 | train_acc=0.6084 | val_loss=1.0223 | val_acc=0.6187
Epoch 029 | loss=0.9519 | train_acc=0.6083 | val_loss=1.0224 | val_acc=0.6187
Epoch 030 | loss=0.9526 | train_acc=0.6084 | val_loss=1.0223 | val_acc=0.6187
Epoch 031 | loss=0.9522 | train_acc=0.6084 | val_loss=1.0223 | val_acc=0.6187
Epoch 032 | loss=0.9516 | train_acc=0.6076 | val_loss=1.0224 | val_acc=0.6187
Epoch 033 | loss=0.9514 | train_acc=0.6081 | val_loss=1.0224 | val_acc=0.6187
Epoch 034 | loss=0.9503 | train_acc=0.6088 | val_loss=1.0221 | val_acc=0.6187
Epoch 035 | loss=0.9511 | train_acc=0.6078 | val_loss=1.0223 | val_acc=0.6187
Epoch 036 | loss=0.9499 | train_acc=0.6087 | val_loss=1.0220 | val_acc=0.6187
Epoch 037 | loss=0.9499 | train_acc=0.6083 | val_loss=1.0216 | val_acc=0.6187
Epoch 038 | loss=0.9490 | train_acc=0.6080 | val_loss=1.0212 | val_acc=0.6187
Epoch 039 | loss=0.9466 | train_acc=0.6083 | val_loss=1.0209 | val_acc=0.6187
Epoch 040 | loss=0.9506 | train_acc=0.6078 | val_loss=1.0210 | val_acc=0.6187
Epoch 041 | loss=0.9451 | train_acc=0.6080 | val_loss=1.0208 | val_acc=0.6187
Epoch 042 | loss=0.9432 | train_acc=0.6081 | val_loss=1.0204 | val_acc=0.6187
Epoch 043 | loss=0.9424 | train_acc=0.6081 | val_loss=1.0193 | val_acc=0.6187
Epoch 044 | loss=0.9418 | train_acc=0.6080 | val_loss=1.0195 | val_acc=0.6187
Epoch 045 | loss=0.9413 | train_acc=0.6086 | val_loss=1.0190 | val_acc=0.6187
Epoch 046 | loss=0.9368 | train_acc=0.6076 | val_loss=1.0186 | val_acc=0.6187
Epoch 047 | loss=0.9382 | train_acc=0.6082 | val_loss=1.0186 | val_acc=0.6187
Epoch 048 | loss=0.9403 | train_acc=0.6081 | val_loss=1.0191 | val_acc=0.6187
Epoch 049 | loss=0.9403 | train_acc=0.6062 | val_loss=1.0190 | val_acc=0.6187
Final Test Loss: 0.9232 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=11.7084 | train_acc=0.6254 | val_loss=77.9786 | val_acc=0.1873
Epoch 001 | loss=6.0273 | train_acc=0.5675 | val_loss=2.3316 | val_acc=0.3217
Epoch 002 | loss=1.3758 | train_acc=0.5447 | val_loss=1.0462 | val_acc=0.6187
Epoch 003 | loss=1.0812 | train_acc=0.5920 | val_loss=1.0356 | val_acc=0.6187
Epoch 004 | loss=1.0363 | train_acc=0.5972 | val_loss=1.0285 | val_acc=0.6187
Epoch 005 | loss=1.0214 | train_acc=0.6009 | val_loss=1.0237 | val_acc=0.6187
Epoch 006 | loss=1.0016 | train_acc=0.6029 | val_loss=1.0203 | val_acc=0.6187
Epoch 007 | loss=0.9939 | train_acc=0.6052 | val_loss=1.0177 | val_acc=0.6187
Epoch 008 | loss=0.9863 | train_acc=0.6061 | val_loss=1.0154 | val_acc=0.6187
Epoch 009 | loss=0.9852 | train_acc=0.6060 | val_loss=1.0136 | val_acc=0.6187
Epoch 010 | loss=0.9788 | train_acc=0.6061 | val_loss=1.0122 | val_acc=0.6187
Epoch 011 | loss=0.9750 | train_acc=0.6060 | val_loss=1.0110 | val_acc=0.6187
Epoch 012 | loss=0.9732 | train_acc=0.6068 | val_loss=1.0101 | val_acc=0.6187
Epoch 013 | loss=0.9680 | train_acc=0.6076 | val_loss=1.0093 | val_acc=0.6187
Epoch 014 | loss=0.9680 | train_acc=0.6073 | val_loss=1.0086 | val_acc=0.6187
Epoch 015 | loss=0.9636 | train_acc=0.6071 | val_loss=1.0082 | val_acc=0.6187
Epoch 016 | loss=0.9617 | train_acc=0.6076 | val_loss=1.0079 | val_acc=0.6187
Epoch 017 | loss=0.9821 | train_acc=0.6051 | val_loss=1.0082 | val_acc=0.6187
Epoch 018 | loss=0.9594 | train_acc=0.6072 | val_loss=1.0081 | val_acc=0.6187
Epoch 019 | loss=0.9584 | train_acc=0.6077 | val_loss=1.0079 | val_acc=0.6187
Epoch 020 | loss=0.9549 | train_acc=0.6072 | val_loss=1.0080 | val_acc=0.6187
Epoch 021 | loss=0.9568 | train_acc=0.6078 | val_loss=1.0079 | val_acc=0.6187
Epoch 022 | loss=0.9526 | train_acc=0.6083 | val_loss=1.0082 | val_acc=0.6187
Epoch 023 | loss=0.9523 | train_acc=0.6085 | val_loss=1.0085 | val_acc=0.6187
Epoch 024 | loss=0.9525 | train_acc=0.6082 | val_loss=1.0086 | val_acc=0.6187
Epoch 025 | loss=0.9499 | train_acc=0.6079 | val_loss=1.0091 | val_acc=0.6187
Epoch 026 | loss=0.9520 | train_acc=0.6080 | val_loss=1.0091 | val_acc=0.6187
Epoch 027 | loss=0.9504 | train_acc=0.6080 | val_loss=1.0093 | val_acc=0.6187
Epoch 028 | loss=0.9487 | train_acc=0.6079 | val_loss=1.0097 | val_acc=0.6187
Epoch 029 | loss=0.9481 | train_acc=0.6080 | val_loss=1.0101 | val_acc=0.6187
Epoch 030 | loss=0.9469 | train_acc=0.6085 | val_loss=1.0105 | val_acc=0.6187
Epoch 031 | loss=0.9477 | train_acc=0.6078 | val_loss=1.0109 | val_acc=0.6187
Epoch 032 | loss=0.9466 | train_acc=0.6082 | val_loss=1.0117 | val_acc=0.6187
Epoch 033 | loss=0.9479 | train_acc=0.6084 | val_loss=1.0118 | val_acc=0.6187
Epoch 034 | loss=0.9462 | train_acc=0.6085 | val_loss=1.0122 | val_acc=0.6187
Epoch 035 | loss=0.9456 | train_acc=0.6078 | val_loss=1.0126 | val_acc=0.6187
Epoch 036 | loss=0.9441 | train_acc=0.6079 | val_loss=1.0133 | val_acc=0.6187
Epoch 037 | loss=0.9442 | train_acc=0.6081 | val_loss=1.0138 | val_acc=0.6187
Epoch 038 | loss=0.9445 | train_acc=0.6084 | val_loss=1.0140 | val_acc=0.6187
Epoch 039 | loss=0.9442 | train_acc=0.6080 | val_loss=1.0144 | val_acc=0.6187
Epoch 040 | loss=0.9453 | train_acc=0.6057 | val_loss=1.0149 | val_acc=0.6187
Epoch 041 | loss=0.9425 | train_acc=0.6082 | val_loss=1.0152 | val_acc=0.6187
Epoch 042 | loss=0.9416 | train_acc=0.6083 | val_loss=1.0162 | val_acc=0.6187
Epoch 043 | loss=0.9438 | train_acc=0.6084 | val_loss=1.0162 | val_acc=0.6187
Epoch 044 | loss=0.9415 | train_acc=0.6083 | val_loss=1.0171 | val_acc=0.6187
Epoch 045 | loss=0.9425 | train_acc=0.6083 | val_loss=1.0173 | val_acc=0.6187
Epoch 046 | loss=0.9412 | train_acc=0.6079 | val_loss=1.0177 | val_acc=0.6187
Epoch 047 | loss=0.9403 | train_acc=0.6084 | val_loss=1.0184 | val_acc=0.6187
Epoch 048 | loss=0.9409 | train_acc=0.6082 | val_loss=1.0188 | val_acc=0.6187
Epoch 049 | loss=0.9384 | train_acc=0.6084 | val_loss=1.0200 | val_acc=0.6187
Final Test Loss: 0.9221 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=5.9841 | train_acc=0.6420 | val_loss=18.5158 | val_acc=0.2573
Epoch 001 | loss=3.2746 | train_acc=0.5945 | val_loss=3.1719 | val_acc=0.2051
Epoch 002 | loss=1.5505 | train_acc=0.5542 | val_loss=1.0342 | val_acc=0.6187
Epoch 003 | loss=1.0535 | train_acc=0.5824 | val_loss=1.0209 | val_acc=0.6187
Epoch 004 | loss=1.0169 | train_acc=0.5964 | val_loss=1.0156 | val_acc=0.6187
Epoch 005 | loss=0.9959 | train_acc=0.5999 | val_loss=1.0142 | val_acc=0.6187
Epoch 006 | loss=0.9841 | train_acc=0.6027 | val_loss=1.0151 | val_acc=0.6187
Epoch 007 | loss=0.9777 | train_acc=0.6046 | val_loss=1.0166 | val_acc=0.6187
Epoch 008 | loss=0.9729 | train_acc=0.6044 | val_loss=1.0181 | val_acc=0.6187
Epoch 009 | loss=0.9693 | train_acc=0.6058 | val_loss=1.0194 | val_acc=0.6187
Epoch 010 | loss=0.9649 | train_acc=0.6057 | val_loss=1.0206 | val_acc=0.6187
Epoch 011 | loss=0.9633 | train_acc=0.6067 | val_loss=1.0215 | val_acc=0.6187
Epoch 012 | loss=0.9618 | train_acc=0.6066 | val_loss=1.0223 | val_acc=0.6187
Epoch 013 | loss=0.9608 | train_acc=0.6068 | val_loss=1.0227 | val_acc=0.6187
Epoch 014 | loss=0.9594 | train_acc=0.6071 | val_loss=1.0232 | val_acc=0.6187
Epoch 015 | loss=0.9581 | train_acc=0.6075 | val_loss=1.0235 | val_acc=0.6187
Epoch 016 | loss=0.9582 | train_acc=0.6077 | val_loss=1.0237 | val_acc=0.6187
Epoch 017 | loss=0.9573 | train_acc=0.6080 | val_loss=1.0239 | val_acc=0.6187
Epoch 018 | loss=0.9564 | train_acc=0.6077 | val_loss=1.0241 | val_acc=0.6187
Epoch 019 | loss=0.9577 | train_acc=0.6070 | val_loss=1.0242 | val_acc=0.6187
Epoch 020 | loss=0.9574 | train_acc=0.6073 | val_loss=1.0243 | val_acc=0.6187
Epoch 021 | loss=0.9567 | train_acc=0.6076 | val_loss=1.0244 | val_acc=0.6187
Epoch 022 | loss=0.9562 | train_acc=0.6073 | val_loss=1.0244 | val_acc=0.6187
Epoch 023 | loss=0.9565 | train_acc=0.6073 | val_loss=1.0244 | val_acc=0.6187
Epoch 024 | loss=0.9556 | train_acc=0.6078 | val_loss=1.0245 | val_acc=0.6187
Epoch 025 | loss=0.9560 | train_acc=0.6078 | val_loss=1.0244 | val_acc=0.6187
Epoch 026 | loss=0.9560 | train_acc=0.6075 | val_loss=1.0244 | val_acc=0.6187
Epoch 027 | loss=0.9561 | train_acc=0.6073 | val_loss=1.0244 | val_acc=0.6187
Epoch 028 | loss=0.9549 | train_acc=0.6079 | val_loss=1.0244 | val_acc=0.6187
Epoch 029 | loss=0.9544 | train_acc=0.6085 | val_loss=1.0245 | val_acc=0.6187
Epoch 030 | loss=0.9539 | train_acc=0.6085 | val_loss=1.0245 | val_acc=0.6187
Epoch 031 | loss=0.9545 | train_acc=0.6079 | val_loss=1.0246 | val_acc=0.6187
Epoch 032 | loss=0.9544 | train_acc=0.6080 | val_loss=1.0246 | val_acc=0.6187
Epoch 033 | loss=0.9538 | train_acc=0.6085 | val_loss=1.0246 | val_acc=0.6187
Epoch 034 | loss=0.9538 | train_acc=0.6082 | val_loss=1.0246 | val_acc=0.6187
Epoch 035 | loss=0.9539 | train_acc=0.6081 | val_loss=1.0246 | val_acc=0.6187
Epoch 036 | loss=0.9544 | train_acc=0.6078 | val_loss=1.0246 | val_acc=0.6187
Epoch 037 | loss=0.9532 | train_acc=0.6087 | val_loss=1.0246 | val_acc=0.6187
Epoch 038 | loss=0.9541 | train_acc=0.6081 | val_loss=1.0246 | val_acc=0.6187
Epoch 039 | loss=0.9540 | train_acc=0.6078 | val_loss=1.0245 | val_acc=0.6187
Epoch 040 | loss=0.9536 | train_acc=0.6082 | val_loss=1.0245 | val_acc=0.6187
Epoch 041 | loss=0.9538 | train_acc=0.6077 | val_loss=1.0245 | val_acc=0.6187
Epoch 042 | loss=0.9526 | train_acc=0.6083 | val_loss=1.0245 | val_acc=0.6187
Epoch 043 | loss=0.9451 | train_acc=0.6079 | val_loss=1.0325 | val_acc=0.6187
Epoch 044 | loss=0.9588 | train_acc=0.6083 | val_loss=1.0318 | val_acc=0.6187
Epoch 045 | loss=0.9574 | train_acc=0.6082 | val_loss=1.0317 | val_acc=0.6187
Epoch 046 | loss=0.9574 | train_acc=0.6083 | val_loss=1.0314 | val_acc=0.6187
Epoch 047 | loss=0.9551 | train_acc=0.6070 | val_loss=1.0306 | val_acc=0.6187
Epoch 048 | loss=0.9544 | train_acc=0.6084 | val_loss=1.0307 | val_acc=0.6187
Epoch 049 | loss=0.9535 | train_acc=0.6085 | val_loss=1.0309 | val_acc=0.6187
Final Test Loss: 0.9225 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=4.9867 | train_acc=0.5667 | val_loss=1.0678 | val_acc=0.6094
Epoch 001 | loss=1.1285 | train_acc=0.5869 | val_loss=1.0438 | val_acc=0.6187
Epoch 002 | loss=1.0179 | train_acc=0.6052 | val_loss=1.0334 | val_acc=0.6187
Epoch 003 | loss=0.9976 | train_acc=0.6061 | val_loss=1.0271 | val_acc=0.6187
Epoch 004 | loss=0.9907 | train_acc=0.6074 | val_loss=1.0229 | val_acc=0.6187
Epoch 005 | loss=0.9854 | train_acc=0.6072 | val_loss=1.0195 | val_acc=0.6187
Epoch 006 | loss=0.9799 | train_acc=0.6074 | val_loss=1.0164 | val_acc=0.6187
Epoch 007 | loss=0.9738 | train_acc=0.6075 | val_loss=1.0141 | val_acc=0.6187
Epoch 008 | loss=0.9700 | train_acc=0.6079 | val_loss=1.0120 | val_acc=0.6187
Epoch 009 | loss=0.9664 | train_acc=0.6075 | val_loss=1.0102 | val_acc=0.6187
Epoch 010 | loss=0.9601 | train_acc=0.6083 | val_loss=1.0083 | val_acc=0.6187
Epoch 011 | loss=0.9600 | train_acc=0.6083 | val_loss=1.0071 | val_acc=0.6187
Epoch 012 | loss=0.9556 | train_acc=0.6082 | val_loss=1.0063 | val_acc=0.6187
Epoch 013 | loss=0.9542 | train_acc=0.6078 | val_loss=1.0048 | val_acc=0.6187
Epoch 014 | loss=0.9507 | train_acc=0.6083 | val_loss=1.0052 | val_acc=0.6187
Epoch 015 | loss=0.9498 | train_acc=0.6081 | val_loss=1.0047 | val_acc=0.6187
Epoch 016 | loss=0.9478 | train_acc=0.6077 | val_loss=1.0046 | val_acc=0.6187
Epoch 017 | loss=0.9472 | train_acc=0.6081 | val_loss=1.0051 | val_acc=0.6187
Epoch 018 | loss=0.9464 | train_acc=0.6077 | val_loss=1.0051 | val_acc=0.6187
Epoch 019 | loss=0.9444 | train_acc=0.6078 | val_loss=1.0045 | val_acc=0.6187
Epoch 020 | loss=0.9418 | train_acc=0.6080 | val_loss=1.0035 | val_acc=0.6187
Epoch 021 | loss=0.9417 | train_acc=0.6077 | val_loss=1.0043 | val_acc=0.6187
Epoch 022 | loss=0.9408 | train_acc=0.6180 | val_loss=1.0053 | val_acc=0.6187
Epoch 023 | loss=0.9588 | train_acc=0.6062 | val_loss=1.0080 | val_acc=0.6187
Epoch 024 | loss=0.9370 | train_acc=0.6062 | val_loss=1.0067 | val_acc=0.6187
Epoch 025 | loss=0.9400 | train_acc=0.6063 | val_loss=1.0070 | val_acc=0.6187
Epoch 026 | loss=0.9405 | train_acc=0.6082 | val_loss=1.0006 | val_acc=0.6187
Epoch 027 | loss=0.9284 | train_acc=0.6080 | val_loss=1.0018 | val_acc=0.6187
Epoch 028 | loss=0.9257 | train_acc=0.6079 | val_loss=1.0072 | val_acc=0.6187
Epoch 029 | loss=0.9410 | train_acc=0.6079 | val_loss=1.0073 | val_acc=0.6187
Epoch 030 | loss=0.9378 | train_acc=0.6082 | val_loss=1.0009 | val_acc=0.6187
Epoch 031 | loss=0.9305 | train_acc=0.6077 | val_loss=0.9970 | val_acc=0.6187
Epoch 032 | loss=0.9330 | train_acc=0.6075 | val_loss=0.9965 | val_acc=0.6187
Epoch 033 | loss=0.9384 | train_acc=0.6085 | val_loss=0.9925 | val_acc=0.6187
Epoch 034 | loss=0.9385 | train_acc=0.6069 | val_loss=1.0088 | val_acc=0.6187
Epoch 035 | loss=0.9373 | train_acc=0.6065 | val_loss=1.0052 | val_acc=0.6187
Epoch 036 | loss=0.9338 | train_acc=0.6069 | val_loss=1.0084 | val_acc=0.6187
Epoch 037 | loss=0.9347 | train_acc=0.6074 | val_loss=1.0076 | val_acc=0.6187
Epoch 038 | loss=0.9283 | train_acc=0.6075 | val_loss=1.0118 | val_acc=0.6187
Epoch 039 | loss=0.9286 | train_acc=0.6081 | val_loss=1.0108 | val_acc=0.6187
Epoch 040 | loss=0.9209 | train_acc=0.6084 | val_loss=1.0132 | val_acc=0.6187
Epoch 041 | loss=0.9261 | train_acc=0.6086 | val_loss=1.0103 | val_acc=0.6187
Epoch 042 | loss=0.9248 | train_acc=0.6075 | val_loss=1.0032 | val_acc=0.6187
Epoch 043 | loss=0.9212 | train_acc=0.6081 | val_loss=0.9897 | val_acc=0.6187
Epoch 044 | loss=0.9131 | train_acc=0.6087 | val_loss=0.9825 | val_acc=0.6187
Epoch 045 | loss=0.9147 | train_acc=0.6079 | val_loss=0.9569 | val_acc=0.6187
Epoch 046 | loss=0.9160 | train_acc=0.6076 | val_loss=1.0178 | val_acc=0.6187
Epoch 047 | loss=0.9255 | train_acc=0.6088 | val_loss=1.0174 | val_acc=0.6187
Epoch 048 | loss=0.9268 | train_acc=0.6074 | val_loss=1.0178 | val_acc=0.6187
Epoch 049 | loss=0.9258 | train_acc=0.6075 | val_loss=1.0183 | val_acc=0.6187
Final Test Loss: 0.9245 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.8452 | train_acc=0.5649 | val_loss=1.0536 | val_acc=0.6187
Epoch 001 | loss=1.0289 | train_acc=0.6071 | val_loss=1.0277 | val_acc=0.6187
Epoch 002 | loss=0.9960 | train_acc=0.6080 | val_loss=1.0144 | val_acc=0.6187
Epoch 003 | loss=0.9776 | train_acc=0.6084 | val_loss=1.0098 | val_acc=0.6187
Epoch 004 | loss=0.9670 | train_acc=0.6082 | val_loss=1.0098 | val_acc=0.6187
Epoch 005 | loss=0.9617 | train_acc=0.6085 | val_loss=1.0115 | val_acc=0.6187
Epoch 006 | loss=0.9579 | train_acc=0.6081 | val_loss=1.0135 | val_acc=0.6187
Epoch 007 | loss=0.9563 | train_acc=0.6081 | val_loss=1.0153 | val_acc=0.6187
Epoch 008 | loss=0.9537 | train_acc=0.6088 | val_loss=1.0170 | val_acc=0.6187
Epoch 009 | loss=0.9536 | train_acc=0.6081 | val_loss=1.0184 | val_acc=0.6187
Epoch 010 | loss=0.9533 | train_acc=0.6080 | val_loss=1.0195 | val_acc=0.6187
Epoch 011 | loss=0.9534 | train_acc=0.6080 | val_loss=1.0211 | val_acc=0.6187
Epoch 012 | loss=0.9524 | train_acc=0.6080 | val_loss=1.0217 | val_acc=0.6187
Epoch 013 | loss=0.9520 | train_acc=0.6081 | val_loss=1.0221 | val_acc=0.6187
Epoch 014 | loss=0.9522 | train_acc=0.6081 | val_loss=1.0224 | val_acc=0.6187
Epoch 015 | loss=0.9516 | train_acc=0.6085 | val_loss=1.0228 | val_acc=0.6187
Epoch 016 | loss=0.9522 | train_acc=0.6082 | val_loss=1.0231 | val_acc=0.6187
Epoch 017 | loss=0.9514 | train_acc=0.6086 | val_loss=1.0233 | val_acc=0.6187
Epoch 018 | loss=0.9514 | train_acc=0.6083 | val_loss=1.0235 | val_acc=0.6187
Epoch 019 | loss=0.9508 | train_acc=0.6087 | val_loss=1.0236 | val_acc=0.6187
Epoch 020 | loss=0.9509 | train_acc=0.6082 | val_loss=1.0238 | val_acc=0.6187
Epoch 021 | loss=0.9507 | train_acc=0.6079 | val_loss=1.0241 | val_acc=0.6187
Epoch 022 | loss=0.9514 | train_acc=0.6083 | val_loss=1.0238 | val_acc=0.6187
Epoch 023 | loss=0.9507 | train_acc=0.6080 | val_loss=1.0239 | val_acc=0.6187
Epoch 024 | loss=0.9512 | train_acc=0.6083 | val_loss=1.0243 | val_acc=0.6187
Epoch 025 | loss=0.9501 | train_acc=0.6082 | val_loss=1.0243 | val_acc=0.6187
Epoch 026 | loss=0.9501 | train_acc=0.6083 | val_loss=1.0243 | val_acc=0.6187
Epoch 027 | loss=0.9504 | train_acc=0.6086 | val_loss=1.0246 | val_acc=0.6187
Epoch 028 | loss=0.9502 | train_acc=0.6078 | val_loss=1.0246 | val_acc=0.6187
Epoch 029 | loss=0.9502 | train_acc=0.6082 | val_loss=1.0244 | val_acc=0.6187
Epoch 030 | loss=0.9508 | train_acc=0.6080 | val_loss=1.0244 | val_acc=0.6187
Epoch 031 | loss=0.9497 | train_acc=0.6080 | val_loss=1.0257 | val_acc=0.6187
Epoch 032 | loss=0.9490 | train_acc=0.6083 | val_loss=1.0244 | val_acc=0.6187
Epoch 033 | loss=0.9500 | train_acc=0.6086 | val_loss=1.0243 | val_acc=0.6187
Epoch 034 | loss=0.9507 | train_acc=0.6087 | val_loss=1.0252 | val_acc=0.6187
Epoch 035 | loss=0.9526 | train_acc=0.6076 | val_loss=1.0227 | val_acc=0.6187
Epoch 036 | loss=0.9520 | train_acc=0.6081 | val_loss=1.0233 | val_acc=0.6187
Epoch 037 | loss=0.9506 | train_acc=0.6077 | val_loss=1.0243 | val_acc=0.6187
Epoch 038 | loss=0.9505 | train_acc=0.6084 | val_loss=1.0249 | val_acc=0.6187
Epoch 039 | loss=0.9507 | train_acc=0.6084 | val_loss=1.0241 | val_acc=0.6187
Epoch 040 | loss=0.9500 | train_acc=0.6086 | val_loss=1.0247 | val_acc=0.6187
Epoch 041 | loss=0.9493 | train_acc=0.6084 | val_loss=1.0246 | val_acc=0.6187
Epoch 042 | loss=0.9499 | train_acc=0.6082 | val_loss=1.0248 | val_acc=0.6187
Epoch 043 | loss=0.9487 | train_acc=0.6077 | val_loss=1.0244 | val_acc=0.6187
Epoch 044 | loss=0.9497 | train_acc=0.6087 | val_loss=1.0247 | val_acc=0.6187
Epoch 045 | loss=0.9496 | train_acc=0.6085 | val_loss=1.0247 | val_acc=0.6187
Epoch 046 | loss=0.9491 | train_acc=0.6082 | val_loss=1.0251 | val_acc=0.6187
Epoch 047 | loss=0.9486 | train_acc=0.6087 | val_loss=1.0251 | val_acc=0.6187
Epoch 048 | loss=0.9496 | train_acc=0.6079 | val_loss=1.0251 | val_acc=0.6187
Epoch 049 | loss=0.9490 | train_acc=0.6084 | val_loss=1.0251 | val_acc=0.6187
Final Test Loss: 0.9203 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=6.0549 | train_acc=0.6264 | val_loss=1.4191 | val_acc=0.4695
Epoch 001 | loss=1.3459 | train_acc=0.5858 | val_loss=1.0375 | val_acc=0.6187
Epoch 002 | loss=1.0176 | train_acc=0.6053 | val_loss=1.0181 | val_acc=0.6187
Epoch 003 | loss=0.9998 | train_acc=0.6069 | val_loss=1.0101 | val_acc=0.6187
Epoch 004 | loss=0.9899 | train_acc=0.6070 | val_loss=1.0072 | val_acc=0.6187
Epoch 005 | loss=0.9863 | train_acc=0.6076 | val_loss=1.0029 | val_acc=0.6187
Epoch 006 | loss=0.9788 | train_acc=0.6076 | val_loss=1.0006 | val_acc=0.6187
Epoch 007 | loss=0.9732 | train_acc=0.6081 | val_loss=0.9984 | val_acc=0.6187
Epoch 008 | loss=0.9712 | train_acc=0.6079 | val_loss=0.9990 | val_acc=0.6187
Epoch 009 | loss=0.9670 | train_acc=0.6078 | val_loss=0.9980 | val_acc=0.6187
Epoch 010 | loss=0.9639 | train_acc=0.6081 | val_loss=0.9973 | val_acc=0.6187
Epoch 011 | loss=0.9613 | train_acc=0.6079 | val_loss=0.9955 | val_acc=0.6187
Epoch 012 | loss=0.9580 | train_acc=0.6081 | val_loss=0.9957 | val_acc=0.6187
Epoch 013 | loss=0.9553 | train_acc=0.6080 | val_loss=0.9925 | val_acc=0.6187
Epoch 014 | loss=0.9516 | train_acc=0.6080 | val_loss=0.9916 | val_acc=0.6187
Epoch 015 | loss=0.9499 | train_acc=0.6083 | val_loss=0.9913 | val_acc=0.6187
Epoch 016 | loss=0.9487 | train_acc=0.6082 | val_loss=0.9924 | val_acc=0.6187
Epoch 017 | loss=0.9467 | train_acc=0.6080 | val_loss=0.9934 | val_acc=0.6187
Epoch 018 | loss=0.9450 | train_acc=0.6080 | val_loss=0.9942 | val_acc=0.6187
Epoch 019 | loss=0.9447 | train_acc=0.6083 | val_loss=0.9953 | val_acc=0.6187
Epoch 020 | loss=0.9422 | train_acc=0.6082 | val_loss=0.9980 | val_acc=0.6187
Epoch 021 | loss=0.9425 | train_acc=0.6078 | val_loss=1.0000 | val_acc=0.6187
Epoch 022 | loss=0.9417 | train_acc=0.6081 | val_loss=0.9965 | val_acc=0.6187
Epoch 023 | loss=0.9411 | train_acc=0.6076 | val_loss=0.9980 | val_acc=0.6187
Epoch 024 | loss=0.9605 | train_acc=0.6047 | val_loss=0.9975 | val_acc=0.6187
Epoch 025 | loss=0.9439 | train_acc=0.6078 | val_loss=1.0046 | val_acc=0.6187
Epoch 026 | loss=0.9383 | train_acc=0.6080 | val_loss=1.0046 | val_acc=0.6187
Epoch 027 | loss=0.9410 | train_acc=0.6082 | val_loss=1.0016 | val_acc=0.6187
Epoch 028 | loss=0.9402 | train_acc=0.6072 | val_loss=1.0019 | val_acc=0.6187
Epoch 029 | loss=0.9399 | train_acc=0.6080 | val_loss=0.9838 | val_acc=0.6187
Epoch 030 | loss=0.9391 | train_acc=0.6083 | val_loss=0.9794 | val_acc=0.6187
Epoch 031 | loss=0.9392 | train_acc=0.6083 | val_loss=0.9716 | val_acc=0.6187
Epoch 032 | loss=0.9428 | train_acc=0.6064 | val_loss=0.9781 | val_acc=0.6187
Epoch 033 | loss=0.9374 | train_acc=0.6083 | val_loss=0.9733 | val_acc=0.6187
Epoch 034 | loss=0.9667 | train_acc=0.6068 | val_loss=1.0194 | val_acc=0.6187
Epoch 035 | loss=0.9306 | train_acc=0.6060 | val_loss=1.0234 | val_acc=0.6187
Epoch 036 | loss=0.9411 | train_acc=0.6082 | val_loss=1.0327 | val_acc=0.6187
Epoch 037 | loss=0.9416 | train_acc=0.6068 | val_loss=1.0502 | val_acc=0.6187
Epoch 038 | loss=0.9380 | train_acc=0.6086 | val_loss=1.0637 | val_acc=0.6187
Epoch 039 | loss=0.9389 | train_acc=0.6087 | val_loss=1.0407 | val_acc=0.6187
Epoch 040 | loss=0.9379 | train_acc=0.6083 | val_loss=1.0503 | val_acc=0.6187
Epoch 041 | loss=0.9373 | train_acc=0.6081 | val_loss=1.0430 | val_acc=0.6187
Epoch 042 | loss=0.9357 | train_acc=0.6087 | val_loss=1.0287 | val_acc=0.6187
Epoch 043 | loss=0.9388 | train_acc=0.6076 | val_loss=1.0469 | val_acc=0.6187
Epoch 044 | loss=0.9379 | train_acc=0.6078 | val_loss=1.0537 | val_acc=0.6187
Epoch 045 | loss=0.9367 | train_acc=0.6084 | val_loss=1.0513 | val_acc=0.6187
Epoch 046 | loss=0.9324 | train_acc=0.6062 | val_loss=1.0438 | val_acc=0.6187
Epoch 047 | loss=0.9376 | train_acc=0.6080 | val_loss=1.0203 | val_acc=0.6187
Epoch 048 | loss=0.9536 | train_acc=0.6069 | val_loss=1.0265 | val_acc=0.6187
Epoch 049 | loss=0.9397 | train_acc=0.6049 | val_loss=1.0325 | val_acc=0.6187
Final Test Loss: 0.9331 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=2.1005 | train_acc=0.5662 | val_loss=1.1438 | val_acc=0.5276
Epoch 001 | loss=1.0646 | train_acc=0.6002 | val_loss=1.0280 | val_acc=0.6187
Epoch 002 | loss=1.0020 | train_acc=0.6078 | val_loss=1.0135 | val_acc=0.6187
Epoch 003 | loss=0.9808 | train_acc=0.6081 | val_loss=1.0121 | val_acc=0.6187
Epoch 004 | loss=0.9711 | train_acc=0.6080 | val_loss=1.0135 | val_acc=0.6187
Epoch 005 | loss=0.9651 | train_acc=0.6077 | val_loss=1.0160 | val_acc=0.6187
Epoch 006 | loss=0.9620 | train_acc=0.6083 | val_loss=1.0182 | val_acc=0.6187
Epoch 007 | loss=0.9592 | train_acc=0.6082 | val_loss=1.0200 | val_acc=0.6187
Epoch 008 | loss=0.9589 | train_acc=0.6083 | val_loss=1.0212 | val_acc=0.6187
Epoch 009 | loss=0.9567 | train_acc=0.6085 | val_loss=1.0223 | val_acc=0.6187
Epoch 010 | loss=0.9571 | train_acc=0.6082 | val_loss=1.0230 | val_acc=0.6187
Epoch 011 | loss=0.9565 | train_acc=0.6085 | val_loss=1.0233 | val_acc=0.6187
Epoch 012 | loss=0.9562 | train_acc=0.6088 | val_loss=1.0237 | val_acc=0.6187
Epoch 013 | loss=0.9568 | train_acc=0.6083 | val_loss=1.0238 | val_acc=0.6187
Epoch 014 | loss=0.9561 | train_acc=0.6079 | val_loss=1.0239 | val_acc=0.6187
Epoch 015 | loss=0.9569 | train_acc=0.6079 | val_loss=1.0240 | val_acc=0.6187
Epoch 016 | loss=0.9565 | train_acc=0.6084 | val_loss=1.0240 | val_acc=0.6187
Epoch 017 | loss=0.9560 | train_acc=0.6081 | val_loss=1.0241 | val_acc=0.6187
Epoch 018 | loss=0.9558 | train_acc=0.6081 | val_loss=1.0239 | val_acc=0.6187
Epoch 019 | loss=0.9566 | train_acc=0.6080 | val_loss=1.0242 | val_acc=0.6187
Epoch 020 | loss=0.9554 | train_acc=0.6083 | val_loss=1.0243 | val_acc=0.6187
Epoch 021 | loss=0.9560 | train_acc=0.6083 | val_loss=1.0242 | val_acc=0.6187
Epoch 022 | loss=0.9557 | train_acc=0.6080 | val_loss=1.0243 | val_acc=0.6187
Epoch 023 | loss=0.9557 | train_acc=0.6083 | val_loss=1.0243 | val_acc=0.6187
Epoch 024 | loss=0.9553 | train_acc=0.6084 | val_loss=1.0243 | val_acc=0.6187
Epoch 025 | loss=0.9555 | train_acc=0.6079 | val_loss=1.0242 | val_acc=0.6187
Epoch 026 | loss=0.9549 | train_acc=0.6084 | val_loss=1.0243 | val_acc=0.6187
Epoch 027 | loss=0.9559 | train_acc=0.6084 | val_loss=1.0242 | val_acc=0.6187
Epoch 028 | loss=0.9558 | train_acc=0.6085 | val_loss=1.0242 | val_acc=0.6187
Epoch 029 | loss=0.9576 | train_acc=0.6080 | val_loss=1.0239 | val_acc=0.6187
Epoch 030 | loss=0.9568 | train_acc=0.6084 | val_loss=1.0241 | val_acc=0.6187
Epoch 031 | loss=0.9559 | train_acc=0.6085 | val_loss=1.0241 | val_acc=0.6187
Epoch 032 | loss=0.9556 | train_acc=0.6079 | val_loss=1.0241 | val_acc=0.6187
Epoch 033 | loss=0.9553 | train_acc=0.6078 | val_loss=1.0241 | val_acc=0.6187
Epoch 034 | loss=0.9543 | train_acc=0.6083 | val_loss=1.0242 | val_acc=0.6187
Epoch 035 | loss=0.9178 | train_acc=0.6468 | val_loss=1.0673 | val_acc=0.4735
Epoch 036 | loss=0.9973 | train_acc=0.6048 | val_loss=1.0209 | val_acc=0.6187
Epoch 037 | loss=0.9605 | train_acc=0.6087 | val_loss=1.0208 | val_acc=0.6187
Epoch 038 | loss=0.9576 | train_acc=0.6079 | val_loss=1.0220 | val_acc=0.6187
Epoch 039 | loss=0.9562 | train_acc=0.6082 | val_loss=1.0227 | val_acc=0.6187
Epoch 040 | loss=0.9557 | train_acc=0.6080 | val_loss=1.0231 | val_acc=0.6187
Epoch 041 | loss=0.9548 | train_acc=0.6088 | val_loss=1.0235 | val_acc=0.6187
Epoch 042 | loss=0.9549 | train_acc=0.6086 | val_loss=1.0237 | val_acc=0.6187
Epoch 043 | loss=0.9543 | train_acc=0.6084 | val_loss=1.0238 | val_acc=0.6187
Epoch 044 | loss=0.9545 | train_acc=0.6082 | val_loss=1.0239 | val_acc=0.6187
Epoch 045 | loss=0.9538 | train_acc=0.6088 | val_loss=1.0241 | val_acc=0.6187
Epoch 046 | loss=0.9538 | train_acc=0.6085 | val_loss=1.0241 | val_acc=0.6187
Epoch 047 | loss=0.9536 | train_acc=0.6084 | val_loss=1.0242 | val_acc=0.6187
Epoch 048 | loss=0.9537 | train_acc=0.6080 | val_loss=1.0242 | val_acc=0.6187
Epoch 049 | loss=0.9552 | train_acc=0.6082 | val_loss=1.0242 | val_acc=0.6187
Final Test Loss: 0.9200 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=6.7235 | train_acc=0.6363 | val_loss=3.8708 | val_acc=0.2525
Epoch 001 | loss=1.6692 | train_acc=0.5728 | val_loss=1.0502 | val_acc=0.6187
Epoch 002 | loss=1.0447 | train_acc=0.5940 | val_loss=1.0261 | val_acc=0.6187
Epoch 003 | loss=1.0155 | train_acc=0.6038 | val_loss=1.0194 | val_acc=0.6187
Epoch 004 | loss=1.0029 | train_acc=0.6050 | val_loss=0.9926 | val_acc=0.6187
Epoch 005 | loss=0.9938 | train_acc=0.6061 | val_loss=0.9891 | val_acc=0.6187
Epoch 006 | loss=0.9868 | train_acc=0.6072 | val_loss=0.9904 | val_acc=0.6187
Epoch 007 | loss=0.9833 | train_acc=0.6072 | val_loss=0.9916 | val_acc=0.6187
Epoch 008 | loss=0.9748 | train_acc=0.6075 | val_loss=1.0017 | val_acc=0.6187
Epoch 009 | loss=0.9705 | train_acc=0.6076 | val_loss=0.9943 | val_acc=0.6187
Epoch 010 | loss=0.9775 | train_acc=0.6063 | val_loss=0.9976 | val_acc=0.6187
Epoch 011 | loss=0.9661 | train_acc=0.6076 | val_loss=0.9968 | val_acc=0.6187
Epoch 012 | loss=0.9591 | train_acc=0.6074 | val_loss=1.0019 | val_acc=0.6187
Epoch 013 | loss=0.9636 | train_acc=0.6084 | val_loss=0.9932 | val_acc=0.6187
Epoch 014 | loss=0.9557 | train_acc=0.6081 | val_loss=0.9930 | val_acc=0.6187
Epoch 015 | loss=0.9560 | train_acc=0.6075 | val_loss=0.9875 | val_acc=0.6187
Epoch 016 | loss=0.9547 | train_acc=0.6081 | val_loss=0.9872 | val_acc=0.6187
Epoch 017 | loss=0.9531 | train_acc=0.6076 | val_loss=0.9912 | val_acc=0.6187
Epoch 018 | loss=0.9524 | train_acc=0.6079 | val_loss=0.9886 | val_acc=0.6187
Epoch 019 | loss=0.9510 | train_acc=0.6083 | val_loss=0.9922 | val_acc=0.6187
Epoch 020 | loss=0.9513 | train_acc=0.6079 | val_loss=0.9920 | val_acc=0.6187
Epoch 021 | loss=0.9513 | train_acc=0.6080 | val_loss=0.9930 | val_acc=0.6187
Epoch 022 | loss=0.9476 | train_acc=0.6075 | val_loss=1.0043 | val_acc=0.6187
Epoch 023 | loss=0.9492 | train_acc=0.6080 | val_loss=1.0046 | val_acc=0.6187
Epoch 024 | loss=0.9455 | train_acc=0.6082 | val_loss=1.0043 | val_acc=0.6187
Epoch 025 | loss=0.9599 | train_acc=0.6082 | val_loss=1.0063 | val_acc=0.6187
Epoch 026 | loss=0.9856 | train_acc=0.6085 | val_loss=1.0058 | val_acc=0.6187
Epoch 027 | loss=0.9667 | train_acc=0.6029 | val_loss=0.9961 | val_acc=0.6187
Epoch 028 | loss=0.9511 | train_acc=0.6085 | val_loss=1.0021 | val_acc=0.6187
Epoch 029 | loss=0.9487 | train_acc=0.6079 | val_loss=1.0007 | val_acc=0.6187
Epoch 030 | loss=0.9448 | train_acc=0.6082 | val_loss=1.0027 | val_acc=0.6187
Epoch 031 | loss=0.9436 | train_acc=0.6081 | val_loss=1.0010 | val_acc=0.6187
Epoch 032 | loss=0.9430 | train_acc=0.6083 | val_loss=1.0039 | val_acc=0.6187
Epoch 033 | loss=0.9422 | train_acc=0.6082 | val_loss=1.0085 | val_acc=0.6187
Epoch 034 | loss=0.9430 | train_acc=0.6082 | val_loss=1.0084 | val_acc=0.6187
Epoch 035 | loss=0.9393 | train_acc=0.6085 | val_loss=1.0090 | val_acc=0.6187
Epoch 036 | loss=0.9434 | train_acc=0.6083 | val_loss=1.0122 | val_acc=0.6187
Epoch 037 | loss=0.9414 | train_acc=0.6060 | val_loss=1.0140 | val_acc=0.6187
Epoch 038 | loss=0.9418 | train_acc=0.6082 | val_loss=1.0117 | val_acc=0.6187
Epoch 039 | loss=0.9430 | train_acc=0.6092 | val_loss=3.6677 | val_acc=0.2640
Epoch 040 | loss=1.0055 | train_acc=0.6030 | val_loss=1.0153 | val_acc=0.6168
Epoch 041 | loss=0.9860 | train_acc=0.6091 | val_loss=1.2830 | val_acc=0.5183
Epoch 042 | loss=0.9759 | train_acc=0.6059 | val_loss=1.0158 | val_acc=0.6187
Epoch 043 | loss=0.9431 | train_acc=0.6087 | val_loss=1.0159 | val_acc=0.6187
Epoch 044 | loss=0.9418 | train_acc=0.6083 | val_loss=1.0170 | val_acc=0.6187
Epoch 045 | loss=0.9415 | train_acc=0.6053 | val_loss=1.0172 | val_acc=0.6187
Epoch 046 | loss=0.9434 | train_acc=0.6061 | val_loss=1.0166 | val_acc=0.6187
Epoch 047 | loss=0.9331 | train_acc=0.6058 | val_loss=1.0155 | val_acc=0.6187
Epoch 048 | loss=0.9368 | train_acc=0.6070 | val_loss=1.0143 | val_acc=0.6187
Epoch 049 | loss=0.9435 | train_acc=0.6044 | val_loss=1.0140 | val_acc=0.6187
Final Test Loss: 0.9184 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=3.1061 | train_acc=0.6278 | val_loss=2.3771 | val_acc=0.1988
Epoch 001 | loss=1.1110 | train_acc=0.5935 | val_loss=1.0271 | val_acc=0.6187
Epoch 002 | loss=0.9942 | train_acc=0.6069 | val_loss=1.0140 | val_acc=0.6187
Epoch 003 | loss=0.9738 | train_acc=0.6079 | val_loss=1.0105 | val_acc=0.6187
Epoch 004 | loss=0.9642 | train_acc=0.6078 | val_loss=1.0114 | val_acc=0.6187
Epoch 005 | loss=0.9603 | train_acc=0.6079 | val_loss=1.0136 | val_acc=0.6187
Epoch 006 | loss=0.9577 | train_acc=0.6077 | val_loss=1.0155 | val_acc=0.6187
Epoch 007 | loss=0.9552 | train_acc=0.6084 | val_loss=1.0170 | val_acc=0.6187
Epoch 008 | loss=0.9551 | train_acc=0.6084 | val_loss=1.0182 | val_acc=0.6187
Epoch 009 | loss=0.9552 | train_acc=0.6082 | val_loss=1.0191 | val_acc=0.6187
Epoch 010 | loss=0.9551 | train_acc=0.6082 | val_loss=1.0198 | val_acc=0.6187
Epoch 011 | loss=0.9544 | train_acc=0.6080 | val_loss=1.0202 | val_acc=0.6187
Epoch 012 | loss=0.9539 | train_acc=0.6084 | val_loss=1.0206 | val_acc=0.6187
Epoch 013 | loss=0.9536 | train_acc=0.6080 | val_loss=1.0209 | val_acc=0.6187
Epoch 014 | loss=0.9551 | train_acc=0.6089 | val_loss=1.0211 | val_acc=0.6187
Epoch 015 | loss=0.9543 | train_acc=0.6082 | val_loss=1.0211 | val_acc=0.6187
Epoch 016 | loss=0.9550 | train_acc=0.6081 | val_loss=1.0213 | val_acc=0.6187
Epoch 017 | loss=0.9551 | train_acc=0.6084 | val_loss=1.0214 | val_acc=0.6187
Epoch 018 | loss=0.9544 | train_acc=0.6083 | val_loss=1.0214 | val_acc=0.6187
Epoch 019 | loss=0.9549 | train_acc=0.6081 | val_loss=1.0213 | val_acc=0.6187
Epoch 020 | loss=0.9546 | train_acc=0.6082 | val_loss=1.0214 | val_acc=0.6187
Epoch 021 | loss=0.9546 | train_acc=0.6087 | val_loss=1.0214 | val_acc=0.6187
Epoch 022 | loss=0.9548 | train_acc=0.6085 | val_loss=1.0215 | val_acc=0.6187
Epoch 023 | loss=0.9559 | train_acc=0.6082 | val_loss=1.0214 | val_acc=0.6187
Epoch 024 | loss=0.9549 | train_acc=0.6084 | val_loss=1.0215 | val_acc=0.6187
Epoch 025 | loss=0.9552 | train_acc=0.6082 | val_loss=1.0215 | val_acc=0.6187
Epoch 026 | loss=0.9560 | train_acc=0.6079 | val_loss=1.0216 | val_acc=0.6187
Epoch 027 | loss=0.9558 | train_acc=0.6086 | val_loss=1.0217 | val_acc=0.6187
Epoch 028 | loss=0.9560 | train_acc=0.6082 | val_loss=1.0216 | val_acc=0.6187
Epoch 029 | loss=0.9563 | train_acc=0.6081 | val_loss=1.0216 | val_acc=0.6187
Epoch 030 | loss=0.9558 | train_acc=0.6080 | val_loss=1.0216 | val_acc=0.6187
Epoch 031 | loss=0.9564 | train_acc=0.6085 | val_loss=1.0215 | val_acc=0.6187
Epoch 032 | loss=0.9560 | train_acc=0.6086 | val_loss=1.0216 | val_acc=0.6187
Epoch 033 | loss=0.9559 | train_acc=0.6083 | val_loss=1.0216 | val_acc=0.6187
Epoch 034 | loss=0.9557 | train_acc=0.6086 | val_loss=1.0215 | val_acc=0.6187
Epoch 035 | loss=0.9552 | train_acc=0.6085 | val_loss=1.0215 | val_acc=0.6187
Epoch 036 | loss=0.9553 | train_acc=0.6079 | val_loss=1.0216 | val_acc=0.6187
Epoch 037 | loss=0.9554 | train_acc=0.6085 | val_loss=1.0217 | val_acc=0.6187
Epoch 038 | loss=0.9558 | train_acc=0.6081 | val_loss=1.0218 | val_acc=0.6187
Epoch 039 | loss=0.9562 | train_acc=0.6084 | val_loss=1.0219 | val_acc=0.6187
Epoch 040 | loss=0.9548 | train_acc=0.6068 | val_loss=1.0213 | val_acc=0.6187
Epoch 041 | loss=0.9562 | train_acc=0.6081 | val_loss=1.0211 | val_acc=0.6187
Epoch 042 | loss=0.9556 | train_acc=0.6087 | val_loss=1.0213 | val_acc=0.6187
Epoch 043 | loss=0.9558 | train_acc=0.6083 | val_loss=1.0220 | val_acc=0.6187
Epoch 044 | loss=0.9556 | train_acc=0.6082 | val_loss=1.0230 | val_acc=0.6187
Epoch 045 | loss=0.9554 | train_acc=0.6084 | val_loss=1.0230 | val_acc=0.6187
Epoch 046 | loss=0.9503 | train_acc=0.6169 | val_loss=1.2755 | val_acc=0.2951
Epoch 047 | loss=1.0281 | train_acc=0.6060 | val_loss=1.0245 | val_acc=0.6187
Epoch 048 | loss=0.9584 | train_acc=0.6081 | val_loss=1.0238 | val_acc=0.6187
Epoch 049 | loss=0.9560 | train_acc=0.6077 | val_loss=1.0249 | val_acc=0.6187
Final Test Loss: 0.9212 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=12.6034 | train_acc=0.5756 | val_loss=22.5015 | val_acc=0.1529
Epoch 001 | loss=5.1179 | train_acc=0.4685 | val_loss=1.0876 | val_acc=0.6187
Epoch 002 | loss=1.3778 | train_acc=0.5144 | val_loss=1.0705 | val_acc=0.6187
Epoch 003 | loss=1.1874 | train_acc=0.5667 | val_loss=1.0562 | val_acc=0.6187
Epoch 004 | loss=1.1128 | train_acc=0.5835 | val_loss=1.0445 | val_acc=0.6187
Epoch 005 | loss=1.0730 | train_acc=0.5931 | val_loss=1.0350 | val_acc=0.6187
Epoch 006 | loss=1.0522 | train_acc=0.5976 | val_loss=1.0276 | val_acc=0.6187
Epoch 007 | loss=1.0284 | train_acc=0.5992 | val_loss=1.0219 | val_acc=0.6187
Epoch 008 | loss=1.0157 | train_acc=0.6023 | val_loss=1.0175 | val_acc=0.6187
Epoch 009 | loss=1.0020 | train_acc=0.6030 | val_loss=1.0142 | val_acc=0.6187
Epoch 010 | loss=0.9924 | train_acc=0.6040 | val_loss=1.0118 | val_acc=0.6187
Epoch 011 | loss=0.9869 | train_acc=0.6052 | val_loss=1.0101 | val_acc=0.6187
Epoch 012 | loss=0.9793 | train_acc=0.6058 | val_loss=1.0089 | val_acc=0.6187
Epoch 013 | loss=0.9741 | train_acc=0.6064 | val_loss=1.0080 | val_acc=0.6187
Epoch 014 | loss=0.9692 | train_acc=0.6060 | val_loss=1.0075 | val_acc=0.6187
Epoch 015 | loss=0.9648 | train_acc=0.6064 | val_loss=1.0073 | val_acc=0.6187
Epoch 016 | loss=0.9630 | train_acc=0.6064 | val_loss=1.0073 | val_acc=0.6187
Epoch 017 | loss=0.9576 | train_acc=0.6065 | val_loss=1.0074 | val_acc=0.6187
Epoch 018 | loss=0.9561 | train_acc=0.6070 | val_loss=1.0077 | val_acc=0.6187
Epoch 019 | loss=0.9539 | train_acc=0.6072 | val_loss=1.0080 | val_acc=0.6187
Epoch 020 | loss=0.9516 | train_acc=0.6074 | val_loss=1.0085 | val_acc=0.6187
Epoch 021 | loss=0.9484 | train_acc=0.6072 | val_loss=1.0090 | val_acc=0.6187
Epoch 022 | loss=0.9484 | train_acc=0.6078 | val_loss=1.0096 | val_acc=0.6187
Epoch 023 | loss=0.9462 | train_acc=0.6073 | val_loss=1.0101 | val_acc=0.6187
Epoch 024 | loss=0.9451 | train_acc=0.6080 | val_loss=1.0107 | val_acc=0.6187
Epoch 025 | loss=0.9438 | train_acc=0.6074 | val_loss=1.0112 | val_acc=0.6187
Epoch 026 | loss=0.9426 | train_acc=0.6078 | val_loss=1.0119 | val_acc=0.6187
Epoch 027 | loss=0.9426 | train_acc=0.6079 | val_loss=1.0124 | val_acc=0.6187
Epoch 028 | loss=0.9414 | train_acc=0.6074 | val_loss=1.0130 | val_acc=0.6187
Epoch 029 | loss=0.9412 | train_acc=0.6076 | val_loss=1.0135 | val_acc=0.6187
Epoch 030 | loss=0.9417 | train_acc=0.6078 | val_loss=1.0142 | val_acc=0.6187
Epoch 031 | loss=0.9404 | train_acc=0.6076 | val_loss=1.0147 | val_acc=0.6187
Epoch 032 | loss=0.9382 | train_acc=0.6079 | val_loss=1.0152 | val_acc=0.6187
Epoch 033 | loss=0.9398 | train_acc=0.6080 | val_loss=1.0158 | val_acc=0.6187
Epoch 034 | loss=0.9388 | train_acc=0.6079 | val_loss=1.0162 | val_acc=0.6187
Epoch 035 | loss=0.9381 | train_acc=0.6076 | val_loss=1.0167 | val_acc=0.6187
Epoch 036 | loss=0.9409 | train_acc=0.6078 | val_loss=1.0172 | val_acc=0.6187
Epoch 037 | loss=0.9393 | train_acc=0.6077 | val_loss=1.0176 | val_acc=0.6187
Epoch 038 | loss=0.9405 | train_acc=0.6075 | val_loss=1.0181 | val_acc=0.6187
Epoch 039 | loss=0.9391 | train_acc=0.6083 | val_loss=1.0184 | val_acc=0.6187
Epoch 040 | loss=0.9366 | train_acc=0.6078 | val_loss=1.0188 | val_acc=0.6187
Epoch 041 | loss=0.9365 | train_acc=0.6079 | val_loss=1.0192 | val_acc=0.6187
Epoch 042 | loss=0.9358 | train_acc=0.6081 | val_loss=1.0196 | val_acc=0.6187
Epoch 043 | loss=0.9359 | train_acc=0.6082 | val_loss=1.0199 | val_acc=0.6187
Epoch 044 | loss=0.9360 | train_acc=0.6079 | val_loss=1.0203 | val_acc=0.6187
Epoch 045 | loss=0.9361 | train_acc=0.6084 | val_loss=1.0205 | val_acc=0.6187
Epoch 046 | loss=0.9354 | train_acc=0.6083 | val_loss=1.0208 | val_acc=0.6187
Epoch 047 | loss=0.9340 | train_acc=0.6080 | val_loss=1.0211 | val_acc=0.6187
Epoch 048 | loss=0.9346 | train_acc=0.6082 | val_loss=1.0213 | val_acc=0.6187
Epoch 049 | loss=0.9355 | train_acc=0.6080 | val_loss=1.0214 | val_acc=0.6187
Final Test Loss: 0.9150 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=3.5988 | train_acc=0.5826 | val_loss=1.4085 | val_acc=0.4047
Epoch 001 | loss=1.4428 | train_acc=0.5408 | val_loss=1.0411 | val_acc=0.6187
Epoch 002 | loss=1.0259 | train_acc=0.5963 | val_loss=1.0260 | val_acc=0.6187
Epoch 003 | loss=0.9995 | train_acc=0.6019 | val_loss=1.0180 | val_acc=0.6187
Epoch 004 | loss=0.9844 | train_acc=0.6040 | val_loss=1.0147 | val_acc=0.6187
Epoch 005 | loss=0.9715 | train_acc=0.6056 | val_loss=1.0138 | val_acc=0.6187
Epoch 006 | loss=0.9655 | train_acc=0.6059 | val_loss=1.0145 | val_acc=0.6187
Epoch 007 | loss=0.9601 | train_acc=0.6068 | val_loss=1.0159 | val_acc=0.6187
Epoch 008 | loss=0.9568 | train_acc=0.6067 | val_loss=1.0175 | val_acc=0.6187
Epoch 009 | loss=0.9548 | train_acc=0.6071 | val_loss=1.0190 | val_acc=0.6187
Epoch 010 | loss=0.9528 | train_acc=0.6071 | val_loss=1.0204 | val_acc=0.6187
Epoch 011 | loss=0.9511 | train_acc=0.6073 | val_loss=1.0215 | val_acc=0.6187
Epoch 012 | loss=0.9499 | train_acc=0.6078 | val_loss=1.0225 | val_acc=0.6187
Epoch 013 | loss=0.9494 | train_acc=0.6072 | val_loss=1.0233 | val_acc=0.6187
Epoch 014 | loss=0.9477 | train_acc=0.6075 | val_loss=1.0239 | val_acc=0.6187
Epoch 015 | loss=0.9489 | train_acc=0.6076 | val_loss=1.0245 | val_acc=0.6187
Epoch 016 | loss=0.9470 | train_acc=0.6080 | val_loss=1.0248 | val_acc=0.6187
Epoch 017 | loss=0.9470 | train_acc=0.6083 | val_loss=1.0251 | val_acc=0.6187
Epoch 018 | loss=0.9464 | train_acc=0.6080 | val_loss=1.0253 | val_acc=0.6187
Epoch 019 | loss=0.9477 | train_acc=0.6081 | val_loss=1.0254 | val_acc=0.6187
Epoch 020 | loss=0.9469 | train_acc=0.6077 | val_loss=1.0253 | val_acc=0.6187
Epoch 021 | loss=0.9460 | train_acc=0.6082 | val_loss=1.0253 | val_acc=0.6187
Epoch 022 | loss=0.9460 | train_acc=0.6080 | val_loss=1.0252 | val_acc=0.6187
Epoch 023 | loss=0.9466 | train_acc=0.6078 | val_loss=1.0250 | val_acc=0.6187
Epoch 024 | loss=0.9461 | train_acc=0.6080 | val_loss=1.0249 | val_acc=0.6187
Epoch 025 | loss=0.9458 | train_acc=0.6081 | val_loss=1.0246 | val_acc=0.6187
Epoch 026 | loss=0.9463 | train_acc=0.6076 | val_loss=1.0244 | val_acc=0.6187
Epoch 027 | loss=0.9458 | train_acc=0.6084 | val_loss=1.0242 | val_acc=0.6187
Epoch 028 | loss=0.9452 | train_acc=0.6077 | val_loss=1.0239 | val_acc=0.6187
Epoch 029 | loss=0.9455 | train_acc=0.6080 | val_loss=1.0236 | val_acc=0.6187
Epoch 030 | loss=0.9460 | train_acc=0.6080 | val_loss=1.0233 | val_acc=0.6187
Epoch 031 | loss=0.9446 | train_acc=0.6076 | val_loss=1.0230 | val_acc=0.6187
Epoch 032 | loss=0.9445 | train_acc=0.6079 | val_loss=1.0227 | val_acc=0.6187
Epoch 033 | loss=0.9449 | train_acc=0.6078 | val_loss=1.0223 | val_acc=0.6187
Epoch 034 | loss=0.9446 | train_acc=0.6081 | val_loss=1.0219 | val_acc=0.6187
Epoch 035 | loss=0.9446 | train_acc=0.6085 | val_loss=1.0217 | val_acc=0.6187
Epoch 036 | loss=0.9458 | train_acc=0.6082 | val_loss=1.0216 | val_acc=0.6187
Epoch 037 | loss=0.9442 | train_acc=0.6086 | val_loss=1.0211 | val_acc=0.6187
Epoch 038 | loss=0.9454 | train_acc=0.6083 | val_loss=1.0208 | val_acc=0.6187
Epoch 039 | loss=0.9446 | train_acc=0.6080 | val_loss=1.0207 | val_acc=0.6187
Epoch 040 | loss=0.9443 | train_acc=0.6087 | val_loss=1.0207 | val_acc=0.6187
Epoch 041 | loss=0.9439 | train_acc=0.6087 | val_loss=1.0204 | val_acc=0.6187
Epoch 042 | loss=0.9445 | train_acc=0.6079 | val_loss=1.0202 | val_acc=0.6187
Epoch 043 | loss=0.9440 | train_acc=0.6089 | val_loss=1.0204 | val_acc=0.6187
Epoch 044 | loss=0.9434 | train_acc=0.6086 | val_loss=1.0202 | val_acc=0.6187
Epoch 045 | loss=0.9441 | train_acc=0.6081 | val_loss=1.0200 | val_acc=0.6187
Epoch 046 | loss=0.9439 | train_acc=0.6081 | val_loss=1.0200 | val_acc=0.6187
Epoch 047 | loss=0.9455 | train_acc=0.6082 | val_loss=1.0209 | val_acc=0.6187
Epoch 048 | loss=0.9452 | train_acc=0.6080 | val_loss=1.0204 | val_acc=0.6187
Epoch 049 | loss=0.9434 | train_acc=0.6089 | val_loss=1.0202 | val_acc=0.6187
Final Test Loss: 0.9153 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=11.1538 | train_acc=0.6280 | val_loss=76.7926 | val_acc=0.1803
Epoch 001 | loss=11.4227 | train_acc=0.6096 | val_loss=24.5544 | val_acc=0.1803
Epoch 002 | loss=3.9803 | train_acc=0.5289 | val_loss=1.0808 | val_acc=0.5746
Epoch 003 | loss=1.4957 | train_acc=0.5194 | val_loss=1.0730 | val_acc=0.6187
Epoch 004 | loss=1.1607 | train_acc=0.5465 | val_loss=1.0574 | val_acc=0.6187
Epoch 005 | loss=1.0932 | train_acc=0.5837 | val_loss=1.0454 | val_acc=0.6187
Epoch 006 | loss=1.0657 | train_acc=0.5921 | val_loss=1.0357 | val_acc=0.6187
Epoch 007 | loss=1.0430 | train_acc=0.5973 | val_loss=1.0281 | val_acc=0.6187
Epoch 008 | loss=1.0245 | train_acc=0.6004 | val_loss=1.0221 | val_acc=0.6187
Epoch 009 | loss=1.0094 | train_acc=0.6020 | val_loss=1.0173 | val_acc=0.6187
Epoch 010 | loss=0.9986 | train_acc=0.6031 | val_loss=1.0136 | val_acc=0.6187
Epoch 011 | loss=0.9912 | train_acc=0.6044 | val_loss=1.0108 | val_acc=0.6187
Epoch 012 | loss=0.9833 | train_acc=0.6049 | val_loss=1.0087 | val_acc=0.6187
Epoch 013 | loss=0.9767 | train_acc=0.6049 | val_loss=1.0072 | val_acc=0.6187
Epoch 014 | loss=0.9705 | train_acc=0.6054 | val_loss=1.0061 | val_acc=0.6187
Epoch 015 | loss=0.9670 | train_acc=0.6064 | val_loss=1.0055 | val_acc=0.6187
Epoch 016 | loss=0.9645 | train_acc=0.6066 | val_loss=1.0052 | val_acc=0.6187
Epoch 017 | loss=0.9591 | train_acc=0.6071 | val_loss=1.0050 | val_acc=0.6187
Epoch 018 | loss=0.9561 | train_acc=0.6068 | val_loss=1.0050 | val_acc=0.6187
Epoch 019 | loss=0.9542 | train_acc=0.6066 | val_loss=1.0052 | val_acc=0.6187
Epoch 020 | loss=0.9521 | train_acc=0.6075 | val_loss=1.0055 | val_acc=0.6187
Epoch 021 | loss=0.9518 | train_acc=0.6069 | val_loss=1.0059 | val_acc=0.6187
Epoch 022 | loss=0.9484 | train_acc=0.6075 | val_loss=1.0063 | val_acc=0.6187
Epoch 023 | loss=0.9463 | train_acc=0.6074 | val_loss=1.0068 | val_acc=0.6187
Epoch 024 | loss=0.9450 | train_acc=0.6075 | val_loss=1.0074 | val_acc=0.6187
Epoch 025 | loss=0.9452 | train_acc=0.6074 | val_loss=1.0079 | val_acc=0.6187
Epoch 026 | loss=0.9434 | train_acc=0.6075 | val_loss=1.0085 | val_acc=0.6187
Epoch 027 | loss=0.9424 | train_acc=0.6073 | val_loss=1.0090 | val_acc=0.6187
Epoch 028 | loss=0.9415 | train_acc=0.6074 | val_loss=1.0096 | val_acc=0.6187
Epoch 029 | loss=0.9415 | train_acc=0.6078 | val_loss=1.0101 | val_acc=0.6187
Epoch 030 | loss=0.9401 | train_acc=0.6073 | val_loss=1.0106 | val_acc=0.6187
Epoch 031 | loss=0.9406 | train_acc=0.6072 | val_loss=1.0113 | val_acc=0.6187
Epoch 032 | loss=0.9390 | train_acc=0.6080 | val_loss=1.0118 | val_acc=0.6187
Epoch 033 | loss=0.9389 | train_acc=0.6075 | val_loss=1.0123 | val_acc=0.6187
Epoch 034 | loss=0.9367 | train_acc=0.6076 | val_loss=1.0126 | val_acc=0.6187
Epoch 035 | loss=0.9366 | train_acc=0.6081 | val_loss=1.0131 | val_acc=0.6187
Epoch 036 | loss=0.9366 | train_acc=0.6081 | val_loss=1.0136 | val_acc=0.6187
Epoch 037 | loss=0.9367 | train_acc=0.6072 | val_loss=1.0140 | val_acc=0.6187
Epoch 038 | loss=0.9356 | train_acc=0.6079 | val_loss=1.0144 | val_acc=0.6187
Epoch 039 | loss=0.9350 | train_acc=0.6081 | val_loss=1.0147 | val_acc=0.6187
Epoch 040 | loss=0.9352 | train_acc=0.6079 | val_loss=1.0150 | val_acc=0.6187
Epoch 041 | loss=0.9337 | train_acc=0.6087 | val_loss=1.0152 | val_acc=0.6187
Epoch 042 | loss=0.9351 | train_acc=0.6080 | val_loss=1.0152 | val_acc=0.6187
Epoch 043 | loss=0.9341 | train_acc=0.6082 | val_loss=1.0152 | val_acc=0.6187
Epoch 044 | loss=0.9329 | train_acc=0.6085 | val_loss=1.0157 | val_acc=0.6187
Epoch 045 | loss=0.9330 | train_acc=0.6077 | val_loss=1.0162 | val_acc=0.6187
Epoch 046 | loss=0.9328 | train_acc=0.6084 | val_loss=1.0160 | val_acc=0.6187
Epoch 047 | loss=0.9334 | train_acc=0.6082 | val_loss=1.0169 | val_acc=0.6187
Epoch 048 | loss=0.9329 | train_acc=0.6080 | val_loss=1.0175 | val_acc=0.6187
Epoch 049 | loss=0.9329 | train_acc=0.6079 | val_loss=1.0178 | val_acc=0.6187
Final Test Loss: 0.9149 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=3.6478 | train_acc=0.6558 | val_loss=6.5278 | val_acc=0.1922
Epoch 001 | loss=2.1968 | train_acc=0.5274 | val_loss=1.0600 | val_acc=0.6183
Epoch 002 | loss=1.0780 | train_acc=0.5766 | val_loss=1.0480 | val_acc=0.6187
Epoch 003 | loss=1.0361 | train_acc=0.5980 | val_loss=1.0370 | val_acc=0.6187
Epoch 004 | loss=1.0190 | train_acc=0.6031 | val_loss=1.0297 | val_acc=0.6187
Epoch 005 | loss=1.0053 | train_acc=0.6055 | val_loss=1.0237 | val_acc=0.6187
Epoch 006 | loss=0.9946 | train_acc=0.6066 | val_loss=1.0189 | val_acc=0.6187
Epoch 007 | loss=0.9867 | train_acc=0.6077 | val_loss=1.0155 | val_acc=0.6187
Epoch 008 | loss=0.9807 | train_acc=0.6078 | val_loss=1.0130 | val_acc=0.6187
Epoch 009 | loss=0.9739 | train_acc=0.6078 | val_loss=1.0111 | val_acc=0.6187
Epoch 010 | loss=0.9695 | train_acc=0.6077 | val_loss=1.0096 | val_acc=0.6187
Epoch 011 | loss=0.9654 | train_acc=0.6075 | val_loss=1.0086 | val_acc=0.6187
Epoch 012 | loss=0.9621 | train_acc=0.6079 | val_loss=1.0078 | val_acc=0.6187
Epoch 013 | loss=0.9588 | train_acc=0.6079 | val_loss=1.0072 | val_acc=0.6187
Epoch 014 | loss=0.9554 | train_acc=0.6083 | val_loss=1.0071 | val_acc=0.6187
Epoch 015 | loss=0.9524 | train_acc=0.6080 | val_loss=1.0069 | val_acc=0.6187
Epoch 016 | loss=0.9505 | train_acc=0.6079 | val_loss=1.0067 | val_acc=0.6187
Epoch 017 | loss=0.9485 | train_acc=0.6080 | val_loss=1.0067 | val_acc=0.6187
Epoch 018 | loss=0.9464 | train_acc=0.6085 | val_loss=1.0071 | val_acc=0.6187
Epoch 019 | loss=0.9449 | train_acc=0.6080 | val_loss=1.0073 | val_acc=0.6187
Epoch 020 | loss=0.9444 | train_acc=0.6080 | val_loss=1.0075 | val_acc=0.6187
Epoch 021 | loss=0.9432 | train_acc=0.6082 | val_loss=1.0080 | val_acc=0.6187
Epoch 022 | loss=0.9420 | train_acc=0.6078 | val_loss=1.0084 | val_acc=0.6187
Epoch 023 | loss=0.9399 | train_acc=0.6084 | val_loss=1.0088 | val_acc=0.6187
Epoch 024 | loss=0.9409 | train_acc=0.6083 | val_loss=1.0093 | val_acc=0.6187
Epoch 025 | loss=0.9397 | train_acc=0.6082 | val_loss=1.0098 | val_acc=0.6187
Epoch 026 | loss=0.9379 | train_acc=0.6083 | val_loss=1.0101 | val_acc=0.6187
Epoch 027 | loss=0.9389 | train_acc=0.6082 | val_loss=1.0105 | val_acc=0.6187
Epoch 028 | loss=0.9374 | train_acc=0.6075 | val_loss=1.0110 | val_acc=0.6187
Epoch 029 | loss=0.9373 | train_acc=0.6076 | val_loss=1.0112 | val_acc=0.6187
Epoch 030 | loss=0.9359 | train_acc=0.6085 | val_loss=1.0116 | val_acc=0.6187
Epoch 031 | loss=0.9374 | train_acc=0.6079 | val_loss=1.0116 | val_acc=0.6187
Epoch 032 | loss=0.9350 | train_acc=0.6083 | val_loss=1.0122 | val_acc=0.6187
Epoch 033 | loss=0.9351 | train_acc=0.6081 | val_loss=1.0124 | val_acc=0.6187
Epoch 034 | loss=0.9346 | train_acc=0.6082 | val_loss=1.0125 | val_acc=0.6187
Epoch 035 | loss=0.9345 | train_acc=0.6083 | val_loss=1.0121 | val_acc=0.6187
Epoch 036 | loss=0.9328 | train_acc=0.6082 | val_loss=1.0120 | val_acc=0.6187
Epoch 037 | loss=0.9338 | train_acc=0.6078 | val_loss=1.0142 | val_acc=0.6187
Epoch 038 | loss=0.9328 | train_acc=0.6088 | val_loss=1.0149 | val_acc=0.6187
Epoch 039 | loss=0.9359 | train_acc=0.6081 | val_loss=1.0149 | val_acc=0.6187
Epoch 040 | loss=1.0508 | train_acc=0.6083 | val_loss=1.0152 | val_acc=0.6187
Epoch 041 | loss=0.9585 | train_acc=0.6080 | val_loss=1.0156 | val_acc=0.6187
Epoch 042 | loss=1.0954 | train_acc=0.6154 | val_loss=3.8375 | val_acc=0.2436
Epoch 043 | loss=1.0351 | train_acc=0.6015 | val_loss=1.0179 | val_acc=0.6187
Epoch 044 | loss=0.9364 | train_acc=0.6081 | val_loss=1.0183 | val_acc=0.6187
Epoch 045 | loss=0.9371 | train_acc=0.6087 | val_loss=1.0188 | val_acc=0.6187
Epoch 046 | loss=0.9360 | train_acc=0.6083 | val_loss=1.0191 | val_acc=0.6187
Epoch 047 | loss=0.9394 | train_acc=0.6077 | val_loss=1.0194 | val_acc=0.6187
Epoch 048 | loss=0.9358 | train_acc=0.6076 | val_loss=1.0197 | val_acc=0.6187
Epoch 049 | loss=0.9352 | train_acc=0.6082 | val_loss=1.0200 | val_acc=0.6187
Final Test Loss: 0.9155 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=11.7495 | train_acc=0.6450 | val_loss=82.4236 | val_acc=0.1529
Epoch 001 | loss=13.2345 | train_acc=0.6094 | val_loss=42.6722 | val_acc=0.1803
Epoch 002 | loss=5.4721 | train_acc=0.5687 | val_loss=2.9346 | val_acc=0.1814
Epoch 003 | loss=1.5984 | train_acc=0.4928 | val_loss=1.0749 | val_acc=0.6146
Epoch 004 | loss=1.1424 | train_acc=0.5549 | val_loss=1.0599 | val_acc=0.6187
Epoch 005 | loss=1.0943 | train_acc=0.5838 | val_loss=1.0483 | val_acc=0.6187
Epoch 006 | loss=1.0628 | train_acc=0.5940 | val_loss=1.0390 | val_acc=0.6187
Epoch 007 | loss=1.0400 | train_acc=0.5978 | val_loss=1.0313 | val_acc=0.6187
Epoch 008 | loss=1.0243 | train_acc=0.6010 | val_loss=1.0252 | val_acc=0.6187
Epoch 009 | loss=1.0118 | train_acc=0.6022 | val_loss=1.0200 | val_acc=0.6187
Epoch 010 | loss=1.0007 | train_acc=0.6032 | val_loss=1.0160 | val_acc=0.6187
Epoch 011 | loss=0.9926 | train_acc=0.6048 | val_loss=1.0129 | val_acc=0.6187
Epoch 012 | loss=0.9842 | train_acc=0.6052 | val_loss=1.0106 | val_acc=0.6187
Epoch 013 | loss=0.9774 | train_acc=0.6058 | val_loss=1.0089 | val_acc=0.6187
Epoch 014 | loss=0.9738 | train_acc=0.6060 | val_loss=1.0076 | val_acc=0.6187
Epoch 015 | loss=0.9680 | train_acc=0.6059 | val_loss=1.0069 | val_acc=0.6187
Epoch 016 | loss=0.9645 | train_acc=0.6063 | val_loss=1.0062 | val_acc=0.6187
Epoch 017 | loss=0.9621 | train_acc=0.6065 | val_loss=1.0060 | val_acc=0.6187
Epoch 018 | loss=0.9589 | train_acc=0.6068 | val_loss=1.0059 | val_acc=0.6187
Epoch 019 | loss=0.9543 | train_acc=0.6070 | val_loss=1.0059 | val_acc=0.6187
Epoch 020 | loss=0.9529 | train_acc=0.6071 | val_loss=1.0061 | val_acc=0.6187
Epoch 021 | loss=0.9507 | train_acc=0.6068 | val_loss=1.0064 | val_acc=0.6187
Epoch 022 | loss=0.9504 | train_acc=0.6075 | val_loss=1.0068 | val_acc=0.6187
Epoch 023 | loss=0.9487 | train_acc=0.6073 | val_loss=1.0073 | val_acc=0.6187
Epoch 024 | loss=0.9470 | train_acc=0.6070 | val_loss=1.0078 | val_acc=0.6187
Epoch 025 | loss=0.9441 | train_acc=0.6078 | val_loss=1.0083 | val_acc=0.6187
Epoch 026 | loss=0.9432 | train_acc=0.6075 | val_loss=1.0087 | val_acc=0.6187
Epoch 027 | loss=0.9417 | train_acc=0.6074 | val_loss=1.0091 | val_acc=0.6187
Epoch 028 | loss=0.9414 | train_acc=0.6075 | val_loss=1.0096 | val_acc=0.6187
Epoch 029 | loss=0.9403 | train_acc=0.6082 | val_loss=1.0100 | val_acc=0.6187
Epoch 030 | loss=0.9393 | train_acc=0.6081 | val_loss=1.0107 | val_acc=0.6187
Epoch 031 | loss=0.9396 | train_acc=0.6076 | val_loss=1.0109 | val_acc=0.6187
Epoch 032 | loss=0.9383 | train_acc=0.6077 | val_loss=1.0113 | val_acc=0.6187
Epoch 033 | loss=0.9366 | train_acc=0.6078 | val_loss=1.0115 | val_acc=0.6187
Epoch 034 | loss=0.9382 | train_acc=0.6077 | val_loss=1.0120 | val_acc=0.6187
Epoch 035 | loss=0.9367 | train_acc=0.6080 | val_loss=1.0124 | val_acc=0.6187
Epoch 036 | loss=0.9360 | train_acc=0.6075 | val_loss=1.0125 | val_acc=0.6187
Epoch 037 | loss=0.9362 | train_acc=0.6073 | val_loss=1.0128 | val_acc=0.6187
Epoch 038 | loss=0.9348 | train_acc=0.6079 | val_loss=1.0129 | val_acc=0.6187
Epoch 039 | loss=0.9351 | train_acc=0.6079 | val_loss=1.0132 | val_acc=0.6187
Epoch 040 | loss=0.9349 | train_acc=0.6078 | val_loss=1.0134 | val_acc=0.6187
Epoch 041 | loss=0.9343 | train_acc=0.6081 | val_loss=1.0140 | val_acc=0.6187
Epoch 042 | loss=0.9336 | train_acc=0.6081 | val_loss=1.0142 | val_acc=0.6187
Epoch 043 | loss=0.9336 | train_acc=0.6084 | val_loss=1.0149 | val_acc=0.6187
Epoch 044 | loss=0.9343 | train_acc=0.6076 | val_loss=1.0152 | val_acc=0.6187
Epoch 045 | loss=0.9341 | train_acc=0.6080 | val_loss=1.0159 | val_acc=0.6187
Epoch 046 | loss=0.9325 | train_acc=0.6084 | val_loss=1.0162 | val_acc=0.6187
Epoch 047 | loss=0.9339 | train_acc=0.6081 | val_loss=1.0162 | val_acc=0.6187
Epoch 048 | loss=0.9318 | train_acc=0.6076 | val_loss=1.0172 | val_acc=0.6187
Epoch 049 | loss=0.9320 | train_acc=0.6080 | val_loss=1.0188 | val_acc=0.6187
Final Test Loss: 0.9169 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=6.0801 | train_acc=0.6448 | val_loss=9.2472 | val_acc=0.1873
Epoch 001 | loss=2.5222 | train_acc=0.5251 | val_loss=1.0718 | val_acc=0.6183
Epoch 002 | loss=1.1453 | train_acc=0.5636 | val_loss=1.0608 | val_acc=0.6187
Epoch 003 | loss=1.0569 | train_acc=0.5816 | val_loss=1.0496 | val_acc=0.6187
Epoch 004 | loss=1.1072 | train_acc=0.5993 | val_loss=1.0403 | val_acc=0.6187
Epoch 005 | loss=1.0195 | train_acc=0.5986 | val_loss=1.0327 | val_acc=0.6187
Epoch 006 | loss=1.0060 | train_acc=0.6033 | val_loss=1.0271 | val_acc=0.6187
Epoch 007 | loss=0.9961 | train_acc=0.6055 | val_loss=1.0224 | val_acc=0.6187
Epoch 008 | loss=0.9882 | train_acc=0.6065 | val_loss=1.0182 | val_acc=0.6187
Epoch 009 | loss=0.9810 | train_acc=0.6072 | val_loss=1.0157 | val_acc=0.6187
Epoch 010 | loss=0.9756 | train_acc=0.6073 | val_loss=1.0135 | val_acc=0.6187
Epoch 011 | loss=0.9709 | train_acc=0.6077 | val_loss=1.0110 | val_acc=0.6187
Epoch 012 | loss=0.9659 | train_acc=0.6072 | val_loss=1.0085 | val_acc=0.6187
Epoch 013 | loss=0.9626 | train_acc=0.6073 | val_loss=1.0085 | val_acc=0.6187
Epoch 014 | loss=0.9593 | train_acc=0.6082 | val_loss=1.0078 | val_acc=0.6187
Epoch 015 | loss=0.9558 | train_acc=0.6080 | val_loss=1.0073 | val_acc=0.6187
Epoch 016 | loss=0.9544 | train_acc=0.6078 | val_loss=1.0070 | val_acc=0.6187
Epoch 017 | loss=0.9515 | train_acc=0.6078 | val_loss=1.0076 | val_acc=0.6187
Epoch 018 | loss=0.9491 | train_acc=0.6083 | val_loss=1.0075 | val_acc=0.6187
Epoch 019 | loss=0.9471 | train_acc=0.6085 | val_loss=1.0073 | val_acc=0.6187
Epoch 020 | loss=0.9460 | train_acc=0.6084 | val_loss=1.0069 | val_acc=0.6187
Epoch 021 | loss=0.9440 | train_acc=0.6084 | val_loss=1.0065 | val_acc=0.6187
Epoch 022 | loss=0.9429 | train_acc=0.6086 | val_loss=1.0069 | val_acc=0.6187
Epoch 023 | loss=0.9414 | train_acc=0.6086 | val_loss=1.0070 | val_acc=0.6187
Epoch 024 | loss=0.9413 | train_acc=0.6080 | val_loss=1.0074 | val_acc=0.6187
Epoch 025 | loss=0.9391 | train_acc=0.6082 | val_loss=1.0075 | val_acc=0.6187
Epoch 026 | loss=0.9996 | train_acc=0.6080 | val_loss=1.0059 | val_acc=0.6187
Epoch 027 | loss=0.9449 | train_acc=0.6082 | val_loss=1.0074 | val_acc=0.6187
Epoch 028 | loss=0.9385 | train_acc=0.6081 | val_loss=1.0088 | val_acc=0.6187
Epoch 029 | loss=0.9406 | train_acc=0.6082 | val_loss=1.0091 | val_acc=0.6187
Epoch 030 | loss=0.9372 | train_acc=0.6088 | val_loss=1.0088 | val_acc=0.6187
Epoch 031 | loss=0.9368 | train_acc=0.6085 | val_loss=1.0082 | val_acc=0.6187
Epoch 032 | loss=0.9352 | train_acc=0.6084 | val_loss=1.0081 | val_acc=0.6187
Epoch 033 | loss=0.9356 | train_acc=0.6084 | val_loss=1.0095 | val_acc=0.6187
Epoch 034 | loss=0.9341 | train_acc=0.6086 | val_loss=1.0097 | val_acc=0.6187
Epoch 035 | loss=0.9347 | train_acc=0.6078 | val_loss=1.0085 | val_acc=0.6187
Epoch 036 | loss=0.9338 | train_acc=0.6078 | val_loss=1.0069 | val_acc=0.6187
Epoch 037 | loss=0.9338 | train_acc=0.6083 | val_loss=1.0114 | val_acc=0.6187
Epoch 038 | loss=0.9336 | train_acc=0.6085 | val_loss=1.0130 | val_acc=0.6187
Epoch 039 | loss=0.9331 | train_acc=0.6076 | val_loss=1.0097 | val_acc=0.6187
Epoch 040 | loss=0.9321 | train_acc=0.6088 | val_loss=1.0103 | val_acc=0.6187
Epoch 041 | loss=0.9307 | train_acc=0.6082 | val_loss=1.0087 | val_acc=0.6187
Epoch 042 | loss=0.9311 | train_acc=0.6085 | val_loss=1.0122 | val_acc=0.6187
Epoch 043 | loss=0.9317 | train_acc=0.6089 | val_loss=1.0124 | val_acc=0.6187
Epoch 044 | loss=0.9316 | train_acc=0.6083 | val_loss=1.0130 | val_acc=0.6187
Epoch 045 | loss=0.9307 | train_acc=0.6077 | val_loss=1.0116 | val_acc=0.6187
Epoch 046 | loss=0.9306 | train_acc=0.6082 | val_loss=1.0141 | val_acc=0.6187
Epoch 047 | loss=0.9305 | train_acc=0.6083 | val_loss=1.0127 | val_acc=0.6187
Epoch 048 | loss=0.9286 | train_acc=0.6082 | val_loss=1.0133 | val_acc=0.6187
Epoch 049 | loss=0.9281 | train_acc=0.6089 | val_loss=1.0128 | val_acc=0.6187
Final Test Loss: 0.9099 | Test Accuracy: 0.6316
