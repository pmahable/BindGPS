## SLURM PROLOG ###############################################################
##    Job ID : 13965784
##  Job Name : sweep_runs
##  Nodelist : gpu2007
##      CPUs : 1
##  Mem/Node : 32000 MB
## Directory : /oscar/data/larschan/shared_data/BindGPS/model
##   Job Started : Mon Nov 10 13:05:10 EST 2025
###############################################################################
Running model parameter sweep with GAT (NO SVM)
Create sweep with ID: ivpdewin
Sweep URL: https://wandb.ai/bind-gps/gps-gat-model-no-svm-parameter-test2/sweeps/ivpdewin
Created sweep: ivpdewin
Starting 24 runs...
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.9496 | train_acc=0.6520 | val_loss=3.1971 | val_acc=0.1873
Epoch 001 | loss=1.2911 | train_acc=0.5381 | val_loss=1.0910 | val_acc=0.4017
Epoch 002 | loss=1.1567 | train_acc=0.5710 | val_loss=1.0502 | val_acc=0.6187
Epoch 003 | loss=1.0352 | train_acc=0.5810 | val_loss=1.0421 | val_acc=0.6187
Epoch 004 | loss=1.0278 | train_acc=0.5960 | val_loss=1.0228 | val_acc=0.6187
Epoch 005 | loss=0.9970 | train_acc=0.6011 | val_loss=1.0133 | val_acc=0.6187
Epoch 006 | loss=0.9784 | train_acc=0.6030 | val_loss=1.0120 | val_acc=0.6187
Epoch 007 | loss=0.9684 | train_acc=0.6028 | val_loss=1.0131 | val_acc=0.6187
Epoch 008 | loss=0.9618 | train_acc=0.6032 | val_loss=1.0154 | val_acc=0.6187
Epoch 009 | loss=0.9622 | train_acc=0.6036 | val_loss=1.0172 | val_acc=0.6187
Epoch 010 | loss=0.9596 | train_acc=0.6021 | val_loss=1.0186 | val_acc=0.6187
Epoch 011 | loss=0.9557 | train_acc=0.6024 | val_loss=1.0193 | val_acc=0.6187
Epoch 012 | loss=0.9580 | train_acc=0.6015 | val_loss=1.0190 | val_acc=0.6187
Epoch 013 | loss=0.9558 | train_acc=0.6015 | val_loss=1.0166 | val_acc=0.6187
Epoch 014 | loss=0.9281 | train_acc=0.5998 | val_loss=0.9711 | val_acc=0.5998
Epoch 015 | loss=0.9773 | train_acc=0.5947 | val_loss=1.0099 | val_acc=0.6187
Epoch 016 | loss=0.9204 | train_acc=0.6108 | val_loss=1.8485 | val_acc=0.4554
Epoch 017 | loss=1.0919 | train_acc=0.5981 | val_loss=1.6342 | val_acc=0.4221
Epoch 018 | loss=1.0349 | train_acc=0.5892 | val_loss=1.0233 | val_acc=0.6157
Epoch 019 | loss=1.1511 | train_acc=0.6473 | val_loss=1.2639 | val_acc=0.4780
Epoch 020 | loss=1.0357 | train_acc=0.5820 | val_loss=1.0213 | val_acc=0.6187
Epoch 021 | loss=1.0034 | train_acc=0.6007 | val_loss=1.0002 | val_acc=0.6187
Epoch 022 | loss=1.0178 | train_acc=0.5989 | val_loss=1.0140 | val_acc=0.5628
Epoch 023 | loss=0.9805 | train_acc=0.6002 | val_loss=0.9804 | val_acc=0.6187
Epoch 024 | loss=0.9570 | train_acc=0.6024 | val_loss=0.9598 | val_acc=0.6187
Epoch 025 | loss=0.9849 | train_acc=0.6012 | val_loss=0.9671 | val_acc=0.5768
Epoch 026 | loss=0.9561 | train_acc=0.5997 | val_loss=0.9215 | val_acc=0.6187
Epoch 027 | loss=0.9424 | train_acc=0.6023 | val_loss=0.9108 | val_acc=0.6187
Epoch 028 | loss=0.9259 | train_acc=0.6021 | val_loss=0.9207 | val_acc=0.6187
Epoch 029 | loss=0.9238 | train_acc=0.6017 | val_loss=0.9216 | val_acc=0.6187
Epoch 030 | loss=0.9203 | train_acc=0.6013 | val_loss=0.9249 | val_acc=0.6187
Epoch 031 | loss=0.9203 | train_acc=0.6021 | val_loss=0.9223 | val_acc=0.6187
Epoch 032 | loss=0.9175 | train_acc=0.6022 | val_loss=0.9190 | val_acc=0.6187
Epoch 033 | loss=0.9150 | train_acc=0.6028 | val_loss=0.9280 | val_acc=0.6187
Epoch 034 | loss=0.9195 | train_acc=0.6013 | val_loss=0.9313 | val_acc=0.6187
Epoch 035 | loss=0.9195 | train_acc=0.6031 | val_loss=0.9266 | val_acc=0.6187
Epoch 036 | loss=0.9210 | train_acc=0.6022 | val_loss=0.9236 | val_acc=0.6187
Epoch 037 | loss=0.9166 | train_acc=0.6027 | val_loss=0.9273 | val_acc=0.6187
Epoch 038 | loss=0.9165 | train_acc=0.6035 | val_loss=0.9240 | val_acc=0.6187
Epoch 039 | loss=0.9160 | train_acc=0.6019 | val_loss=0.9335 | val_acc=0.6187
Epoch 040 | loss=0.9141 | train_acc=0.6021 | val_loss=0.9162 | val_acc=0.6187
Epoch 041 | loss=0.9156 | train_acc=0.6021 | val_loss=0.9208 | val_acc=0.6187
Epoch 042 | loss=0.9138 | train_acc=0.6026 | val_loss=0.9159 | val_acc=0.6187
Epoch 043 | loss=0.9150 | train_acc=0.6026 | val_loss=0.9380 | val_acc=0.6187
Epoch 044 | loss=0.9174 | train_acc=0.6026 | val_loss=0.9268 | val_acc=0.6187
Epoch 045 | loss=0.9137 | train_acc=0.6021 | val_loss=0.9216 | val_acc=0.6187
Epoch 046 | loss=0.9121 | train_acc=0.6023 | val_loss=0.9312 | val_acc=0.6187
Epoch 047 | loss=0.9127 | train_acc=0.6026 | val_loss=0.9285 | val_acc=0.6187
Epoch 048 | loss=0.9129 | train_acc=0.6012 | val_loss=0.9142 | val_acc=0.6187
Epoch 049 | loss=0.9168 | train_acc=0.6021 | val_loss=0.9271 | val_acc=0.6187
Final Test Loss: 0.8229 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=2.2787 | train_acc=0.6616 | val_loss=7.0895 | val_acc=0.1866
Epoch 001 | loss=1.6840 | train_acc=0.6072 | val_loss=3.5739 | val_acc=0.1947
Epoch 002 | loss=1.2348 | train_acc=0.5807 | val_loss=1.2942 | val_acc=0.2388
Epoch 003 | loss=1.1371 | train_acc=0.5765 | val_loss=1.0515 | val_acc=0.6187
Epoch 004 | loss=1.1189 | train_acc=0.5846 | val_loss=1.0498 | val_acc=0.6187
Epoch 005 | loss=1.1483 | train_acc=0.5924 | val_loss=1.0404 | val_acc=0.6168
Epoch 006 | loss=1.1768 | train_acc=0.5764 | val_loss=1.0440 | val_acc=0.5191
Epoch 007 | loss=1.1695 | train_acc=0.5753 | val_loss=1.0313 | val_acc=0.5976
Epoch 008 | loss=1.0285 | train_acc=0.5863 | val_loss=1.0211 | val_acc=0.6187
Epoch 009 | loss=0.9953 | train_acc=0.6066 | val_loss=1.0139 | val_acc=0.6187
Epoch 010 | loss=0.9791 | train_acc=0.6080 | val_loss=1.0124 | val_acc=0.6187
Epoch 011 | loss=0.9684 | train_acc=0.6084 | val_loss=1.0139 | val_acc=0.6187
Epoch 012 | loss=0.9627 | train_acc=0.6077 | val_loss=1.0162 | val_acc=0.6187
Epoch 013 | loss=0.9589 | train_acc=0.6080 | val_loss=1.0185 | val_acc=0.6187
Epoch 014 | loss=0.9564 | train_acc=0.6085 | val_loss=1.0205 | val_acc=0.6187
Epoch 015 | loss=0.9551 | train_acc=0.6080 | val_loss=1.0219 | val_acc=0.6187
Epoch 016 | loss=0.9537 | train_acc=0.6081 | val_loss=1.0231 | val_acc=0.6187
Epoch 017 | loss=0.9530 | train_acc=0.6086 | val_loss=1.0240 | val_acc=0.6187
Epoch 018 | loss=0.9530 | train_acc=0.6081 | val_loss=1.0245 | val_acc=0.6187
Epoch 019 | loss=0.9522 | train_acc=0.6084 | val_loss=1.0250 | val_acc=0.6187
Epoch 020 | loss=0.9525 | train_acc=0.6082 | val_loss=1.0252 | val_acc=0.6187
Epoch 021 | loss=0.9515 | train_acc=0.6080 | val_loss=1.0255 | val_acc=0.6187
Epoch 022 | loss=0.9509 | train_acc=0.6084 | val_loss=1.0257 | val_acc=0.6187
Epoch 023 | loss=0.9511 | train_acc=0.6083 | val_loss=1.0258 | val_acc=0.6187
Epoch 024 | loss=0.9509 | train_acc=0.6083 | val_loss=1.0255 | val_acc=0.6187
Epoch 025 | loss=0.9524 | train_acc=0.6083 | val_loss=1.0247 | val_acc=0.6187
Epoch 026 | loss=0.9511 | train_acc=0.6082 | val_loss=1.0247 | val_acc=0.6187
Epoch 027 | loss=0.9503 | train_acc=0.6080 | val_loss=1.0243 | val_acc=0.6187
Epoch 028 | loss=0.9499 | train_acc=0.6086 | val_loss=1.0246 | val_acc=0.6187
Epoch 029 | loss=0.9442 | train_acc=0.6084 | val_loss=0.9625 | val_acc=0.6183
Epoch 030 | loss=0.9692 | train_acc=0.6146 | val_loss=1.5286 | val_acc=0.4328
Epoch 031 | loss=0.9975 | train_acc=0.6517 | val_loss=1.7223 | val_acc=0.4372
Epoch 032 | loss=1.1767 | train_acc=0.6193 | val_loss=4.2292 | val_acc=0.2184
Epoch 033 | loss=1.3595 | train_acc=0.6137 | val_loss=1.1511 | val_acc=0.5257
Epoch 034 | loss=1.1850 | train_acc=0.5958 | val_loss=1.0005 | val_acc=0.6183
Epoch 035 | loss=1.0248 | train_acc=0.6064 | val_loss=1.0004 | val_acc=0.6187
Epoch 036 | loss=1.0332 | train_acc=0.6060 | val_loss=0.9911 | val_acc=0.6187
Epoch 037 | loss=1.0342 | train_acc=0.6073 | val_loss=1.0006 | val_acc=0.6005
Epoch 038 | loss=0.9696 | train_acc=0.6049 | val_loss=0.9715 | val_acc=0.6187
Epoch 039 | loss=0.9467 | train_acc=0.6074 | val_loss=0.9599 | val_acc=0.6187
Epoch 040 | loss=0.9307 | train_acc=0.6070 | val_loss=0.9648 | val_acc=0.6187
Epoch 041 | loss=0.9284 | train_acc=0.6084 | val_loss=0.9553 | val_acc=0.6187
Epoch 042 | loss=0.9210 | train_acc=0.6075 | val_loss=0.9627 | val_acc=0.6187
Epoch 043 | loss=0.9230 | train_acc=0.6082 | val_loss=0.9665 | val_acc=0.6187
Epoch 044 | loss=0.9212 | train_acc=0.6084 | val_loss=0.9578 | val_acc=0.6187
Epoch 045 | loss=0.9162 | train_acc=0.6087 | val_loss=0.9591 | val_acc=0.6187
Epoch 046 | loss=0.9162 | train_acc=0.6077 | val_loss=0.9568 | val_acc=0.6187
Epoch 047 | loss=0.9149 | train_acc=0.6079 | val_loss=0.9584 | val_acc=0.6187
Epoch 048 | loss=0.9149 | train_acc=0.6080 | val_loss=0.9672 | val_acc=0.6187
Epoch 049 | loss=0.9134 | train_acc=0.6080 | val_loss=0.9645 | val_acc=0.6187
Final Test Loss: 0.8870 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.8863 | train_acc=0.6640 | val_loss=4.0789 | val_acc=0.1870
Epoch 001 | loss=1.3034 | train_acc=0.5585 | val_loss=1.1002 | val_acc=0.2721
Epoch 002 | loss=1.1201 | train_acc=0.5808 | val_loss=1.0476 | val_acc=0.6187
Epoch 003 | loss=1.0951 | train_acc=0.6083 | val_loss=1.0962 | val_acc=0.4058
Epoch 004 | loss=1.0997 | train_acc=0.5785 | val_loss=1.0363 | val_acc=0.6187
Epoch 005 | loss=1.0248 | train_acc=0.6001 | val_loss=1.0186 | val_acc=0.6187
Epoch 006 | loss=0.9926 | train_acc=0.6085 | val_loss=1.0120 | val_acc=0.6187
Epoch 007 | loss=0.9741 | train_acc=0.6072 | val_loss=1.0124 | val_acc=0.6187
Epoch 008 | loss=0.9717 | train_acc=0.6060 | val_loss=1.0120 | val_acc=0.6187
Epoch 009 | loss=0.9707 | train_acc=0.6091 | val_loss=1.0127 | val_acc=0.6187
Epoch 010 | loss=0.9597 | train_acc=0.6097 | val_loss=1.0104 | val_acc=0.6187
Epoch 011 | loss=0.9558 | train_acc=0.6094 | val_loss=1.0099 | val_acc=0.6187
Epoch 012 | loss=0.9709 | train_acc=0.6633 | val_loss=3.4114 | val_acc=0.2795
Epoch 013 | loss=0.9740 | train_acc=0.6531 | val_loss=2.0153 | val_acc=0.3491
Epoch 014 | loss=1.0611 | train_acc=0.5987 | val_loss=0.9987 | val_acc=0.5039
Epoch 015 | loss=0.9198 | train_acc=0.5973 | val_loss=1.0248 | val_acc=0.5109
Epoch 016 | loss=1.0412 | train_acc=0.5991 | val_loss=0.9749 | val_acc=0.5994
Epoch 017 | loss=0.9418 | train_acc=0.5992 | val_loss=0.9937 | val_acc=0.6187
Epoch 018 | loss=0.9995 | train_acc=0.6083 | val_loss=0.9551 | val_acc=0.6183
Epoch 019 | loss=0.9126 | train_acc=0.6077 | val_loss=0.9852 | val_acc=0.6175
Epoch 020 | loss=0.9425 | train_acc=0.6082 | val_loss=0.9300 | val_acc=0.6187
Epoch 021 | loss=0.8861 | train_acc=0.6090 | val_loss=0.9274 | val_acc=0.6175
Epoch 022 | loss=0.8841 | train_acc=0.6092 | val_loss=0.9242 | val_acc=0.6187
Epoch 023 | loss=0.8738 | train_acc=0.6095 | val_loss=0.9414 | val_acc=0.6168
Epoch 024 | loss=0.8876 | train_acc=0.6094 | val_loss=0.9232 | val_acc=0.6187
Epoch 025 | loss=0.8717 | train_acc=0.6093 | val_loss=0.9285 | val_acc=0.6187
Epoch 026 | loss=0.8742 | train_acc=0.6091 | val_loss=0.9026 | val_acc=0.6187
Epoch 027 | loss=0.8582 | train_acc=0.6094 | val_loss=0.9141 | val_acc=0.6187
Epoch 028 | loss=0.8655 | train_acc=0.6093 | val_loss=0.8803 | val_acc=0.6187
Epoch 029 | loss=0.8501 | train_acc=0.6096 | val_loss=0.8927 | val_acc=0.6187
Epoch 030 | loss=0.8596 | train_acc=0.6100 | val_loss=0.8790 | val_acc=0.6187
Epoch 031 | loss=0.8515 | train_acc=0.6092 | val_loss=0.9186 | val_acc=0.6187
Epoch 032 | loss=0.8715 | train_acc=0.6097 | val_loss=0.9055 | val_acc=0.6187
Epoch 033 | loss=0.8607 | train_acc=0.6094 | val_loss=0.8782 | val_acc=0.6187
Epoch 034 | loss=0.8496 | train_acc=0.6096 | val_loss=0.8734 | val_acc=0.6187
Epoch 035 | loss=0.8466 | train_acc=0.6097 | val_loss=0.9208 | val_acc=0.6187
Epoch 036 | loss=0.8503 | train_acc=0.6094 | val_loss=0.8535 | val_acc=0.6187
Epoch 037 | loss=0.8362 | train_acc=0.6092 | val_loss=0.9194 | val_acc=0.6187
Epoch 038 | loss=0.8518 | train_acc=0.6097 | val_loss=0.8562 | val_acc=0.6187
Epoch 039 | loss=0.8490 | train_acc=0.6081 | val_loss=0.8659 | val_acc=0.6187
Epoch 040 | loss=0.8756 | train_acc=0.6077 | val_loss=0.8622 | val_acc=0.6187
Epoch 041 | loss=0.8685 | train_acc=0.6074 | val_loss=0.9831 | val_acc=0.6194
Epoch 042 | loss=0.9426 | train_acc=0.6087 | val_loss=0.8775 | val_acc=0.6187
Epoch 043 | loss=0.9001 | train_acc=0.6081 | val_loss=1.0088 | val_acc=0.6187
Epoch 044 | loss=0.8874 | train_acc=0.6096 | val_loss=0.9054 | val_acc=0.6187
Epoch 045 | loss=0.8748 | train_acc=0.6086 | val_loss=0.8339 | val_acc=0.6187
Epoch 046 | loss=0.8448 | train_acc=0.6092 | val_loss=0.8476 | val_acc=0.6201
Epoch 047 | loss=0.8592 | train_acc=0.6083 | val_loss=0.8600 | val_acc=0.6187
Epoch 048 | loss=0.8502 | train_acc=0.6094 | val_loss=0.8324 | val_acc=0.6187
Epoch 049 | loss=0.8386 | train_acc=0.6091 | val_loss=0.8467 | val_acc=0.6187
Final Test Loss: 0.7705 | Test Accuracy: 0.6322
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=2.5060 | train_acc=0.6650 | val_loss=2.7410 | val_acc=0.1866
Epoch 001 | loss=1.2990 | train_acc=0.6290 | val_loss=1.7667 | val_acc=0.2033
Epoch 002 | loss=1.2564 | train_acc=0.5405 | val_loss=1.0667 | val_acc=0.6024
Epoch 003 | loss=1.1542 | train_acc=0.5665 | val_loss=1.0625 | val_acc=0.5320
Epoch 004 | loss=1.0944 | train_acc=0.5881 | val_loss=1.0394 | val_acc=0.6187
Epoch 005 | loss=1.0641 | train_acc=0.5992 | val_loss=1.0311 | val_acc=0.6187
Epoch 006 | loss=1.0268 | train_acc=0.6026 | val_loss=1.0248 | val_acc=0.6187
Epoch 007 | loss=1.0042 | train_acc=0.6052 | val_loss=0.9961 | val_acc=0.6187
Epoch 008 | loss=0.9585 | train_acc=0.6081 | val_loss=1.0137 | val_acc=0.6168
Epoch 009 | loss=0.9843 | train_acc=0.6066 | val_loss=0.9869 | val_acc=0.6187
Epoch 010 | loss=0.9170 | train_acc=0.6092 | val_loss=0.9950 | val_acc=0.6064
Epoch 011 | loss=0.9186 | train_acc=0.6088 | val_loss=0.9549 | val_acc=0.6187
Epoch 012 | loss=0.8728 | train_acc=0.6079 | val_loss=0.9566 | val_acc=0.6187
Epoch 013 | loss=0.8558 | train_acc=0.6092 | val_loss=0.9692 | val_acc=0.6142
Epoch 014 | loss=0.8619 | train_acc=0.6086 | val_loss=0.9471 | val_acc=0.6187
Epoch 015 | loss=0.8421 | train_acc=0.6093 | val_loss=0.9608 | val_acc=0.6183
Epoch 016 | loss=0.8492 | train_acc=0.6096 | val_loss=0.9560 | val_acc=0.6187
Epoch 017 | loss=0.8394 | train_acc=0.6095 | val_loss=0.9339 | val_acc=0.6179
Epoch 018 | loss=0.8370 | train_acc=0.6096 | val_loss=0.9302 | val_acc=0.6164
Epoch 019 | loss=0.8289 | train_acc=0.6094 | val_loss=0.9313 | val_acc=0.6168
Epoch 020 | loss=0.8262 | train_acc=0.6089 | val_loss=0.9204 | val_acc=0.6187
Epoch 021 | loss=0.8164 | train_acc=0.6096 | val_loss=0.9100 | val_acc=0.6187
Epoch 022 | loss=0.8143 | train_acc=0.6094 | val_loss=0.9144 | val_acc=0.6187
Epoch 023 | loss=0.8138 | train_acc=0.6088 | val_loss=0.8995 | val_acc=0.6187
Epoch 024 | loss=0.8033 | train_acc=0.6095 | val_loss=0.8893 | val_acc=0.6187
Epoch 025 | loss=0.8004 | train_acc=0.6095 | val_loss=0.9033 | val_acc=0.6187
Epoch 026 | loss=0.8026 | train_acc=0.6094 | val_loss=0.8884 | val_acc=0.6187
Epoch 027 | loss=0.7892 | train_acc=0.6096 | val_loss=0.8872 | val_acc=0.6187
Epoch 028 | loss=0.7813 | train_acc=0.6098 | val_loss=0.8691 | val_acc=0.6175
Epoch 029 | loss=0.7813 | train_acc=0.6093 | val_loss=0.8524 | val_acc=0.6187
Epoch 030 | loss=0.7705 | train_acc=0.6096 | val_loss=0.8482 | val_acc=0.6187
Epoch 031 | loss=0.7695 | train_acc=0.6099 | val_loss=0.8749 | val_acc=0.6187
Epoch 032 | loss=0.7744 | train_acc=0.6093 | val_loss=0.8779 | val_acc=0.6187
Epoch 033 | loss=0.7703 | train_acc=0.6091 | val_loss=0.8434 | val_acc=0.6187
Epoch 034 | loss=0.7610 | train_acc=0.6097 | val_loss=0.8510 | val_acc=0.6187
Epoch 035 | loss=0.7642 | train_acc=0.6096 | val_loss=0.8487 | val_acc=0.6187
Epoch 036 | loss=0.7689 | train_acc=0.6098 | val_loss=0.8685 | val_acc=0.6187
Epoch 037 | loss=0.7700 | train_acc=0.6094 | val_loss=0.8721 | val_acc=0.6187
Epoch 038 | loss=0.7690 | train_acc=0.6090 | val_loss=0.8568 | val_acc=0.6187
Epoch 039 | loss=0.7695 | train_acc=0.6096 | val_loss=0.8637 | val_acc=0.6187
Epoch 040 | loss=0.7648 | train_acc=0.6096 | val_loss=0.8539 | val_acc=0.6187
Epoch 041 | loss=0.7658 | train_acc=0.6099 | val_loss=0.8430 | val_acc=0.6190
Epoch 042 | loss=0.7567 | train_acc=0.6071 | val_loss=0.8616 | val_acc=0.6187
Epoch 043 | loss=0.7668 | train_acc=0.6091 | val_loss=0.8557 | val_acc=0.6187
Epoch 044 | loss=0.7615 | train_acc=0.6085 | val_loss=0.8463 | val_acc=0.6187
Epoch 045 | loss=0.7538 | train_acc=0.5938 | val_loss=0.8410 | val_acc=0.6716
Epoch 046 | loss=0.7570 | train_acc=0.5948 | val_loss=0.8354 | val_acc=0.6646
Epoch 047 | loss=0.7524 | train_acc=0.6351 | val_loss=0.8536 | val_acc=0.6516
Epoch 048 | loss=0.7495 | train_acc=0.6574 | val_loss=0.8537 | val_acc=0.6686
Epoch 049 | loss=0.7551 | train_acc=0.6589 | val_loss=0.8319 | val_acc=0.6690
Final Test Loss: 0.7806 | Test Accuracy: 0.6702
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=3.4674 | train_acc=0.6423 | val_loss=6.3314 | val_acc=0.1929
Epoch 001 | loss=2.1069 | train_acc=0.5210 | val_loss=1.2307 | val_acc=0.3295
Epoch 002 | loss=1.1430 | train_acc=0.5397 | val_loss=1.0536 | val_acc=0.6187
Epoch 003 | loss=1.0680 | train_acc=0.5793 | val_loss=1.0455 | val_acc=0.6187
Epoch 004 | loss=1.0476 | train_acc=0.5921 | val_loss=1.0364 | val_acc=0.6187
Epoch 005 | loss=1.0225 | train_acc=0.5960 | val_loss=1.0291 | val_acc=0.6187
Epoch 006 | loss=1.0054 | train_acc=0.5984 | val_loss=1.0238 | val_acc=0.6187
Epoch 007 | loss=0.9962 | train_acc=0.6003 | val_loss=1.0198 | val_acc=0.6187
Epoch 008 | loss=0.9885 | train_acc=0.6010 | val_loss=1.0165 | val_acc=0.6187
Epoch 009 | loss=0.9823 | train_acc=0.6019 | val_loss=1.0140 | val_acc=0.6187
Epoch 010 | loss=0.9751 | train_acc=0.6023 | val_loss=1.0121 | val_acc=0.6187
Epoch 011 | loss=0.9717 | train_acc=0.5999 | val_loss=1.0106 | val_acc=0.6187
Epoch 012 | loss=0.9665 | train_acc=0.6013 | val_loss=1.0095 | val_acc=0.6187
Epoch 013 | loss=0.9625 | train_acc=0.6013 | val_loss=1.0087 | val_acc=0.6187
Epoch 014 | loss=0.9605 | train_acc=0.6018 | val_loss=1.0082 | val_acc=0.6187
Epoch 015 | loss=0.9559 | train_acc=0.6025 | val_loss=1.0077 | val_acc=0.6187
Epoch 016 | loss=0.9548 | train_acc=0.6012 | val_loss=1.0075 | val_acc=0.6187
Epoch 017 | loss=0.9510 | train_acc=0.6022 | val_loss=1.0075 | val_acc=0.6187
Epoch 018 | loss=0.9498 | train_acc=0.6028 | val_loss=1.0073 | val_acc=0.6187
Epoch 019 | loss=0.9487 | train_acc=0.6016 | val_loss=1.0075 | val_acc=0.6187
Epoch 020 | loss=0.9466 | train_acc=0.6021 | val_loss=1.0077 | val_acc=0.6187
Epoch 021 | loss=0.9455 | train_acc=0.6022 | val_loss=1.0078 | val_acc=0.6187
Epoch 022 | loss=0.9445 | train_acc=0.6023 | val_loss=1.0081 | val_acc=0.6187
Epoch 023 | loss=0.9430 | train_acc=0.6012 | val_loss=1.0083 | val_acc=0.6187
Epoch 024 | loss=0.9422 | train_acc=0.6019 | val_loss=1.0086 | val_acc=0.6187
Epoch 025 | loss=0.9407 | train_acc=0.6021 | val_loss=1.0091 | val_acc=0.6187
Epoch 026 | loss=0.9397 | train_acc=0.6028 | val_loss=1.0093 | val_acc=0.6187
Epoch 027 | loss=0.9397 | train_acc=0.6018 | val_loss=1.0098 | val_acc=0.6187
Epoch 028 | loss=0.9382 | train_acc=0.6025 | val_loss=1.0100 | val_acc=0.6187
Epoch 029 | loss=0.9388 | train_acc=0.6026 | val_loss=1.0102 | val_acc=0.6187
Epoch 030 | loss=0.9387 | train_acc=0.6028 | val_loss=1.0105 | val_acc=0.6187
Epoch 031 | loss=0.9370 | train_acc=0.6029 | val_loss=1.0110 | val_acc=0.6187
Epoch 032 | loss=0.9358 | train_acc=0.6018 | val_loss=1.0105 | val_acc=0.6187
Epoch 033 | loss=0.9361 | train_acc=0.6037 | val_loss=1.0104 | val_acc=0.6187
Epoch 034 | loss=0.9359 | train_acc=0.6020 | val_loss=1.0106 | val_acc=0.6187
Epoch 035 | loss=0.9352 | train_acc=0.6031 | val_loss=1.0109 | val_acc=0.6187
Epoch 036 | loss=0.9953 | train_acc=0.6036 | val_loss=1.0120 | val_acc=0.6187
Epoch 037 | loss=0.9665 | train_acc=0.6025 | val_loss=1.0134 | val_acc=0.6187
Epoch 038 | loss=0.9408 | train_acc=0.6012 | val_loss=1.0155 | val_acc=0.6187
Epoch 039 | loss=0.9366 | train_acc=0.6025 | val_loss=1.0156 | val_acc=0.6187
Epoch 040 | loss=0.9372 | train_acc=0.6019 | val_loss=1.0158 | val_acc=0.6187
Epoch 041 | loss=0.9367 | train_acc=0.6014 | val_loss=1.0157 | val_acc=0.6187
Epoch 042 | loss=0.9359 | train_acc=0.6022 | val_loss=1.0166 | val_acc=0.6187
Epoch 043 | loss=0.9365 | train_acc=0.6033 | val_loss=1.0176 | val_acc=0.6187
Epoch 044 | loss=0.9346 | train_acc=0.6036 | val_loss=1.0180 | val_acc=0.6187
Epoch 045 | loss=0.9350 | train_acc=0.6027 | val_loss=1.0183 | val_acc=0.6187
Epoch 046 | loss=0.9335 | train_acc=0.6032 | val_loss=1.0186 | val_acc=0.6187
Epoch 047 | loss=0.9342 | train_acc=0.6032 | val_loss=1.0185 | val_acc=0.6187
Epoch 048 | loss=0.9341 | train_acc=0.6029 | val_loss=1.0179 | val_acc=0.6187
Epoch 049 | loss=0.9337 | train_acc=0.6022 | val_loss=1.0176 | val_acc=0.6187
Final Test Loss: 0.9122 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=3.8496 | train_acc=0.6586 | val_loss=9.4371 | val_acc=0.1914
Epoch 001 | loss=2.5037 | train_acc=0.5330 | val_loss=1.0678 | val_acc=0.5328
Epoch 002 | loss=1.3703 | train_acc=0.5800 | val_loss=1.0534 | val_acc=0.6183
Epoch 003 | loss=1.0774 | train_acc=0.5800 | val_loss=1.0420 | val_acc=0.6187
Epoch 004 | loss=1.0268 | train_acc=0.5962 | val_loss=1.0327 | val_acc=0.6187
Epoch 005 | loss=1.0098 | train_acc=0.6033 | val_loss=1.0257 | val_acc=0.6187
Epoch 006 | loss=0.9982 | train_acc=0.6053 | val_loss=1.0206 | val_acc=0.6187
Epoch 007 | loss=0.9890 | train_acc=0.6074 | val_loss=1.0169 | val_acc=0.6187
Epoch 008 | loss=0.9825 | train_acc=0.6078 | val_loss=1.0140 | val_acc=0.6187
Epoch 009 | loss=0.9743 | train_acc=0.6078 | val_loss=1.0119 | val_acc=0.6187
Epoch 010 | loss=0.9712 | train_acc=0.6074 | val_loss=1.0103 | val_acc=0.6187
Epoch 011 | loss=0.9671 | train_acc=0.6074 | val_loss=1.0091 | val_acc=0.6187
Epoch 012 | loss=0.9622 | train_acc=0.6078 | val_loss=1.0082 | val_acc=0.6187
Epoch 013 | loss=0.9597 | train_acc=0.6078 | val_loss=1.0076 | val_acc=0.6187
Epoch 014 | loss=0.9559 | train_acc=0.6082 | val_loss=1.0072 | val_acc=0.6187
Epoch 015 | loss=0.9542 | train_acc=0.6079 | val_loss=1.0070 | val_acc=0.6187
Epoch 016 | loss=0.9514 | train_acc=0.6078 | val_loss=1.0070 | val_acc=0.6187
Epoch 017 | loss=0.9492 | train_acc=0.6079 | val_loss=1.0071 | val_acc=0.6187
Epoch 018 | loss=0.9474 | train_acc=0.6084 | val_loss=1.0072 | val_acc=0.6187
Epoch 019 | loss=0.9463 | train_acc=0.6080 | val_loss=1.0075 | val_acc=0.6187
Epoch 020 | loss=0.9451 | train_acc=0.6080 | val_loss=1.0078 | val_acc=0.6187
Epoch 021 | loss=0.9437 | train_acc=0.6082 | val_loss=1.0081 | val_acc=0.6187
Epoch 022 | loss=0.9435 | train_acc=0.6078 | val_loss=1.0085 | val_acc=0.6187
Epoch 023 | loss=0.9408 | train_acc=0.6084 | val_loss=1.0089 | val_acc=0.6187
Epoch 024 | loss=0.9404 | train_acc=0.6083 | val_loss=1.0093 | val_acc=0.6187
Epoch 025 | loss=0.9398 | train_acc=0.6082 | val_loss=1.0096 | val_acc=0.6187
Epoch 026 | loss=0.9396 | train_acc=0.6083 | val_loss=1.0101 | val_acc=0.6187
Epoch 027 | loss=0.9383 | train_acc=0.6082 | val_loss=1.0104 | val_acc=0.6187
Epoch 028 | loss=0.9386 | train_acc=0.6075 | val_loss=1.0107 | val_acc=0.6187
Epoch 029 | loss=0.9379 | train_acc=0.6076 | val_loss=1.0109 | val_acc=0.6187
Epoch 030 | loss=0.9382 | train_acc=0.6084 | val_loss=1.0115 | val_acc=0.6187
Epoch 031 | loss=0.9371 | train_acc=0.6079 | val_loss=1.0117 | val_acc=0.6187
Epoch 032 | loss=0.9367 | train_acc=0.6083 | val_loss=1.0123 | val_acc=0.6187
Epoch 033 | loss=0.9361 | train_acc=0.6081 | val_loss=1.0126 | val_acc=0.6187
Epoch 034 | loss=0.9365 | train_acc=0.6082 | val_loss=1.0133 | val_acc=0.6187
Epoch 035 | loss=0.9344 | train_acc=0.6083 | val_loss=1.0136 | val_acc=0.6187
Epoch 036 | loss=0.9333 | train_acc=0.6082 | val_loss=1.0135 | val_acc=0.6187
Epoch 037 | loss=0.9353 | train_acc=0.6078 | val_loss=1.0138 | val_acc=0.6187
Epoch 038 | loss=0.9332 | train_acc=0.6088 | val_loss=1.0142 | val_acc=0.6187
Epoch 039 | loss=0.9344 | train_acc=0.6081 | val_loss=1.0152 | val_acc=0.6187
Epoch 040 | loss=0.9330 | train_acc=0.6084 | val_loss=1.0152 | val_acc=0.6187
Epoch 041 | loss=0.9338 | train_acc=0.6080 | val_loss=1.0158 | val_acc=0.6187
Epoch 042 | loss=0.9325 | train_acc=0.6079 | val_loss=1.0166 | val_acc=0.6187
Epoch 043 | loss=0.9327 | train_acc=0.6082 | val_loss=1.0159 | val_acc=0.6187
Epoch 044 | loss=0.9320 | train_acc=0.6081 | val_loss=1.0157 | val_acc=0.6187
Epoch 045 | loss=0.9327 | train_acc=0.6087 | val_loss=1.0174 | val_acc=0.6187
Epoch 046 | loss=1.0949 | train_acc=0.6113 | val_loss=3.0146 | val_acc=0.2825
Epoch 047 | loss=1.0901 | train_acc=0.6739 | val_loss=22.6892 | val_acc=0.2018
Epoch 048 | loss=2.3721 | train_acc=0.6500 | val_loss=7.9567 | val_acc=0.2618
Epoch 049 | loss=2.0204 | train_acc=0.6151 | val_loss=2.6021 | val_acc=0.2762
Final Test Loss: 3.7104 | Test Accuracy: 0.2511
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=4.2336 | train_acc=0.6525 | val_loss=9.1454 | val_acc=0.1910
Epoch 001 | loss=2.5046 | train_acc=0.5596 | val_loss=1.3626 | val_acc=0.2562
Epoch 002 | loss=1.2840 | train_acc=0.5314 | val_loss=1.0541 | val_acc=0.6190
Epoch 003 | loss=1.0655 | train_acc=0.5806 | val_loss=1.0458 | val_acc=0.6187
Epoch 004 | loss=1.0331 | train_acc=0.5978 | val_loss=1.0372 | val_acc=0.6187
Epoch 005 | loss=1.0164 | train_acc=0.6035 | val_loss=1.0307 | val_acc=0.6187
Epoch 006 | loss=1.0074 | train_acc=0.6058 | val_loss=1.0251 | val_acc=0.6187
Epoch 007 | loss=0.9979 | train_acc=0.6070 | val_loss=1.0203 | val_acc=0.6187
Epoch 008 | loss=0.9888 | train_acc=0.6079 | val_loss=1.0169 | val_acc=0.6187
Epoch 009 | loss=0.9799 | train_acc=0.6083 | val_loss=1.0134 | val_acc=0.6187
Epoch 010 | loss=0.9745 | train_acc=0.6086 | val_loss=1.0106 | val_acc=0.6187
Epoch 011 | loss=0.9683 | train_acc=0.6091 | val_loss=1.0077 | val_acc=0.6187
Epoch 012 | loss=0.9652 | train_acc=0.6089 | val_loss=1.0065 | val_acc=0.6187
Epoch 013 | loss=0.9635 | train_acc=0.6093 | val_loss=1.0045 | val_acc=0.6187
Epoch 014 | loss=0.9581 | train_acc=0.6094 | val_loss=1.0028 | val_acc=0.6187
Epoch 015 | loss=0.9559 | train_acc=0.6090 | val_loss=1.0019 | val_acc=0.6187
Epoch 016 | loss=0.9517 | train_acc=0.6090 | val_loss=1.0015 | val_acc=0.6187
Epoch 017 | loss=0.9491 | train_acc=0.6096 | val_loss=1.0013 | val_acc=0.6187
Epoch 018 | loss=0.9483 | train_acc=0.6095 | val_loss=1.0008 | val_acc=0.6187
Epoch 019 | loss=0.9495 | train_acc=0.6096 | val_loss=1.0037 | val_acc=0.6187
Epoch 020 | loss=0.9480 | train_acc=0.6094 | val_loss=1.0048 | val_acc=0.6187
Epoch 021 | loss=0.9444 | train_acc=0.6094 | val_loss=1.0032 | val_acc=0.6187
Epoch 022 | loss=0.9431 | train_acc=0.6097 | val_loss=1.0023 | val_acc=0.6187
Epoch 023 | loss=0.9424 | train_acc=0.6094 | val_loss=1.0039 | val_acc=0.6187
Epoch 024 | loss=0.9415 | train_acc=0.6096 | val_loss=1.0045 | val_acc=0.6187
Epoch 025 | loss=0.9369 | train_acc=0.6092 | val_loss=1.0049 | val_acc=0.6187
Epoch 026 | loss=0.9375 | train_acc=0.6099 | val_loss=1.0055 | val_acc=0.6187
Epoch 027 | loss=0.9372 | train_acc=0.6097 | val_loss=1.0044 | val_acc=0.6187
Epoch 028 | loss=0.9354 | train_acc=0.6099 | val_loss=1.0039 | val_acc=0.6187
Epoch 029 | loss=0.9341 | train_acc=0.6093 | val_loss=1.0053 | val_acc=0.6187
Epoch 030 | loss=0.9348 | train_acc=0.6099 | val_loss=1.0045 | val_acc=0.6187
Epoch 031 | loss=0.9348 | train_acc=0.6090 | val_loss=1.0061 | val_acc=0.6187
Epoch 032 | loss=0.9335 | train_acc=0.6094 | val_loss=1.0054 | val_acc=0.6187
Epoch 033 | loss=0.9313 | train_acc=0.6095 | val_loss=1.0052 | val_acc=0.6187
Epoch 034 | loss=0.9312 | train_acc=0.6096 | val_loss=1.0051 | val_acc=0.6187
Epoch 035 | loss=0.9308 | train_acc=0.6096 | val_loss=1.0052 | val_acc=0.6187
Epoch 036 | loss=0.9307 | train_acc=0.6098 | val_loss=1.0049 | val_acc=0.6187
Epoch 037 | loss=0.9304 | train_acc=0.6096 | val_loss=1.0053 | val_acc=0.6187
Epoch 038 | loss=0.9285 | train_acc=0.6095 | val_loss=1.0000 | val_acc=0.6187
Epoch 039 | loss=0.9265 | train_acc=0.6092 | val_loss=0.9988 | val_acc=0.6187
Epoch 040 | loss=0.9262 | train_acc=0.6094 | val_loss=0.9981 | val_acc=0.6187
Epoch 041 | loss=0.9239 | train_acc=0.6101 | val_loss=0.9955 | val_acc=0.6187
Epoch 042 | loss=0.9195 | train_acc=0.6099 | val_loss=0.9978 | val_acc=0.6187
Epoch 043 | loss=0.9232 | train_acc=0.6098 | val_loss=0.9962 | val_acc=0.6187
Epoch 044 | loss=0.9181 | train_acc=0.6098 | val_loss=0.9896 | val_acc=0.6187
Epoch 045 | loss=0.9284 | train_acc=0.6092 | val_loss=1.0133 | val_acc=0.6187
Epoch 046 | loss=1.0822 | train_acc=0.6103 | val_loss=0.9928 | val_acc=0.6187
Epoch 047 | loss=1.0336 | train_acc=0.6757 | val_loss=4.7030 | val_acc=0.3617
Epoch 048 | loss=1.8631 | train_acc=0.5985 | val_loss=1.3251 | val_acc=0.4839
Epoch 049 | loss=0.9604 | train_acc=0.6082 | val_loss=1.0138 | val_acc=0.6187
Final Test Loss: 0.9229 | Test Accuracy: 0.6322
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=3.8827 | train_acc=0.6589 | val_loss=2.1321 | val_acc=0.1955
Epoch 001 | loss=1.5863 | train_acc=0.5907 | val_loss=1.6299 | val_acc=0.2262
Epoch 002 | loss=1.2987 | train_acc=0.5448 | val_loss=1.0585 | val_acc=0.6187
Epoch 003 | loss=1.2729 | train_acc=0.5935 | val_loss=1.0492 | val_acc=0.6083
Epoch 004 | loss=1.0974 | train_acc=0.5853 | val_loss=1.0429 | val_acc=0.6187
Epoch 005 | loss=1.0601 | train_acc=0.5978 | val_loss=1.0369 | val_acc=0.6187
Epoch 006 | loss=1.0545 | train_acc=0.5955 | val_loss=1.0313 | val_acc=0.6187
Epoch 007 | loss=1.0238 | train_acc=0.6023 | val_loss=1.0255 | val_acc=0.6187
Epoch 008 | loss=0.9984 | train_acc=0.6067 | val_loss=1.0203 | val_acc=0.6187
Epoch 009 | loss=0.9882 | train_acc=0.6083 | val_loss=1.0164 | val_acc=0.6187
Epoch 010 | loss=0.9792 | train_acc=0.6092 | val_loss=1.0135 | val_acc=0.6187
Epoch 011 | loss=0.9739 | train_acc=0.6093 | val_loss=1.0113 | val_acc=0.6187
Epoch 012 | loss=0.9698 | train_acc=0.6094 | val_loss=1.0094 | val_acc=0.6187
Epoch 013 | loss=0.9640 | train_acc=0.6096 | val_loss=1.0080 | val_acc=0.6187
Epoch 014 | loss=0.9549 | train_acc=0.6095 | val_loss=1.0061 | val_acc=0.6187
Epoch 015 | loss=0.9492 | train_acc=0.6098 | val_loss=1.0036 | val_acc=0.6187
Epoch 016 | loss=0.9423 | train_acc=0.6095 | val_loss=1.0033 | val_acc=0.6187
Epoch 017 | loss=0.9465 | train_acc=0.6096 | val_loss=1.0001 | val_acc=0.6187
Epoch 018 | loss=0.9306 | train_acc=0.6097 | val_loss=0.9931 | val_acc=0.6187
Epoch 019 | loss=0.9171 | train_acc=0.6097 | val_loss=0.9868 | val_acc=0.6187
Epoch 020 | loss=0.9112 | train_acc=0.6098 | val_loss=0.9904 | val_acc=0.6187
Epoch 021 | loss=0.9007 | train_acc=0.6095 | val_loss=0.9818 | val_acc=0.6187
Epoch 022 | loss=0.8835 | train_acc=0.6096 | val_loss=0.9828 | val_acc=0.6187
Epoch 023 | loss=0.8808 | train_acc=0.6097 | val_loss=0.9668 | val_acc=0.6187
Epoch 024 | loss=0.8701 | train_acc=0.6094 | val_loss=0.9809 | val_acc=0.6187
Epoch 025 | loss=0.8820 | train_acc=0.6097 | val_loss=0.9643 | val_acc=0.6187
Epoch 026 | loss=0.8682 | train_acc=0.6097 | val_loss=0.9739 | val_acc=0.6187
Epoch 027 | loss=0.8723 | train_acc=0.6097 | val_loss=0.9670 | val_acc=0.6187
Epoch 028 | loss=0.8669 | train_acc=0.6099 | val_loss=0.9774 | val_acc=0.6187
Epoch 029 | loss=0.8717 | train_acc=0.6097 | val_loss=0.9669 | val_acc=0.6187
Epoch 030 | loss=0.8479 | train_acc=0.6099 | val_loss=0.9630 | val_acc=0.6187
Epoch 031 | loss=0.8460 | train_acc=0.6098 | val_loss=0.9567 | val_acc=0.6187
Epoch 032 | loss=0.8433 | train_acc=0.6094 | val_loss=0.9562 | val_acc=0.6187
Epoch 033 | loss=0.8357 | train_acc=0.6095 | val_loss=0.9565 | val_acc=0.6187
Epoch 034 | loss=0.8337 | train_acc=0.6095 | val_loss=0.9666 | val_acc=0.6187
Epoch 035 | loss=0.8357 | train_acc=0.6097 | val_loss=0.9624 | val_acc=0.6187
Epoch 036 | loss=0.8304 | train_acc=0.6098 | val_loss=0.9697 | val_acc=0.6187
Epoch 037 | loss=0.8296 | train_acc=0.6098 | val_loss=0.9539 | val_acc=0.6187
Epoch 038 | loss=0.8226 | train_acc=0.6096 | val_loss=0.9652 | val_acc=0.6187
Epoch 039 | loss=0.8291 | train_acc=0.6096 | val_loss=0.9568 | val_acc=0.6187
Epoch 040 | loss=0.8262 | train_acc=0.6099 | val_loss=0.9582 | val_acc=0.6187
Epoch 041 | loss=0.8200 | train_acc=0.6096 | val_loss=0.9580 | val_acc=0.6187
Epoch 042 | loss=0.8219 | train_acc=0.6099 | val_loss=0.9678 | val_acc=0.6187
Epoch 043 | loss=0.8219 | train_acc=0.6096 | val_loss=0.9509 | val_acc=0.6187
Epoch 044 | loss=0.8159 | train_acc=0.6099 | val_loss=0.9721 | val_acc=0.6187
Epoch 045 | loss=0.8173 | train_acc=0.6095 | val_loss=0.9495 | val_acc=0.6187
Epoch 046 | loss=0.8135 | train_acc=0.6099 | val_loss=0.9471 | val_acc=0.6187
Epoch 047 | loss=0.8338 | train_acc=0.6097 | val_loss=0.9722 | val_acc=0.6187
Epoch 048 | loss=0.8634 | train_acc=0.6082 | val_loss=0.9533 | val_acc=0.6187
Epoch 049 | loss=0.8465 | train_acc=0.6091 | val_loss=0.9397 | val_acc=0.6187
Final Test Loss: 0.8871 | Test Accuracy: 0.6322
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=6.2299 | train_acc=0.6174 | val_loss=12.0310 | val_acc=0.1529
Epoch 001 | loss=4.4676 | train_acc=0.5117 | val_loss=3.4651 | val_acc=0.1840
Epoch 002 | loss=1.5998 | train_acc=0.4986 | val_loss=1.0821 | val_acc=0.6127
Epoch 003 | loss=1.1404 | train_acc=0.5419 | val_loss=1.0665 | val_acc=0.6187
Epoch 004 | loss=1.0714 | train_acc=0.5760 | val_loss=1.0532 | val_acc=0.6187
Epoch 005 | loss=1.0429 | train_acc=0.5897 | val_loss=1.0427 | val_acc=0.6187
Epoch 006 | loss=1.0255 | train_acc=0.5956 | val_loss=1.0345 | val_acc=0.6187
Epoch 007 | loss=1.0139 | train_acc=0.5980 | val_loss=1.0281 | val_acc=0.6187
Epoch 008 | loss=1.0029 | train_acc=0.5995 | val_loss=1.0231 | val_acc=0.6187
Epoch 009 | loss=0.9944 | train_acc=0.6000 | val_loss=1.0191 | val_acc=0.6187
Epoch 010 | loss=0.9859 | train_acc=0.6007 | val_loss=1.0161 | val_acc=0.6187
Epoch 011 | loss=0.9800 | train_acc=0.6000 | val_loss=1.0139 | val_acc=0.6187
Epoch 012 | loss=0.9745 | train_acc=0.6017 | val_loss=1.0122 | val_acc=0.6187
Epoch 013 | loss=0.9701 | train_acc=0.6026 | val_loss=1.0109 | val_acc=0.6187
Epoch 014 | loss=0.9652 | train_acc=0.6015 | val_loss=1.0101 | val_acc=0.6187
Epoch 015 | loss=0.9630 | train_acc=0.6016 | val_loss=1.0096 | val_acc=0.6187
Epoch 016 | loss=0.9592 | train_acc=0.6008 | val_loss=1.0093 | val_acc=0.6187
Epoch 017 | loss=0.9558 | train_acc=0.6009 | val_loss=1.0092 | val_acc=0.6187
Epoch 018 | loss=0.9540 | train_acc=0.6014 | val_loss=1.0092 | val_acc=0.6187
Epoch 019 | loss=0.9524 | train_acc=0.6016 | val_loss=1.0094 | val_acc=0.6187
Epoch 020 | loss=0.9498 | train_acc=0.6018 | val_loss=1.0097 | val_acc=0.6187
Epoch 021 | loss=0.9474 | train_acc=0.6016 | val_loss=1.0100 | val_acc=0.6187
Epoch 022 | loss=0.9471 | train_acc=0.6022 | val_loss=1.0104 | val_acc=0.6187
Epoch 023 | loss=0.9452 | train_acc=0.6008 | val_loss=1.0108 | val_acc=0.6187
Epoch 024 | loss=0.9447 | train_acc=0.6039 | val_loss=1.0113 | val_acc=0.6187
Epoch 025 | loss=0.9432 | train_acc=0.6028 | val_loss=1.0118 | val_acc=0.6187
Epoch 026 | loss=0.9421 | train_acc=0.6024 | val_loss=1.0122 | val_acc=0.6187
Epoch 027 | loss=0.9419 | train_acc=0.6015 | val_loss=1.0127 | val_acc=0.6187
Epoch 028 | loss=0.9407 | train_acc=0.6025 | val_loss=1.0132 | val_acc=0.6187
Epoch 029 | loss=0.9403 | train_acc=0.6024 | val_loss=1.0137 | val_acc=0.6187
Epoch 030 | loss=0.9391 | train_acc=0.6029 | val_loss=1.0143 | val_acc=0.6187
Epoch 031 | loss=0.9388 | train_acc=0.6019 | val_loss=1.0148 | val_acc=0.6187
Epoch 032 | loss=0.9382 | train_acc=0.6017 | val_loss=1.0153 | val_acc=0.6187
Epoch 033 | loss=0.9383 | train_acc=0.6026 | val_loss=1.0158 | val_acc=0.6187
Epoch 034 | loss=0.9373 | train_acc=0.6019 | val_loss=1.0163 | val_acc=0.6187
Epoch 035 | loss=1.0579 | train_acc=0.6006 | val_loss=1.0168 | val_acc=0.5964
Epoch 036 | loss=1.5463 | train_acc=0.6088 | val_loss=4.0746 | val_acc=0.2325
Epoch 037 | loss=1.5475 | train_acc=0.6151 | val_loss=1.6994 | val_acc=0.3154
Epoch 038 | loss=1.2925 | train_acc=0.5952 | val_loss=1.0152 | val_acc=0.6187
Epoch 039 | loss=0.9430 | train_acc=0.6020 | val_loss=1.0158 | val_acc=0.6187
Epoch 040 | loss=0.9388 | train_acc=0.6020 | val_loss=1.0163 | val_acc=0.6187
Epoch 041 | loss=0.9369 | train_acc=0.6028 | val_loss=1.0169 | val_acc=0.6187
Epoch 042 | loss=0.9375 | train_acc=0.6028 | val_loss=1.0174 | val_acc=0.6187
Epoch 043 | loss=0.9370 | train_acc=0.6021 | val_loss=1.0180 | val_acc=0.6187
Epoch 044 | loss=0.9362 | train_acc=0.6032 | val_loss=1.0184 | val_acc=0.6187
Epoch 045 | loss=0.9365 | train_acc=0.6019 | val_loss=1.0190 | val_acc=0.6187
Epoch 046 | loss=0.9355 | train_acc=0.6031 | val_loss=1.0194 | val_acc=0.6187
Epoch 047 | loss=0.9362 | train_acc=0.6023 | val_loss=1.0199 | val_acc=0.6187
Epoch 048 | loss=0.9361 | train_acc=0.6021 | val_loss=1.0203 | val_acc=0.6187
Epoch 049 | loss=0.9358 | train_acc=0.6020 | val_loss=1.0208 | val_acc=0.6187
Final Test Loss: 0.9159 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=6.4536 | train_acc=0.6172 | val_loss=15.3063 | val_acc=0.1529
Epoch 001 | loss=4.9041 | train_acc=0.4920 | val_loss=1.5269 | val_acc=0.1803
Epoch 002 | loss=1.2598 | train_acc=0.4475 | val_loss=1.0820 | val_acc=0.6187
Epoch 003 | loss=1.1030 | train_acc=0.5606 | val_loss=1.0660 | val_acc=0.6187
Epoch 004 | loss=1.0660 | train_acc=0.5881 | val_loss=1.0531 | val_acc=0.6187
Epoch 005 | loss=1.0453 | train_acc=0.5983 | val_loss=1.0430 | val_acc=0.6187
Epoch 006 | loss=1.0302 | train_acc=0.6007 | val_loss=1.0350 | val_acc=0.6187
Epoch 007 | loss=1.0175 | train_acc=0.6034 | val_loss=1.0285 | val_acc=0.6187
Epoch 008 | loss=1.0051 | train_acc=0.6048 | val_loss=1.0232 | val_acc=0.6187
Epoch 009 | loss=0.9970 | train_acc=0.6058 | val_loss=1.0193 | val_acc=0.6187
Epoch 010 | loss=0.9874 | train_acc=0.6060 | val_loss=1.0162 | val_acc=0.6187
Epoch 011 | loss=0.9814 | train_acc=0.6070 | val_loss=1.0139 | val_acc=0.6187
Epoch 012 | loss=0.9747 | train_acc=0.6065 | val_loss=1.0122 | val_acc=0.6187
Epoch 013 | loss=0.9699 | train_acc=0.6069 | val_loss=1.0109 | val_acc=0.6187
Epoch 014 | loss=0.9653 | train_acc=0.6073 | val_loss=1.0101 | val_acc=0.6187
Epoch 015 | loss=0.9619 | train_acc=0.6078 | val_loss=1.0097 | val_acc=0.6187
Epoch 016 | loss=0.9586 | train_acc=0.6081 | val_loss=1.0095 | val_acc=0.6187
Epoch 017 | loss=0.9567 | train_acc=0.6078 | val_loss=1.0095 | val_acc=0.6187
Epoch 018 | loss=0.9553 | train_acc=0.6073 | val_loss=1.0096 | val_acc=0.6187
Epoch 019 | loss=0.9523 | train_acc=0.6073 | val_loss=1.0099 | val_acc=0.6187
Epoch 020 | loss=0.9497 | train_acc=0.6075 | val_loss=1.0103 | val_acc=0.6187
Epoch 021 | loss=0.9478 | train_acc=0.6078 | val_loss=1.0107 | val_acc=0.6187
Epoch 022 | loss=0.9471 | train_acc=0.6076 | val_loss=1.0112 | val_acc=0.6187
Epoch 023 | loss=0.9453 | train_acc=0.6080 | val_loss=1.0118 | val_acc=0.6187
Epoch 024 | loss=0.9435 | train_acc=0.6079 | val_loss=1.0123 | val_acc=0.6187
Epoch 025 | loss=0.9427 | train_acc=0.6077 | val_loss=1.0129 | val_acc=0.6187
Epoch 026 | loss=0.9418 | train_acc=0.6080 | val_loss=1.0134 | val_acc=0.6187
Epoch 027 | loss=0.9411 | train_acc=0.6079 | val_loss=1.0140 | val_acc=0.6187
Epoch 028 | loss=0.9407 | train_acc=0.6083 | val_loss=1.0145 | val_acc=0.6187
Epoch 029 | loss=0.9401 | train_acc=0.6079 | val_loss=1.0151 | val_acc=0.6187
Epoch 030 | loss=0.9390 | train_acc=0.6081 | val_loss=1.0156 | val_acc=0.6187
Epoch 031 | loss=0.9384 | train_acc=0.6079 | val_loss=1.0160 | val_acc=0.6187
Epoch 032 | loss=1.6323 | train_acc=0.6557 | val_loss=3.4196 | val_acc=0.2969
Epoch 033 | loss=1.1407 | train_acc=0.5960 | val_loss=1.0176 | val_acc=0.6187
Epoch 034 | loss=0.9392 | train_acc=0.6082 | val_loss=1.0181 | val_acc=0.6187
Epoch 035 | loss=0.9387 | train_acc=0.6082 | val_loss=1.0187 | val_acc=0.6187
Epoch 036 | loss=0.9379 | train_acc=0.6085 | val_loss=1.0192 | val_acc=0.6187
Epoch 037 | loss=0.9379 | train_acc=0.6082 | val_loss=1.0197 | val_acc=0.6187
Epoch 038 | loss=0.9376 | train_acc=0.6075 | val_loss=1.0203 | val_acc=0.6187
Epoch 039 | loss=0.9377 | train_acc=0.6079 | val_loss=1.0208 | val_acc=0.6187
Epoch 040 | loss=0.9370 | train_acc=0.6085 | val_loss=1.0212 | val_acc=0.6187
Epoch 041 | loss=0.9367 | train_acc=0.6080 | val_loss=1.0217 | val_acc=0.6187
Epoch 042 | loss=0.9366 | train_acc=0.6080 | val_loss=1.0221 | val_acc=0.6187
Epoch 043 | loss=0.9369 | train_acc=0.6082 | val_loss=1.0225 | val_acc=0.6187
Epoch 044 | loss=0.9359 | train_acc=0.6085 | val_loss=1.0228 | val_acc=0.6187
Epoch 045 | loss=0.9358 | train_acc=0.6086 | val_loss=1.0232 | val_acc=0.6187
Epoch 046 | loss=0.9359 | train_acc=0.6084 | val_loss=1.0235 | val_acc=0.6187
Epoch 047 | loss=0.9356 | train_acc=0.6085 | val_loss=1.0237 | val_acc=0.6187
Epoch 048 | loss=0.9349 | train_acc=0.6085 | val_loss=1.0240 | val_acc=0.6187
Epoch 049 | loss=0.9360 | train_acc=0.6082 | val_loss=1.0243 | val_acc=0.6187
Final Test Loss: 0.9157 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=7.4193 | train_acc=0.6193 | val_loss=5.7303 | val_acc=0.1529
Epoch 001 | loss=3.9408 | train_acc=0.5048 | val_loss=1.1075 | val_acc=0.1692
Epoch 002 | loss=1.3104 | train_acc=0.4525 | val_loss=1.0893 | val_acc=0.4613
Epoch 003 | loss=1.1364 | train_acc=0.5414 | val_loss=1.0714 | val_acc=0.6187
Epoch 004 | loss=1.0742 | train_acc=0.5816 | val_loss=1.0580 | val_acc=0.6187
Epoch 005 | loss=1.0481 | train_acc=0.5932 | val_loss=1.0470 | val_acc=0.6187
Epoch 006 | loss=1.0325 | train_acc=0.5996 | val_loss=1.0382 | val_acc=0.6187
Epoch 007 | loss=1.0180 | train_acc=0.6033 | val_loss=1.0313 | val_acc=0.6187
Epoch 008 | loss=1.0077 | train_acc=0.6049 | val_loss=1.0257 | val_acc=0.6187
Epoch 009 | loss=0.9973 | train_acc=0.6064 | val_loss=1.0213 | val_acc=0.6187
Epoch 010 | loss=0.9893 | train_acc=0.6077 | val_loss=1.0179 | val_acc=0.6187
Epoch 011 | loss=0.9828 | train_acc=0.6077 | val_loss=1.0152 | val_acc=0.6187
Epoch 012 | loss=0.9771 | train_acc=0.6081 | val_loss=1.0132 | val_acc=0.6187
Epoch 013 | loss=0.9721 | train_acc=0.6090 | val_loss=1.0117 | val_acc=0.6187
Epoch 014 | loss=0.9674 | train_acc=0.6087 | val_loss=1.0107 | val_acc=0.6187
Epoch 015 | loss=0.9625 | train_acc=0.6089 | val_loss=1.0099 | val_acc=0.6187
Epoch 016 | loss=0.9605 | train_acc=0.6090 | val_loss=1.0095 | val_acc=0.6187
Epoch 017 | loss=0.9569 | train_acc=0.6091 | val_loss=1.0093 | val_acc=0.6187
Epoch 018 | loss=0.9546 | train_acc=0.6091 | val_loss=1.0093 | val_acc=0.6187
Epoch 019 | loss=0.9530 | train_acc=0.6090 | val_loss=1.0094 | val_acc=0.6187
Epoch 020 | loss=0.9509 | train_acc=0.6094 | val_loss=1.0096 | val_acc=0.6187
Epoch 021 | loss=0.9491 | train_acc=0.6093 | val_loss=1.0100 | val_acc=0.6187
Epoch 022 | loss=0.9468 | train_acc=0.6093 | val_loss=1.0103 | val_acc=0.6187
Epoch 023 | loss=0.9465 | train_acc=0.6093 | val_loss=1.0108 | val_acc=0.6187
Epoch 024 | loss=0.9447 | train_acc=0.6093 | val_loss=1.0113 | val_acc=0.6187
Epoch 025 | loss=0.9439 | train_acc=0.6095 | val_loss=1.0118 | val_acc=0.6187
Epoch 026 | loss=0.9430 | train_acc=0.6093 | val_loss=1.0123 | val_acc=0.6187
Epoch 027 | loss=0.9414 | train_acc=0.6094 | val_loss=1.0129 | val_acc=0.6187
Epoch 028 | loss=0.9411 | train_acc=0.6094 | val_loss=1.0134 | val_acc=0.6187
Epoch 029 | loss=0.9404 | train_acc=0.6096 | val_loss=1.0140 | val_acc=0.6187
Epoch 030 | loss=0.9393 | train_acc=0.6095 | val_loss=1.0145 | val_acc=0.6187
Epoch 031 | loss=0.9391 | train_acc=0.6094 | val_loss=1.0151 | val_acc=0.6187
Epoch 032 | loss=0.9381 | train_acc=0.6099 | val_loss=1.0156 | val_acc=0.6187
Epoch 033 | loss=0.9381 | train_acc=0.6097 | val_loss=1.0161 | val_acc=0.6187
Epoch 034 | loss=0.9380 | train_acc=0.6098 | val_loss=1.0167 | val_acc=0.6187
Epoch 035 | loss=0.9379 | train_acc=0.6094 | val_loss=1.0172 | val_acc=0.6187
Epoch 036 | loss=0.9373 | train_acc=0.6095 | val_loss=1.0177 | val_acc=0.6187
Epoch 037 | loss=0.9368 | train_acc=0.6102 | val_loss=1.0181 | val_acc=0.6187
Epoch 038 | loss=0.9368 | train_acc=0.6098 | val_loss=1.0186 | val_acc=0.6187
Epoch 039 | loss=0.9367 | train_acc=0.6097 | val_loss=1.0189 | val_acc=0.6187
Epoch 040 | loss=0.9360 | train_acc=0.6097 | val_loss=1.0193 | val_acc=0.6187
Epoch 041 | loss=1.2600 | train_acc=0.6618 | val_loss=3.8652 | val_acc=0.2255
Epoch 042 | loss=1.4234 | train_acc=0.5988 | val_loss=1.1203 | val_acc=0.4502
Epoch 043 | loss=0.9517 | train_acc=0.6066 | val_loss=1.0209 | val_acc=0.6187
Epoch 044 | loss=0.9377 | train_acc=0.6090 | val_loss=1.0213 | val_acc=0.6187
Epoch 045 | loss=0.9374 | train_acc=0.6094 | val_loss=1.0217 | val_acc=0.6187
Epoch 046 | loss=0.9368 | train_acc=0.6095 | val_loss=1.0221 | val_acc=0.6187
Epoch 047 | loss=0.9366 | train_acc=0.6097 | val_loss=1.0225 | val_acc=0.6187
Epoch 048 | loss=0.9362 | train_acc=0.6096 | val_loss=1.0229 | val_acc=0.6187
Epoch 049 | loss=0.9365 | train_acc=0.6098 | val_loss=1.0232 | val_acc=0.6187
Final Test Loss: 0.9164 | Test Accuracy: 0.6322
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=8.9167 | train_acc=0.6402 | val_loss=2.4068 | val_acc=0.1573
Epoch 001 | loss=3.8269 | train_acc=0.5258 | val_loss=1.2096 | val_acc=0.1581
Epoch 002 | loss=1.2403 | train_acc=0.3772 | val_loss=1.0813 | val_acc=0.6187
Epoch 003 | loss=1.0937 | train_acc=0.5593 | val_loss=1.0641 | val_acc=0.6187
Epoch 004 | loss=1.0607 | train_acc=0.5863 | val_loss=1.0510 | val_acc=0.6187
Epoch 005 | loss=1.0382 | train_acc=0.5977 | val_loss=1.0407 | val_acc=0.6187
Epoch 006 | loss=1.0235 | train_acc=0.6016 | val_loss=1.0327 | val_acc=0.6187
Epoch 007 | loss=1.0078 | train_acc=0.6044 | val_loss=1.0263 | val_acc=0.6187
Epoch 008 | loss=0.9983 | train_acc=0.6061 | val_loss=1.0214 | val_acc=0.6187
Epoch 009 | loss=0.9895 | train_acc=0.6072 | val_loss=1.0175 | val_acc=0.6187
Epoch 010 | loss=0.9820 | train_acc=0.6078 | val_loss=1.0145 | val_acc=0.6187
Epoch 011 | loss=0.9752 | train_acc=0.6082 | val_loss=1.0123 | val_acc=0.6187
Epoch 012 | loss=0.9709 | train_acc=0.6087 | val_loss=1.0107 | val_acc=0.6187
Epoch 013 | loss=0.9656 | train_acc=0.6087 | val_loss=1.0095 | val_acc=0.6187
Epoch 014 | loss=0.9614 | train_acc=0.6088 | val_loss=1.0088 | val_acc=0.6187
Epoch 015 | loss=0.9582 | train_acc=0.6091 | val_loss=1.0083 | val_acc=0.6187
Epoch 016 | loss=0.9543 | train_acc=0.6092 | val_loss=1.0081 | val_acc=0.6187
Epoch 017 | loss=0.9524 | train_acc=0.6095 | val_loss=1.0081 | val_acc=0.6187
Epoch 018 | loss=0.9504 | train_acc=0.6095 | val_loss=1.0082 | val_acc=0.6187
Epoch 019 | loss=0.9486 | train_acc=0.6094 | val_loss=1.0084 | val_acc=0.6187
Epoch 020 | loss=0.9469 | train_acc=0.6095 | val_loss=1.0087 | val_acc=0.6187
Epoch 021 | loss=0.9452 | train_acc=0.6094 | val_loss=1.0091 | val_acc=0.6187
Epoch 022 | loss=0.9447 | train_acc=0.6095 | val_loss=1.0096 | val_acc=0.6187
Epoch 023 | loss=0.9427 | train_acc=0.6095 | val_loss=1.0101 | val_acc=0.6187
Epoch 024 | loss=0.9415 | train_acc=0.6096 | val_loss=1.0106 | val_acc=0.6187
Epoch 025 | loss=0.9406 | train_acc=0.6097 | val_loss=1.0111 | val_acc=0.6187
Epoch 026 | loss=0.9399 | train_acc=0.6098 | val_loss=1.0117 | val_acc=0.6187
Epoch 027 | loss=0.9389 | train_acc=0.6095 | val_loss=1.0122 | val_acc=0.6187
Epoch 028 | loss=0.9385 | train_acc=0.6096 | val_loss=1.0127 | val_acc=0.6187
Epoch 029 | loss=0.9389 | train_acc=0.6098 | val_loss=1.0133 | val_acc=0.6187
Epoch 030 | loss=0.9378 | train_acc=0.6096 | val_loss=1.0138 | val_acc=0.6187
Epoch 031 | loss=0.9375 | train_acc=0.6098 | val_loss=1.0143 | val_acc=0.6187
Epoch 032 | loss=0.9371 | train_acc=0.6099 | val_loss=1.0148 | val_acc=0.6187
Epoch 033 | loss=0.9366 | train_acc=0.6098 | val_loss=1.0153 | val_acc=0.6187
Epoch 034 | loss=0.9367 | train_acc=0.6097 | val_loss=1.0158 | val_acc=0.6187
Epoch 035 | loss=0.9361 | train_acc=0.6098 | val_loss=1.0165 | val_acc=0.6187
Epoch 036 | loss=0.9372 | train_acc=0.6097 | val_loss=1.0170 | val_acc=0.6187
Epoch 037 | loss=0.9351 | train_acc=0.6098 | val_loss=1.0175 | val_acc=0.6187
Epoch 038 | loss=0.9355 | train_acc=0.6098 | val_loss=1.0179 | val_acc=0.6187
Epoch 039 | loss=0.9354 | train_acc=0.6097 | val_loss=1.0183 | val_acc=0.6187
Epoch 040 | loss=0.9354 | train_acc=0.6096 | val_loss=1.0187 | val_acc=0.6187
Epoch 041 | loss=0.9351 | train_acc=0.6099 | val_loss=1.0192 | val_acc=0.6187
Epoch 042 | loss=0.9354 | train_acc=0.6097 | val_loss=1.0196 | val_acc=0.6187
Epoch 043 | loss=0.9346 | train_acc=0.6098 | val_loss=1.0199 | val_acc=0.6187
Epoch 044 | loss=0.9355 | train_acc=0.6097 | val_loss=1.0203 | val_acc=0.6187
Epoch 045 | loss=0.9340 | train_acc=0.6099 | val_loss=1.0206 | val_acc=0.6187
Epoch 046 | loss=0.9344 | train_acc=0.6097 | val_loss=1.0209 | val_acc=0.6187
Epoch 047 | loss=0.9336 | train_acc=0.6098 | val_loss=1.0212 | val_acc=0.6187
Epoch 048 | loss=0.9332 | train_acc=0.6099 | val_loss=1.0214 | val_acc=0.6187
Epoch 049 | loss=0.9335 | train_acc=0.6099 | val_loss=1.0216 | val_acc=0.6187
Final Test Loss: 0.9156 | Test Accuracy: 0.6322
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.2768 | train_acc=0.6582 | val_loss=3.5088 | val_acc=0.1970
Epoch 001 | loss=1.3578 | train_acc=0.5122 | val_loss=1.0517 | val_acc=0.6187
Epoch 002 | loss=1.1239 | train_acc=0.5937 | val_loss=1.0745 | val_acc=0.4554
Epoch 003 | loss=1.1336 | train_acc=0.5886 | val_loss=1.0520 | val_acc=0.5476
Epoch 004 | loss=1.1493 | train_acc=0.5844 | val_loss=1.0386 | val_acc=0.6187
Epoch 005 | loss=1.0338 | train_acc=0.5936 | val_loss=1.0293 | val_acc=0.6187
Epoch 006 | loss=1.0092 | train_acc=0.6018 | val_loss=1.0191 | val_acc=0.6187
Epoch 007 | loss=0.9884 | train_acc=0.6028 | val_loss=1.0157 | val_acc=0.6187
Epoch 008 | loss=0.9837 | train_acc=0.6026 | val_loss=1.0123 | val_acc=0.6187
Epoch 009 | loss=0.9713 | train_acc=0.6024 | val_loss=1.0141 | val_acc=0.6187
Epoch 010 | loss=0.9651 | train_acc=0.6031 | val_loss=1.0162 | val_acc=0.6187
Epoch 011 | loss=0.9618 | train_acc=0.6025 | val_loss=1.0172 | val_acc=0.6187
Epoch 012 | loss=0.9592 | train_acc=0.6023 | val_loss=1.0166 | val_acc=0.6187
Epoch 013 | loss=0.9571 | train_acc=0.6026 | val_loss=1.0154 | val_acc=0.6187
Epoch 014 | loss=0.9545 | train_acc=0.6039 | val_loss=1.0117 | val_acc=0.6187
Epoch 015 | loss=0.9549 | train_acc=0.6029 | val_loss=1.0026 | val_acc=0.6187
Epoch 016 | loss=0.9550 | train_acc=0.6019 | val_loss=0.9839 | val_acc=0.6187
Epoch 017 | loss=0.9615 | train_acc=0.6019 | val_loss=0.9932 | val_acc=0.6187
Epoch 018 | loss=0.9528 | train_acc=0.6031 | val_loss=0.9852 | val_acc=0.6187
Epoch 019 | loss=0.9550 | train_acc=0.6018 | val_loss=0.9722 | val_acc=0.6187
Epoch 020 | loss=0.9581 | train_acc=0.6018 | val_loss=0.9779 | val_acc=0.6187
Epoch 021 | loss=0.9573 | train_acc=0.6025 | val_loss=0.9895 | val_acc=0.6187
Epoch 022 | loss=0.9508 | train_acc=0.6023 | val_loss=0.9817 | val_acc=0.6187
Epoch 023 | loss=0.9536 | train_acc=0.6034 | val_loss=1.0054 | val_acc=0.6187
Epoch 024 | loss=0.9523 | train_acc=0.6019 | val_loss=0.9985 | val_acc=0.6187
Epoch 025 | loss=0.9485 | train_acc=0.6022 | val_loss=0.9694 | val_acc=0.6187
Epoch 026 | loss=0.9515 | train_acc=0.6024 | val_loss=0.9675 | val_acc=0.6187
Epoch 027 | loss=0.9520 | train_acc=0.6020 | val_loss=0.9665 | val_acc=0.6187
Epoch 028 | loss=0.9388 | train_acc=0.6122 | val_loss=1.5534 | val_acc=0.4565
Epoch 029 | loss=1.1272 | train_acc=0.6098 | val_loss=4.0043 | val_acc=0.2688
Epoch 030 | loss=1.3320 | train_acc=0.5874 | val_loss=1.0125 | val_acc=0.6187
Epoch 031 | loss=1.0752 | train_acc=0.6030 | val_loss=1.0110 | val_acc=0.6187
Epoch 032 | loss=1.1065 | train_acc=0.6025 | val_loss=1.0192 | val_acc=0.5283
Epoch 033 | loss=1.0600 | train_acc=0.5896 | val_loss=1.0077 | val_acc=0.6168
Epoch 034 | loss=1.1029 | train_acc=0.5979 | val_loss=1.0078 | val_acc=0.6168
Epoch 035 | loss=1.1117 | train_acc=0.5971 | val_loss=1.0096 | val_acc=0.6187
Epoch 036 | loss=1.0011 | train_acc=0.6027 | val_loss=1.0091 | val_acc=0.6187
Epoch 037 | loss=1.0183 | train_acc=0.6079 | val_loss=0.9968 | val_acc=0.6190
Epoch 038 | loss=1.1321 | train_acc=0.5998 | val_loss=1.0051 | val_acc=0.6168
Epoch 039 | loss=1.0753 | train_acc=0.6418 | val_loss=1.2292 | val_acc=0.4720
Epoch 040 | loss=1.1769 | train_acc=0.5865 | val_loss=1.0126 | val_acc=0.6187
Epoch 041 | loss=0.9718 | train_acc=0.6029 | val_loss=1.0113 | val_acc=0.6187
Epoch 042 | loss=0.9623 | train_acc=0.6012 | val_loss=1.0119 | val_acc=0.6187
Epoch 043 | loss=0.9556 | train_acc=0.6019 | val_loss=1.0133 | val_acc=0.6187
Epoch 044 | loss=0.9521 | train_acc=0.6022 | val_loss=1.0147 | val_acc=0.6187
Epoch 045 | loss=0.9495 | train_acc=0.6029 | val_loss=1.0162 | val_acc=0.6187
Epoch 046 | loss=1.0327 | train_acc=0.6012 | val_loss=1.0134 | val_acc=0.6187
Epoch 047 | loss=1.0447 | train_acc=0.6003 | val_loss=1.0114 | val_acc=0.6187
Epoch 048 | loss=1.0543 | train_acc=0.6017 | val_loss=1.0093 | val_acc=0.6187
Epoch 049 | loss=0.9838 | train_acc=0.6010 | val_loss=1.0108 | val_acc=0.6187
Final Test Loss: 0.9369 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.3253 | train_acc=0.6660 | val_loss=3.8674 | val_acc=0.1955
Epoch 001 | loss=1.4047 | train_acc=0.5294 | val_loss=1.0511 | val_acc=0.6183
Epoch 002 | loss=1.1249 | train_acc=0.5879 | val_loss=1.0449 | val_acc=0.6187
Epoch 003 | loss=1.1149 | train_acc=0.5988 | val_loss=1.0369 | val_acc=0.6187
Epoch 004 | loss=1.0953 | train_acc=0.6019 | val_loss=1.0361 | val_acc=0.6187
Epoch 005 | loss=1.0277 | train_acc=0.6029 | val_loss=1.0251 | val_acc=0.6187
Epoch 006 | loss=1.0105 | train_acc=0.6076 | val_loss=1.0154 | val_acc=0.6187
Epoch 007 | loss=0.9911 | train_acc=0.6078 | val_loss=1.0093 | val_acc=0.6187
Epoch 008 | loss=0.9801 | train_acc=0.6082 | val_loss=1.0086 | val_acc=0.6187
Epoch 009 | loss=0.9739 | train_acc=0.6077 | val_loss=1.0124 | val_acc=0.6187
Epoch 010 | loss=0.9663 | train_acc=0.6084 | val_loss=1.0137 | val_acc=0.6187
Epoch 011 | loss=0.9634 | train_acc=0.6080 | val_loss=1.0159 | val_acc=0.6187
Epoch 012 | loss=0.9594 | train_acc=0.6078 | val_loss=1.0168 | val_acc=0.6187
Epoch 013 | loss=0.9603 | train_acc=0.6081 | val_loss=1.0107 | val_acc=0.6187
Epoch 014 | loss=0.9583 | train_acc=0.6084 | val_loss=1.0153 | val_acc=0.6187
Epoch 015 | loss=0.9607 | train_acc=0.6081 | val_loss=1.0183 | val_acc=0.6187
Epoch 016 | loss=0.9554 | train_acc=0.6086 | val_loss=1.0180 | val_acc=0.6187
Epoch 017 | loss=0.9568 | train_acc=0.6080 | val_loss=1.0162 | val_acc=0.6187
Epoch 018 | loss=0.9551 | train_acc=0.6085 | val_loss=1.0116 | val_acc=0.6187
Epoch 019 | loss=0.9538 | train_acc=0.6085 | val_loss=1.0141 | val_acc=0.6187
Epoch 020 | loss=0.9522 | train_acc=0.6079 | val_loss=1.0095 | val_acc=0.6187
Epoch 021 | loss=0.9503 | train_acc=0.6089 | val_loss=1.0062 | val_acc=0.6187
Epoch 022 | loss=0.9593 | train_acc=0.6075 | val_loss=0.9931 | val_acc=0.6187
Epoch 023 | loss=1.1097 | train_acc=0.6078 | val_loss=0.9984 | val_acc=0.6187
Epoch 024 | loss=1.0976 | train_acc=0.6123 | val_loss=1.0273 | val_acc=0.5846
Epoch 025 | loss=1.0856 | train_acc=0.5923 | val_loss=1.0155 | val_acc=0.6187
Epoch 026 | loss=1.0961 | train_acc=0.6183 | val_loss=1.0495 | val_acc=0.5168
Epoch 027 | loss=1.1793 | train_acc=0.5985 | val_loss=1.0396 | val_acc=0.5454
Epoch 028 | loss=1.1259 | train_acc=0.6046 | val_loss=1.0148 | val_acc=0.6187
Epoch 029 | loss=0.9674 | train_acc=0.6066 | val_loss=1.0116 | val_acc=0.6187
Epoch 030 | loss=0.9579 | train_acc=0.6085 | val_loss=1.0090 | val_acc=0.6187
Epoch 031 | loss=0.9537 | train_acc=0.6085 | val_loss=1.0068 | val_acc=0.6187
Epoch 032 | loss=0.9504 | train_acc=0.6082 | val_loss=1.0045 | val_acc=0.6187
Epoch 033 | loss=0.9473 | train_acc=0.6080 | val_loss=1.0017 | val_acc=0.6187
Epoch 034 | loss=0.9440 | train_acc=0.6083 | val_loss=0.9984 | val_acc=0.6187
Epoch 035 | loss=0.9494 | train_acc=0.6087 | val_loss=0.9885 | val_acc=0.6179
Epoch 036 | loss=0.9495 | train_acc=0.6048 | val_loss=0.9894 | val_acc=0.6187
Epoch 037 | loss=0.9319 | train_acc=0.6083 | val_loss=0.9752 | val_acc=0.6187
Epoch 038 | loss=0.9230 | train_acc=0.6083 | val_loss=0.9629 | val_acc=0.6187
Epoch 039 | loss=0.9145 | train_acc=0.6085 | val_loss=0.9515 | val_acc=0.6187
Epoch 040 | loss=0.9132 | train_acc=0.6082 | val_loss=0.9369 | val_acc=0.6187
Epoch 041 | loss=0.9065 | train_acc=0.6081 | val_loss=0.9291 | val_acc=0.6187
Epoch 042 | loss=0.9058 | train_acc=0.6082 | val_loss=0.9342 | val_acc=0.6187
Epoch 043 | loss=0.9042 | train_acc=0.6084 | val_loss=0.9071 | val_acc=0.6187
Epoch 044 | loss=0.8997 | train_acc=0.6081 | val_loss=0.9076 | val_acc=0.6187
Epoch 045 | loss=0.9002 | train_acc=0.6083 | val_loss=0.9032 | val_acc=0.6187
Epoch 046 | loss=0.8984 | train_acc=0.6082 | val_loss=0.8991 | val_acc=0.6187
Epoch 047 | loss=0.8965 | train_acc=0.6084 | val_loss=0.9095 | val_acc=0.6187
Epoch 048 | loss=0.8976 | train_acc=0.6080 | val_loss=0.8984 | val_acc=0.6187
Epoch 049 | loss=0.8933 | train_acc=0.6080 | val_loss=0.8952 | val_acc=0.6187
Final Test Loss: 0.8161 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.1299 | train_acc=0.6616 | val_loss=3.4751 | val_acc=0.1977
Epoch 001 | loss=1.2524 | train_acc=0.5296 | val_loss=1.0532 | val_acc=0.6187
Epoch 002 | loss=1.0375 | train_acc=0.5952 | val_loss=1.0389 | val_acc=0.6187
Epoch 003 | loss=1.0293 | train_acc=0.6048 | val_loss=1.0177 | val_acc=0.6187
Epoch 004 | loss=1.0167 | train_acc=0.6079 | val_loss=0.9971 | val_acc=0.6187
Epoch 005 | loss=0.9855 | train_acc=0.6075 | val_loss=0.9369 | val_acc=0.6187
Epoch 006 | loss=0.9187 | train_acc=0.6058 | val_loss=0.9698 | val_acc=0.6187
Epoch 007 | loss=0.9578 | train_acc=0.6094 | val_loss=0.9014 | val_acc=0.6187
Epoch 008 | loss=0.8968 | train_acc=0.6089 | val_loss=0.9675 | val_acc=0.6187
Epoch 009 | loss=0.9126 | train_acc=0.6087 | val_loss=0.8549 | val_acc=0.6187
Epoch 010 | loss=0.8659 | train_acc=0.6087 | val_loss=0.9255 | val_acc=0.6187
Epoch 011 | loss=0.8848 | train_acc=0.6095 | val_loss=0.8386 | val_acc=0.6187
Epoch 012 | loss=0.8583 | train_acc=0.6093 | val_loss=0.8682 | val_acc=0.6187
Epoch 013 | loss=0.8540 | train_acc=0.6096 | val_loss=0.8661 | val_acc=0.6187
Epoch 014 | loss=0.8557 | train_acc=0.6095 | val_loss=0.8627 | val_acc=0.6187
Epoch 015 | loss=0.8469 | train_acc=0.6095 | val_loss=0.8835 | val_acc=0.6187
Epoch 016 | loss=0.8504 | train_acc=0.6095 | val_loss=0.8653 | val_acc=0.6187
Epoch 017 | loss=0.8376 | train_acc=0.6092 | val_loss=0.8962 | val_acc=0.6187
Epoch 018 | loss=0.8414 | train_acc=0.6092 | val_loss=0.8431 | val_acc=0.6187
Epoch 019 | loss=0.8387 | train_acc=0.6092 | val_loss=0.9110 | val_acc=0.6187
Epoch 020 | loss=0.8384 | train_acc=0.6097 | val_loss=1.0006 | val_acc=0.6187
Epoch 021 | loss=0.8609 | train_acc=0.6100 | val_loss=0.8654 | val_acc=0.6187
Epoch 022 | loss=0.8476 | train_acc=0.6100 | val_loss=0.8606 | val_acc=0.6187
Epoch 023 | loss=0.8477 | train_acc=0.6094 | val_loss=0.8195 | val_acc=0.6187
Epoch 024 | loss=0.8428 | train_acc=0.6099 | val_loss=0.8311 | val_acc=0.6187
Epoch 025 | loss=0.8858 | train_acc=0.6092 | val_loss=0.8384 | val_acc=0.6187
Epoch 026 | loss=0.8538 | train_acc=0.6082 | val_loss=0.9124 | val_acc=0.6187
Epoch 027 | loss=0.8826 | train_acc=0.6099 | val_loss=0.9426 | val_acc=0.6187
Epoch 028 | loss=0.8704 | train_acc=0.6036 | val_loss=0.9856 | val_acc=0.6187
Epoch 029 | loss=0.8864 | train_acc=0.6095 | val_loss=0.8340 | val_acc=0.6187
Epoch 030 | loss=0.8442 | train_acc=0.6096 | val_loss=0.8110 | val_acc=0.6187
Epoch 031 | loss=0.8380 | train_acc=0.6092 | val_loss=0.8629 | val_acc=0.6187
Epoch 032 | loss=0.8364 | train_acc=0.6093 | val_loss=0.8155 | val_acc=0.6187
Epoch 033 | loss=0.9267 | train_acc=0.6100 | val_loss=0.9196 | val_acc=0.6194
Epoch 034 | loss=0.9650 | train_acc=0.6020 | val_loss=0.9000 | val_acc=0.6179
Epoch 035 | loss=0.8810 | train_acc=0.6082 | val_loss=0.9526 | val_acc=0.6187
Epoch 036 | loss=0.9738 | train_acc=0.6307 | val_loss=1.0426 | val_acc=0.6331
Epoch 037 | loss=0.9207 | train_acc=0.5965 | val_loss=0.8487 | val_acc=0.6187
Epoch 038 | loss=0.9405 | train_acc=0.6079 | val_loss=0.9080 | val_acc=0.4698
Epoch 039 | loss=0.8992 | train_acc=0.6075 | val_loss=0.8327 | val_acc=0.6187
Epoch 040 | loss=0.8466 | train_acc=0.6098 | val_loss=0.8171 | val_acc=0.6187
Epoch 041 | loss=0.8749 | train_acc=0.6091 | val_loss=0.8096 | val_acc=0.6187
Epoch 042 | loss=0.8378 | train_acc=0.6088 | val_loss=0.8304 | val_acc=0.6187
Epoch 043 | loss=0.8517 | train_acc=0.6097 | val_loss=0.8086 | val_acc=0.6187
Epoch 044 | loss=0.8349 | train_acc=0.6094 | val_loss=0.8803 | val_acc=0.6187
Epoch 045 | loss=0.8612 | train_acc=0.6097 | val_loss=0.8041 | val_acc=0.6187
Epoch 046 | loss=0.8512 | train_acc=0.6093 | val_loss=0.8281 | val_acc=0.6187
Epoch 047 | loss=0.8799 | train_acc=0.6437 | val_loss=0.9575 | val_acc=0.6938
Epoch 048 | loss=0.9868 | train_acc=0.6026 | val_loss=0.9127 | val_acc=0.6390
Epoch 049 | loss=0.9354 | train_acc=0.6085 | val_loss=0.8139 | val_acc=0.6150
Final Test Loss: 0.7526 | Test Accuracy: 0.6285
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.2924 | train_acc=0.6807 | val_loss=2.0648 | val_acc=0.2044
Epoch 001 | loss=1.1928 | train_acc=0.5782 | val_loss=1.0930 | val_acc=0.4517
Epoch 002 | loss=1.0922 | train_acc=0.5716 | val_loss=1.0448 | val_acc=0.6187
Epoch 003 | loss=1.0465 | train_acc=0.6078 | val_loss=1.0260 | val_acc=0.6187
Epoch 004 | loss=1.0045 | train_acc=0.6094 | val_loss=1.0126 | val_acc=0.6187
Epoch 005 | loss=0.9832 | train_acc=0.6097 | val_loss=1.0007 | val_acc=0.6187
Epoch 006 | loss=0.9745 | train_acc=0.6099 | val_loss=0.9813 | val_acc=0.6187
Epoch 007 | loss=0.9679 | train_acc=0.6096 | val_loss=0.9578 | val_acc=0.6187
Epoch 008 | loss=0.9226 | train_acc=0.6095 | val_loss=0.9861 | val_acc=0.5846
Epoch 009 | loss=0.9331 | train_acc=0.6084 | val_loss=0.9156 | val_acc=0.6187
Epoch 010 | loss=0.8463 | train_acc=0.6081 | val_loss=0.8879 | val_acc=0.6187
Epoch 011 | loss=0.8211 | train_acc=0.6096 | val_loss=0.8698 | val_acc=0.6187
Epoch 012 | loss=0.7982 | train_acc=0.6092 | val_loss=0.8859 | val_acc=0.6187
Epoch 013 | loss=0.7889 | train_acc=0.6088 | val_loss=0.8349 | val_acc=0.6187
Epoch 014 | loss=0.7662 | train_acc=0.6096 | val_loss=0.8635 | val_acc=0.6187
Epoch 015 | loss=0.7632 | train_acc=0.6094 | val_loss=0.8105 | val_acc=0.6187
Epoch 016 | loss=0.7465 | train_acc=0.6098 | val_loss=0.8321 | val_acc=0.6187
Epoch 017 | loss=0.7474 | train_acc=0.6099 | val_loss=0.8301 | val_acc=0.6187
Epoch 018 | loss=0.7413 | train_acc=0.6099 | val_loss=0.8524 | val_acc=0.6187
Epoch 019 | loss=0.7395 | train_acc=0.6099 | val_loss=0.8054 | val_acc=0.6187
Epoch 020 | loss=0.7337 | train_acc=0.6098 | val_loss=0.8389 | val_acc=0.6187
Epoch 021 | loss=0.7392 | train_acc=0.6098 | val_loss=0.8135 | val_acc=0.6187
Epoch 022 | loss=0.7294 | train_acc=0.6100 | val_loss=0.8088 | val_acc=0.6187
Epoch 023 | loss=0.7295 | train_acc=0.6097 | val_loss=0.8022 | val_acc=0.6187
Epoch 024 | loss=0.7256 | train_acc=0.6100 | val_loss=0.8066 | val_acc=0.6187
Epoch 025 | loss=0.7278 | train_acc=0.6098 | val_loss=0.8799 | val_acc=0.6187
Epoch 026 | loss=0.7307 | train_acc=0.6098 | val_loss=0.8486 | val_acc=0.6187
Epoch 027 | loss=0.7301 | train_acc=0.6356 | val_loss=0.8039 | val_acc=0.7075
Epoch 028 | loss=0.7282 | train_acc=0.6783 | val_loss=0.8037 | val_acc=0.6842
Epoch 029 | loss=0.7274 | train_acc=0.6942 | val_loss=0.8209 | val_acc=0.6675
Epoch 030 | loss=0.7262 | train_acc=0.6942 | val_loss=0.8319 | val_acc=0.6453
Epoch 031 | loss=0.7539 | train_acc=0.6463 | val_loss=0.8176 | val_acc=0.7201
Epoch 032 | loss=0.7631 | train_acc=0.7018 | val_loss=0.7958 | val_acc=0.6690
Epoch 033 | loss=0.7599 | train_acc=0.6997 | val_loss=0.8298 | val_acc=0.6509
Epoch 034 | loss=0.7553 | train_acc=0.7050 | val_loss=1.0799 | val_acc=0.2110
Epoch 035 | loss=0.7933 | train_acc=0.6521 | val_loss=0.7993 | val_acc=0.7153
Epoch 036 | loss=0.7368 | train_acc=0.6777 | val_loss=0.7673 | val_acc=0.7105
Epoch 037 | loss=0.7305 | train_acc=0.6761 | val_loss=0.7589 | val_acc=0.7134
Epoch 038 | loss=0.7253 | train_acc=0.6786 | val_loss=0.7531 | val_acc=0.7183
Epoch 039 | loss=0.7193 | train_acc=0.6874 | val_loss=0.7662 | val_acc=0.7197
Epoch 040 | loss=0.7275 | train_acc=0.7026 | val_loss=0.7435 | val_acc=0.7208
Epoch 041 | loss=0.7288 | train_acc=0.6927 | val_loss=0.7490 | val_acc=0.7208
Epoch 042 | loss=0.7197 | train_acc=0.7111 | val_loss=0.7525 | val_acc=0.7120
Epoch 043 | loss=0.7255 | train_acc=0.7045 | val_loss=0.7481 | val_acc=0.7231
Epoch 044 | loss=0.7574 | train_acc=0.5838 | val_loss=0.8020 | val_acc=0.6372
Epoch 045 | loss=0.7420 | train_acc=0.6891 | val_loss=0.7573 | val_acc=0.7012
Epoch 046 | loss=0.7195 | train_acc=0.7107 | val_loss=0.8137 | val_acc=0.6664
Epoch 047 | loss=0.7487 | train_acc=0.7002 | val_loss=0.8698 | val_acc=0.5939
Epoch 048 | loss=0.7485 | train_acc=0.6780 | val_loss=0.8688 | val_acc=0.5924
Epoch 049 | loss=0.7548 | train_acc=0.6708 | val_loss=0.8587 | val_acc=0.6572
Final Test Loss: 0.8452 | Test Accuracy: 0.6656
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.1679 | train_acc=0.6552 | val_loss=3.0670 | val_acc=0.1929
Epoch 001 | loss=1.3749 | train_acc=0.4945 | val_loss=1.0581 | val_acc=0.6183
Epoch 002 | loss=1.0829 | train_acc=0.5844 | val_loss=1.0571 | val_acc=0.6187
Epoch 003 | loss=1.0717 | train_acc=0.5932 | val_loss=1.0359 | val_acc=0.6187
Epoch 004 | loss=1.0545 | train_acc=0.6001 | val_loss=1.0235 | val_acc=0.6187
Epoch 005 | loss=1.0223 | train_acc=0.6032 | val_loss=1.0161 | val_acc=0.6187
Epoch 006 | loss=1.0072 | train_acc=0.6026 | val_loss=1.0126 | val_acc=0.6187
Epoch 007 | loss=0.9874 | train_acc=0.6032 | val_loss=1.0135 | val_acc=0.6187
Epoch 008 | loss=0.9782 | train_acc=0.6025 | val_loss=1.0152 | val_acc=0.6187
Epoch 009 | loss=0.9693 | train_acc=0.6029 | val_loss=1.0177 | val_acc=0.6187
Epoch 010 | loss=0.9660 | train_acc=0.6017 | val_loss=1.0192 | val_acc=0.6187
Epoch 011 | loss=0.9625 | train_acc=0.6033 | val_loss=1.0201 | val_acc=0.6187
Epoch 012 | loss=0.9611 | train_acc=0.6030 | val_loss=1.0205 | val_acc=0.6187
Epoch 013 | loss=0.9597 | train_acc=0.6028 | val_loss=1.0208 | val_acc=0.6187
Epoch 014 | loss=0.9587 | train_acc=0.6027 | val_loss=1.0207 | val_acc=0.6187
Epoch 015 | loss=0.9585 | train_acc=0.6021 | val_loss=1.0203 | val_acc=0.6187
Epoch 016 | loss=0.9578 | train_acc=0.6015 | val_loss=1.0195 | val_acc=0.6187
Epoch 017 | loss=0.9583 | train_acc=0.6028 | val_loss=0.9907 | val_acc=0.6187
Epoch 018 | loss=1.0266 | train_acc=0.6007 | val_loss=0.9972 | val_acc=0.6131
Epoch 019 | loss=1.0155 | train_acc=0.6022 | val_loss=1.0018 | val_acc=0.6187
Epoch 020 | loss=1.0009 | train_acc=0.6012 | val_loss=1.0127 | val_acc=0.6187
Epoch 021 | loss=0.9931 | train_acc=0.6024 | val_loss=0.9913 | val_acc=0.6187
Epoch 022 | loss=1.0904 | train_acc=0.6017 | val_loss=1.0101 | val_acc=0.6187
Epoch 023 | loss=1.0257 | train_acc=0.6006 | val_loss=1.0133 | val_acc=0.6187
Epoch 024 | loss=0.9752 | train_acc=0.6019 | val_loss=1.0121 | val_acc=0.6187
Epoch 025 | loss=0.9845 | train_acc=0.6028 | val_loss=1.0106 | val_acc=0.6187
Epoch 026 | loss=0.9770 | train_acc=0.6020 | val_loss=1.0059 | val_acc=0.6187
Epoch 027 | loss=0.9814 | train_acc=0.6032 | val_loss=1.0079 | val_acc=0.6187
Epoch 028 | loss=0.9642 | train_acc=0.6032 | val_loss=1.0144 | val_acc=0.6187
Epoch 029 | loss=0.9589 | train_acc=0.6028 | val_loss=1.0160 | val_acc=0.6187
Epoch 030 | loss=0.9553 | train_acc=0.6027 | val_loss=1.0180 | val_acc=0.6187
Epoch 031 | loss=0.9524 | train_acc=0.6036 | val_loss=1.0196 | val_acc=0.6187
Epoch 032 | loss=0.9510 | train_acc=0.6030 | val_loss=1.0205 | val_acc=0.6187
Epoch 033 | loss=0.9503 | train_acc=0.6024 | val_loss=1.0213 | val_acc=0.6187
Epoch 034 | loss=0.9493 | train_acc=0.6022 | val_loss=1.0221 | val_acc=0.6187
Epoch 035 | loss=0.9476 | train_acc=0.6024 | val_loss=1.0219 | val_acc=0.6187
Epoch 036 | loss=0.9482 | train_acc=0.6032 | val_loss=1.0220 | val_acc=0.6187
Epoch 037 | loss=0.9489 | train_acc=0.6015 | val_loss=1.0225 | val_acc=0.6187
Epoch 038 | loss=0.9466 | train_acc=0.6033 | val_loss=1.0228 | val_acc=0.6187
Epoch 039 | loss=0.9466 | train_acc=0.6018 | val_loss=1.0228 | val_acc=0.6187
Epoch 040 | loss=0.9500 | train_acc=0.6027 | val_loss=1.0231 | val_acc=0.6187
Epoch 041 | loss=0.9446 | train_acc=0.6029 | val_loss=1.0199 | val_acc=0.6187
Epoch 042 | loss=0.9432 | train_acc=0.6020 | val_loss=1.0211 | val_acc=0.6187
Epoch 043 | loss=0.9435 | train_acc=0.6030 | val_loss=1.0211 | val_acc=0.6187
Epoch 044 | loss=0.9422 | train_acc=0.6019 | val_loss=1.0179 | val_acc=0.6187
Epoch 045 | loss=0.9407 | train_acc=0.6019 | val_loss=1.0175 | val_acc=0.6187
Epoch 046 | loss=0.9403 | train_acc=0.6020 | val_loss=1.0108 | val_acc=0.6187
Epoch 047 | loss=0.9364 | train_acc=0.6021 | val_loss=1.0078 | val_acc=0.6172
Epoch 048 | loss=1.0678 | train_acc=0.6262 | val_loss=5.4074 | val_acc=0.2181
Epoch 049 | loss=1.4047 | train_acc=0.6294 | val_loss=9.0941 | val_acc=0.2018
Final Test Loss: 11.1018 | Test Accuracy: 0.1760
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.2616 | train_acc=0.6566 | val_loss=2.4670 | val_acc=0.1944
Epoch 001 | loss=1.3591 | train_acc=0.4988 | val_loss=1.0643 | val_acc=0.6187
Epoch 002 | loss=1.0781 | train_acc=0.5892 | val_loss=1.0514 | val_acc=0.6187
Epoch 003 | loss=1.0535 | train_acc=0.6014 | val_loss=1.0329 | val_acc=0.6187
Epoch 004 | loss=1.0339 | train_acc=0.6079 | val_loss=1.0224 | val_acc=0.6187
Epoch 005 | loss=1.0126 | train_acc=0.6081 | val_loss=1.0144 | val_acc=0.6187
Epoch 006 | loss=0.9906 | train_acc=0.6084 | val_loss=1.0131 | val_acc=0.6187
Epoch 007 | loss=0.9778 | train_acc=0.6083 | val_loss=1.0157 | val_acc=0.6187
Epoch 008 | loss=0.9716 | train_acc=0.6079 | val_loss=1.0185 | val_acc=0.6187
Epoch 009 | loss=0.9669 | train_acc=0.6087 | val_loss=1.0202 | val_acc=0.6187
Epoch 010 | loss=0.9651 | train_acc=0.6088 | val_loss=1.0214 | val_acc=0.6187
Epoch 011 | loss=0.9633 | train_acc=0.6088 | val_loss=1.0220 | val_acc=0.6187
Epoch 012 | loss=0.9628 | train_acc=0.6081 | val_loss=1.0225 | val_acc=0.6187
Epoch 013 | loss=0.9605 | train_acc=0.6085 | val_loss=1.0229 | val_acc=0.6187
Epoch 014 | loss=0.9611 | train_acc=0.6084 | val_loss=1.0233 | val_acc=0.6187
Epoch 015 | loss=0.9604 | train_acc=0.6085 | val_loss=1.0229 | val_acc=0.6187
Epoch 016 | loss=0.9634 | train_acc=0.6077 | val_loss=1.0230 | val_acc=0.6187
Epoch 017 | loss=0.9619 | train_acc=0.6076 | val_loss=1.0213 | val_acc=0.6187
Epoch 018 | loss=0.9725 | train_acc=0.6244 | val_loss=1.0415 | val_acc=0.5383
Epoch 019 | loss=1.0589 | train_acc=0.5960 | val_loss=1.0170 | val_acc=0.6187
Epoch 020 | loss=0.9792 | train_acc=0.6076 | val_loss=1.0156 | val_acc=0.6187
Epoch 021 | loss=1.0190 | train_acc=0.6068 | val_loss=1.0243 | val_acc=0.6183
Epoch 022 | loss=1.1706 | train_acc=0.6070 | val_loss=1.0268 | val_acc=0.6187
Epoch 023 | loss=1.1004 | train_acc=0.6062 | val_loss=1.0287 | val_acc=0.6164
Epoch 024 | loss=1.0456 | train_acc=0.6058 | val_loss=1.0249 | val_acc=0.6187
Epoch 025 | loss=1.0358 | train_acc=0.6071 | val_loss=1.0186 | val_acc=0.6187
Epoch 026 | loss=1.0210 | train_acc=0.6081 | val_loss=1.0180 | val_acc=0.6187
Epoch 027 | loss=0.9780 | train_acc=0.6084 | val_loss=1.0142 | val_acc=0.6187
Epoch 028 | loss=0.9670 | train_acc=0.6079 | val_loss=1.0124 | val_acc=0.6187
Epoch 029 | loss=0.9619 | train_acc=0.6081 | val_loss=1.0116 | val_acc=0.6187
Epoch 030 | loss=0.9573 | train_acc=0.6082 | val_loss=1.0117 | val_acc=0.6187
Epoch 031 | loss=0.9539 | train_acc=0.6084 | val_loss=1.0109 | val_acc=0.6187
Epoch 032 | loss=0.9516 | train_acc=0.6078 | val_loss=1.0109 | val_acc=0.6187
Epoch 033 | loss=0.9477 | train_acc=0.6084 | val_loss=1.0131 | val_acc=0.6187
Epoch 034 | loss=0.9463 | train_acc=0.6085 | val_loss=1.0161 | val_acc=0.6187
Epoch 035 | loss=0.9421 | train_acc=0.6089 | val_loss=1.0180 | val_acc=0.6187
Epoch 036 | loss=0.9401 | train_acc=0.6084 | val_loss=1.0191 | val_acc=0.6187
Epoch 037 | loss=0.9380 | train_acc=0.6086 | val_loss=1.0179 | val_acc=0.6187
Epoch 038 | loss=0.9357 | train_acc=0.6085 | val_loss=1.0156 | val_acc=0.6187
Epoch 039 | loss=0.9349 | train_acc=0.6081 | val_loss=0.9973 | val_acc=0.6187
Epoch 040 | loss=0.9378 | train_acc=0.6086 | val_loss=0.9940 | val_acc=0.6187
Epoch 041 | loss=0.9322 | train_acc=0.6083 | val_loss=0.9861 | val_acc=0.6187
Epoch 042 | loss=0.9308 | train_acc=0.6084 | val_loss=0.9812 | val_acc=0.6187
Epoch 043 | loss=0.9267 | train_acc=0.6084 | val_loss=0.9802 | val_acc=0.6187
Epoch 044 | loss=0.9274 | train_acc=0.6085 | val_loss=0.9781 | val_acc=0.6187
Epoch 045 | loss=0.9261 | train_acc=0.6080 | val_loss=1.0051 | val_acc=0.6187
Epoch 046 | loss=0.9320 | train_acc=0.6088 | val_loss=0.9923 | val_acc=0.6187
Epoch 047 | loss=0.9263 | train_acc=0.6077 | val_loss=0.9823 | val_acc=0.6187
Epoch 048 | loss=0.9220 | train_acc=0.6082 | val_loss=0.9804 | val_acc=0.6187
Epoch 049 | loss=0.9216 | train_acc=0.6086 | val_loss=0.9680 | val_acc=0.6187
Final Test Loss: 0.8656 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.1288 | train_acc=0.6795 | val_loss=3.1388 | val_acc=0.1910
Epoch 001 | loss=1.3996 | train_acc=0.5011 | val_loss=1.0640 | val_acc=0.6187
Epoch 002 | loss=1.0794 | train_acc=0.6009 | val_loss=1.0442 | val_acc=0.6187
Epoch 003 | loss=1.0454 | train_acc=0.6073 | val_loss=1.0228 | val_acc=0.6187
Epoch 004 | loss=1.0171 | train_acc=0.6098 | val_loss=1.0140 | val_acc=0.6187
Epoch 005 | loss=1.0019 | train_acc=0.6091 | val_loss=1.0105 | val_acc=0.6187
Epoch 006 | loss=0.9831 | train_acc=0.6095 | val_loss=1.0095 | val_acc=0.6187
Epoch 007 | loss=0.9702 | train_acc=0.6097 | val_loss=1.0030 | val_acc=0.6187
Epoch 008 | loss=0.9702 | train_acc=0.6097 | val_loss=1.0096 | val_acc=0.6187
Epoch 009 | loss=0.9594 | train_acc=0.6093 | val_loss=0.9853 | val_acc=0.6187
Epoch 010 | loss=0.9648 | train_acc=0.6097 | val_loss=0.9555 | val_acc=0.6187
Epoch 011 | loss=1.0277 | train_acc=0.6096 | val_loss=1.0225 | val_acc=0.6183
Epoch 012 | loss=0.9830 | train_acc=0.6090 | val_loss=0.9573 | val_acc=0.6187
Epoch 013 | loss=0.9786 | train_acc=0.6094 | val_loss=0.9912 | val_acc=0.6187
Epoch 014 | loss=0.9739 | train_acc=0.6095 | val_loss=1.0152 | val_acc=0.6187
Epoch 015 | loss=0.9541 | train_acc=0.6098 | val_loss=0.9526 | val_acc=0.6187
Epoch 016 | loss=0.9691 | train_acc=0.6099 | val_loss=0.9488 | val_acc=0.6179
Epoch 017 | loss=0.9633 | train_acc=0.6086 | val_loss=0.9741 | val_acc=0.6187
Epoch 018 | loss=0.9560 | train_acc=0.6097 | val_loss=0.9358 | val_acc=0.6187
Epoch 019 | loss=0.9121 | train_acc=0.6086 | val_loss=0.9715 | val_acc=0.6187
Epoch 020 | loss=1.0792 | train_acc=0.6073 | val_loss=0.9951 | val_acc=0.6187
Epoch 021 | loss=1.0569 | train_acc=0.6095 | val_loss=1.0063 | val_acc=0.6187
Epoch 022 | loss=1.0176 | train_acc=0.6089 | val_loss=1.0078 | val_acc=0.6187
Epoch 023 | loss=0.9974 | train_acc=0.6083 | val_loss=0.9547 | val_acc=0.6187
Epoch 024 | loss=0.9304 | train_acc=0.6092 | val_loss=0.9429 | val_acc=0.6187
Epoch 025 | loss=0.9282 | train_acc=0.6085 | val_loss=0.9293 | val_acc=0.6187
Epoch 026 | loss=0.9089 | train_acc=0.6085 | val_loss=0.9088 | val_acc=0.6187
Epoch 027 | loss=0.8950 | train_acc=0.6091 | val_loss=0.9211 | val_acc=0.6187
Epoch 028 | loss=0.9345 | train_acc=0.6091 | val_loss=0.9433 | val_acc=0.6109
Epoch 029 | loss=0.9351 | train_acc=0.6042 | val_loss=0.9484 | val_acc=0.6187
Epoch 030 | loss=0.9090 | train_acc=0.6094 | val_loss=0.9217 | val_acc=0.6187
Epoch 031 | loss=0.8902 | train_acc=0.6084 | val_loss=0.8936 | val_acc=0.6187
Epoch 032 | loss=0.8684 | train_acc=0.6091 | val_loss=0.9005 | val_acc=0.6183
Epoch 033 | loss=0.8676 | train_acc=0.6086 | val_loss=0.8535 | val_acc=0.6187
Epoch 034 | loss=0.8358 | train_acc=0.6092 | val_loss=0.9142 | val_acc=0.6187
Epoch 035 | loss=0.8473 | train_acc=0.6092 | val_loss=0.8805 | val_acc=0.6187
Epoch 036 | loss=0.8378 | train_acc=0.6090 | val_loss=0.8512 | val_acc=0.6187
Epoch 037 | loss=0.8215 | train_acc=0.6096 | val_loss=0.8562 | val_acc=0.6187
Epoch 038 | loss=0.8263 | train_acc=0.6089 | val_loss=0.8343 | val_acc=0.6187
Epoch 039 | loss=0.8175 | train_acc=0.6095 | val_loss=0.7838 | val_acc=0.6187
Epoch 040 | loss=0.8080 | train_acc=0.6086 | val_loss=0.7815 | val_acc=0.6187
Epoch 041 | loss=0.8028 | train_acc=0.6183 | val_loss=0.7871 | val_acc=0.7231
Epoch 042 | loss=0.8452 | train_acc=0.5971 | val_loss=0.7898 | val_acc=0.6350
Epoch 043 | loss=0.8237 | train_acc=0.6643 | val_loss=0.8025 | val_acc=0.7157
Epoch 044 | loss=0.8979 | train_acc=0.6274 | val_loss=0.8610 | val_acc=0.7197
Epoch 045 | loss=0.8371 | train_acc=0.6553 | val_loss=1.1929 | val_acc=0.5243
Epoch 046 | loss=0.8930 | train_acc=0.5903 | val_loss=0.8186 | val_acc=0.6187
Epoch 047 | loss=0.8554 | train_acc=0.5786 | val_loss=0.8091 | val_acc=0.6187
Epoch 048 | loss=0.8266 | train_acc=0.6083 | val_loss=0.7694 | val_acc=0.7175
Epoch 049 | loss=0.8501 | train_acc=0.5763 | val_loss=0.7692 | val_acc=0.7216
Final Test Loss: 0.7135 | Test Accuracy: 0.7342
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.1530 | train_acc=0.6770 | val_loss=2.4048 | val_acc=0.1529
Epoch 001 | loss=1.3392 | train_acc=0.4998 | val_loss=1.0610 | val_acc=0.6187
Epoch 002 | loss=1.0559 | train_acc=0.5966 | val_loss=1.0343 | val_acc=0.6187
Epoch 003 | loss=1.0172 | train_acc=0.6096 | val_loss=1.0196 | val_acc=0.6187
Epoch 004 | loss=1.0064 | train_acc=0.6099 | val_loss=1.0120 | val_acc=0.6187
Epoch 005 | loss=0.9857 | train_acc=0.6099 | val_loss=1.0122 | val_acc=0.6187
Epoch 006 | loss=0.9726 | train_acc=0.6099 | val_loss=1.0130 | val_acc=0.6187
Epoch 007 | loss=0.9635 | train_acc=0.6099 | val_loss=1.0067 | val_acc=0.6187
Epoch 008 | loss=0.9552 | train_acc=0.6099 | val_loss=0.9795 | val_acc=0.6187
Epoch 009 | loss=0.9425 | train_acc=0.6097 | val_loss=0.9221 | val_acc=0.6187
Epoch 010 | loss=0.9714 | train_acc=0.6097 | val_loss=0.9516 | val_acc=0.6187
Epoch 011 | loss=0.9252 | train_acc=0.6099 | val_loss=0.9070 | val_acc=0.6187
Epoch 012 | loss=1.0634 | train_acc=0.6095 | val_loss=0.9696 | val_acc=0.6187
Epoch 013 | loss=1.0010 | train_acc=0.6096 | val_loss=0.9766 | val_acc=0.6187
Epoch 014 | loss=1.0029 | train_acc=0.6101 | val_loss=0.9453 | val_acc=0.6198
Epoch 015 | loss=0.9490 | train_acc=0.6090 | val_loss=0.9603 | val_acc=0.6187
Epoch 016 | loss=0.9337 | train_acc=0.6098 | val_loss=0.9025 | val_acc=0.6187
Epoch 017 | loss=0.8862 | train_acc=0.6097 | val_loss=0.9498 | val_acc=0.6187
Epoch 018 | loss=0.8856 | train_acc=0.6099 | val_loss=0.8935 | val_acc=0.6187
Epoch 019 | loss=0.8088 | train_acc=0.6096 | val_loss=0.8972 | val_acc=0.6187
Epoch 020 | loss=0.7907 | train_acc=0.6086 | val_loss=0.8867 | val_acc=0.6187
Epoch 021 | loss=0.7584 | train_acc=0.6091 | val_loss=0.8244 | val_acc=0.6187
Epoch 022 | loss=0.7498 | train_acc=0.6099 | val_loss=0.8060 | val_acc=0.6187
Epoch 023 | loss=0.7307 | train_acc=0.6099 | val_loss=0.7933 | val_acc=0.6187
Epoch 024 | loss=0.7234 | train_acc=0.6097 | val_loss=0.7859 | val_acc=0.6187
Epoch 025 | loss=0.7144 | train_acc=0.6098 | val_loss=0.7825 | val_acc=0.6187
Epoch 026 | loss=0.7104 | train_acc=0.6100 | val_loss=0.7797 | val_acc=0.6187
Epoch 027 | loss=0.7065 | train_acc=0.6099 | val_loss=0.7738 | val_acc=0.6187
Epoch 028 | loss=0.7010 | train_acc=0.6099 | val_loss=0.7702 | val_acc=0.6187
Epoch 029 | loss=0.6959 | train_acc=0.6230 | val_loss=0.7382 | val_acc=0.7301
Epoch 030 | loss=0.7051 | train_acc=0.6032 | val_loss=0.7610 | val_acc=0.6157
Epoch 031 | loss=0.6922 | train_acc=0.6083 | val_loss=0.7595 | val_acc=0.6187
Epoch 032 | loss=0.6888 | train_acc=0.6097 | val_loss=0.7547 | val_acc=0.6187
Epoch 033 | loss=0.6856 | train_acc=0.7276 | val_loss=0.7503 | val_acc=0.7305
Epoch 034 | loss=0.6819 | train_acc=0.7257 | val_loss=0.7466 | val_acc=0.7308
Epoch 035 | loss=0.6791 | train_acc=0.7241 | val_loss=0.7434 | val_acc=0.7312
Epoch 036 | loss=0.6770 | train_acc=0.7248 | val_loss=0.7391 | val_acc=0.7308
Epoch 037 | loss=0.6750 | train_acc=0.7249 | val_loss=0.7366 | val_acc=0.7308
Epoch 038 | loss=0.6732 | train_acc=0.7262 | val_loss=0.7316 | val_acc=0.7323
Epoch 039 | loss=0.6717 | train_acc=0.7260 | val_loss=0.7293 | val_acc=0.7308
Epoch 040 | loss=0.6706 | train_acc=0.7258 | val_loss=0.7242 | val_acc=0.7320
Epoch 041 | loss=0.6668 | train_acc=0.7269 | val_loss=0.7224 | val_acc=0.7327
Epoch 042 | loss=0.6680 | train_acc=0.7315 | val_loss=0.7185 | val_acc=0.7323
Epoch 043 | loss=0.6640 | train_acc=0.7318 | val_loss=0.7307 | val_acc=0.7227
Epoch 044 | loss=0.6666 | train_acc=0.7219 | val_loss=0.7204 | val_acc=0.7271
Epoch 045 | loss=0.6620 | train_acc=0.7321 | val_loss=0.8395 | val_acc=0.5894
Epoch 046 | loss=0.6652 | train_acc=0.7217 | val_loss=0.7101 | val_acc=0.7294
Epoch 047 | loss=0.6600 | train_acc=0.7309 | val_loss=0.7089 | val_acc=0.7305
Epoch 048 | loss=0.6581 | train_acc=0.7261 | val_loss=0.7059 | val_acc=0.7320
Epoch 049 | loss=0.6564 | train_acc=0.7267 | val_loss=0.7120 | val_acc=0.7316
Final Test Loss: 0.6593 | Test Accuracy: 0.7444
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.2392 | train_acc=0.6181 | val_loss=1.3210 | val_acc=0.2073
Epoch 001 | loss=1.1902 | train_acc=0.5031 | val_loss=1.0377 | val_acc=0.6187
Epoch 002 | loss=1.0708 | train_acc=0.5949 | val_loss=1.0309 | val_acc=0.6187
Epoch 003 | loss=1.0401 | train_acc=0.5979 | val_loss=1.0199 | val_acc=0.6187
Epoch 004 | loss=1.0104 | train_acc=0.6017 | val_loss=1.0124 | val_acc=0.6187
Epoch 005 | loss=0.9856 | train_acc=0.6021 | val_loss=1.0135 | val_acc=0.6187
Epoch 006 | loss=0.9728 | train_acc=0.6024 | val_loss=1.0160 | val_acc=0.6187
Epoch 007 | loss=0.9678 | train_acc=0.6027 | val_loss=1.0177 | val_acc=0.6187
Epoch 008 | loss=0.9649 | train_acc=0.6015 | val_loss=1.0186 | val_acc=0.6187
Epoch 009 | loss=0.9629 | train_acc=0.6016 | val_loss=1.0188 | val_acc=0.6187
Epoch 010 | loss=0.9612 | train_acc=0.6027 | val_loss=1.0190 | val_acc=0.6187
Epoch 011 | loss=0.9606 | train_acc=0.6036 | val_loss=1.0189 | val_acc=0.6187
Epoch 012 | loss=0.9603 | train_acc=0.6015 | val_loss=1.0186 | val_acc=0.6187
Epoch 013 | loss=0.9596 | train_acc=0.6013 | val_loss=1.0180 | val_acc=0.6187
Epoch 014 | loss=0.9583 | train_acc=0.6021 | val_loss=1.0175 | val_acc=0.6187
Epoch 015 | loss=0.9582 | train_acc=0.6028 | val_loss=1.0169 | val_acc=0.6187
Epoch 016 | loss=0.9567 | train_acc=0.6034 | val_loss=1.0162 | val_acc=0.6187
Epoch 017 | loss=0.9564 | train_acc=0.6019 | val_loss=1.0166 | val_acc=0.6187
Epoch 018 | loss=0.9587 | train_acc=0.6022 | val_loss=1.0168 | val_acc=0.6187
Epoch 019 | loss=0.9563 | train_acc=0.6020 | val_loss=1.0163 | val_acc=0.6187
Epoch 020 | loss=0.9543 | train_acc=0.6026 | val_loss=1.0152 | val_acc=0.6187
Epoch 021 | loss=0.9530 | train_acc=0.6035 | val_loss=1.0138 | val_acc=0.6187
Epoch 022 | loss=0.9516 | train_acc=0.6030 | val_loss=1.0125 | val_acc=0.6187
Epoch 023 | loss=0.9514 | train_acc=0.6020 | val_loss=1.0181 | val_acc=0.6187
Epoch 024 | loss=0.9585 | train_acc=0.6017 | val_loss=1.0162 | val_acc=0.6187
Epoch 025 | loss=0.9560 | train_acc=0.6022 | val_loss=1.0166 | val_acc=0.6187
Epoch 026 | loss=0.9539 | train_acc=0.6022 | val_loss=1.0167 | val_acc=0.6187
Epoch 027 | loss=0.9549 | train_acc=0.6024 | val_loss=1.0164 | val_acc=0.6187
Epoch 028 | loss=0.9524 | train_acc=0.6034 | val_loss=1.0162 | val_acc=0.6187
Epoch 029 | loss=0.9529 | train_acc=0.6021 | val_loss=1.0155 | val_acc=0.6187
Epoch 030 | loss=0.9518 | train_acc=0.6041 | val_loss=1.0149 | val_acc=0.6187
Epoch 031 | loss=0.9512 | train_acc=0.6019 | val_loss=1.0145 | val_acc=0.6187
Epoch 032 | loss=0.9505 | train_acc=0.6018 | val_loss=1.0140 | val_acc=0.6187
Epoch 033 | loss=0.9493 | train_acc=0.6033 | val_loss=1.0124 | val_acc=0.6187
Epoch 034 | loss=0.9517 | train_acc=0.6028 | val_loss=1.0125 | val_acc=0.6187
Epoch 035 | loss=0.9515 | train_acc=0.6028 | val_loss=1.0117 | val_acc=0.6187
Epoch 036 | loss=0.9441 | train_acc=0.6018 | val_loss=1.0148 | val_acc=0.6187
Epoch 037 | loss=0.8824 | train_acc=0.7069 | val_loss=7.1889 | val_acc=0.2047
Epoch 038 | loss=1.1667 | train_acc=0.6854 | val_loss=8.1307 | val_acc=0.2036
Epoch 039 | loss=1.7881 | train_acc=0.5924 | val_loss=1.2475 | val_acc=0.2425
Epoch 040 | loss=1.4121 | train_acc=0.5885 | val_loss=1.1446 | val_acc=0.2925
Epoch 041 | loss=1.2851 | train_acc=0.6142 | val_loss=2.9527 | val_acc=0.3317
Epoch 042 | loss=1.2466 | train_acc=0.5858 | val_loss=1.0189 | val_acc=0.6187
Epoch 043 | loss=0.9869 | train_acc=0.6030 | val_loss=1.0151 | val_acc=0.6187
Epoch 044 | loss=0.9759 | train_acc=0.6024 | val_loss=1.0131 | val_acc=0.6187
Epoch 045 | loss=0.9683 | train_acc=0.6010 | val_loss=1.0126 | val_acc=0.6187
Epoch 046 | loss=0.9627 | train_acc=0.6025 | val_loss=1.0132 | val_acc=0.6187
Epoch 047 | loss=0.9583 | train_acc=0.6015 | val_loss=1.0146 | val_acc=0.6187
Epoch 048 | loss=0.9549 | train_acc=0.6018 | val_loss=1.0165 | val_acc=0.6187
Epoch 049 | loss=0.9528 | train_acc=0.6033 | val_loss=1.0183 | val_acc=0.6187
Final Test Loss: 0.9248 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.2650 | train_acc=0.6227 | val_loss=1.1894 | val_acc=0.2207
Epoch 001 | loss=1.1586 | train_acc=0.5088 | val_loss=1.0347 | val_acc=0.6187
Epoch 002 | loss=1.0413 | train_acc=0.6052 | val_loss=1.0239 | val_acc=0.6187
Epoch 003 | loss=1.0152 | train_acc=0.6060 | val_loss=1.0171 | val_acc=0.6187
Epoch 004 | loss=0.9958 | train_acc=0.6075 | val_loss=1.0123 | val_acc=0.6187
Epoch 005 | loss=0.9768 | train_acc=0.6092 | val_loss=1.0143 | val_acc=0.6187
Epoch 006 | loss=0.9697 | train_acc=0.6084 | val_loss=1.0163 | val_acc=0.6187
Epoch 007 | loss=0.9664 | train_acc=0.6080 | val_loss=1.0174 | val_acc=0.6187
Epoch 008 | loss=0.9634 | train_acc=0.6089 | val_loss=1.0182 | val_acc=0.6187
Epoch 009 | loss=0.9620 | train_acc=0.6086 | val_loss=1.0183 | val_acc=0.6187
Epoch 010 | loss=0.9617 | train_acc=0.6083 | val_loss=1.0180 | val_acc=0.6187
Epoch 011 | loss=0.9605 | train_acc=0.6087 | val_loss=1.0180 | val_acc=0.6187
Epoch 012 | loss=0.9598 | train_acc=0.6085 | val_loss=1.0178 | val_acc=0.6187
Epoch 013 | loss=0.9592 | train_acc=0.6082 | val_loss=1.0174 | val_acc=0.6187
Epoch 014 | loss=0.9588 | train_acc=0.6085 | val_loss=1.0170 | val_acc=0.6187
Epoch 015 | loss=0.9582 | train_acc=0.6085 | val_loss=1.0165 | val_acc=0.6187
Epoch 016 | loss=0.9573 | train_acc=0.6084 | val_loss=1.0161 | val_acc=0.6187
Epoch 017 | loss=0.9563 | train_acc=0.6086 | val_loss=1.0157 | val_acc=0.6187
Epoch 018 | loss=0.9565 | train_acc=0.6085 | val_loss=1.0152 | val_acc=0.6187
Epoch 019 | loss=0.9554 | train_acc=0.6084 | val_loss=1.0148 | val_acc=0.6187
Epoch 020 | loss=0.9553 | train_acc=0.6084 | val_loss=1.0146 | val_acc=0.6187
Epoch 021 | loss=0.9543 | train_acc=0.6080 | val_loss=1.0139 | val_acc=0.6187
Epoch 022 | loss=0.9536 | train_acc=0.6081 | val_loss=1.0134 | val_acc=0.6187
Epoch 023 | loss=0.9524 | train_acc=0.6082 | val_loss=1.0138 | val_acc=0.6187
Epoch 024 | loss=0.9543 | train_acc=0.6076 | val_loss=1.0130 | val_acc=0.6187
Epoch 025 | loss=0.9512 | train_acc=0.6078 | val_loss=1.0128 | val_acc=0.6187
Epoch 026 | loss=0.9510 | train_acc=0.6087 | val_loss=1.0127 | val_acc=0.6187
Epoch 027 | loss=0.9511 | train_acc=0.6085 | val_loss=1.0114 | val_acc=0.6187
Epoch 028 | loss=0.9440 | train_acc=0.6085 | val_loss=1.0137 | val_acc=0.6187
Epoch 029 | loss=0.9387 | train_acc=0.6283 | val_loss=2.8268 | val_acc=0.2899
Epoch 030 | loss=0.9442 | train_acc=0.6349 | val_loss=1.1998 | val_acc=0.4221
Epoch 031 | loss=1.1226 | train_acc=0.6111 | val_loss=1.9733 | val_acc=0.3058
Epoch 032 | loss=1.6163 | train_acc=0.5908 | val_loss=1.0210 | val_acc=0.6164
Epoch 033 | loss=1.1192 | train_acc=0.6044 | val_loss=1.0191 | val_acc=0.6187
Epoch 034 | loss=1.0274 | train_acc=0.6081 | val_loss=1.0090 | val_acc=0.6187
Epoch 035 | loss=0.9935 | train_acc=0.6079 | val_loss=1.0042 | val_acc=0.6187
Epoch 036 | loss=0.9729 | train_acc=0.6089 | val_loss=1.0054 | val_acc=0.6187
Epoch 037 | loss=0.9645 | train_acc=0.6080 | val_loss=1.0157 | val_acc=0.6187
Epoch 038 | loss=0.9596 | train_acc=0.6088 | val_loss=1.0144 | val_acc=0.6187
Epoch 039 | loss=0.9564 | train_acc=0.6085 | val_loss=1.0157 | val_acc=0.6187
Epoch 040 | loss=0.9544 | train_acc=0.6081 | val_loss=1.0168 | val_acc=0.6187
Epoch 041 | loss=0.9525 | train_acc=0.6082 | val_loss=1.0173 | val_acc=0.6187
Epoch 042 | loss=0.9507 | train_acc=0.6085 | val_loss=1.0175 | val_acc=0.6187
Epoch 043 | loss=0.9498 | train_acc=0.6081 | val_loss=1.0174 | val_acc=0.6187
Epoch 044 | loss=0.9488 | train_acc=0.6076 | val_loss=1.0160 | val_acc=0.6187
Epoch 045 | loss=0.9472 | train_acc=0.6078 | val_loss=1.0132 | val_acc=0.6187
Epoch 046 | loss=0.9461 | train_acc=0.6087 | val_loss=1.0101 | val_acc=0.6187
Epoch 047 | loss=0.9447 | train_acc=0.6083 | val_loss=1.0095 | val_acc=0.6187
Epoch 048 | loss=0.9429 | train_acc=0.6085 | val_loss=1.0046 | val_acc=0.6187
Epoch 049 | loss=0.9425 | train_acc=0.6086 | val_loss=1.0086 | val_acc=0.6187
Final Test Loss: 0.9115 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.0963 | train_acc=0.6460 | val_loss=1.4995 | val_acc=0.2107
Epoch 001 | loss=1.2151 | train_acc=0.5021 | val_loss=1.0255 | val_acc=0.6187
Epoch 002 | loss=1.0070 | train_acc=0.6097 | val_loss=1.0178 | val_acc=0.6187
Epoch 003 | loss=1.0228 | train_acc=0.6085 | val_loss=1.0112 | val_acc=0.6187
Epoch 004 | loss=0.9950 | train_acc=0.6097 | val_loss=1.0126 | val_acc=0.6187
Epoch 005 | loss=0.9777 | train_acc=0.6096 | val_loss=1.0148 | val_acc=0.6187
Epoch 006 | loss=0.9689 | train_acc=0.6098 | val_loss=1.0160 | val_acc=0.6187
Epoch 007 | loss=0.9660 | train_acc=0.6094 | val_loss=1.0160 | val_acc=0.6187
Epoch 008 | loss=0.9610 | train_acc=0.6098 | val_loss=1.0136 | val_acc=0.6187
Epoch 009 | loss=0.9676 | train_acc=0.6098 | val_loss=1.0100 | val_acc=0.6187
Epoch 010 | loss=0.9555 | train_acc=0.6096 | val_loss=1.0179 | val_acc=0.6187
Epoch 011 | loss=0.9693 | train_acc=0.6096 | val_loss=1.0157 | val_acc=0.6187
Epoch 012 | loss=0.9627 | train_acc=0.6093 | val_loss=1.0158 | val_acc=0.6187
Epoch 013 | loss=0.9578 | train_acc=0.6094 | val_loss=1.0153 | val_acc=0.6187
Epoch 014 | loss=0.9551 | train_acc=0.6096 | val_loss=1.0143 | val_acc=0.6187
Epoch 015 | loss=0.9519 | train_acc=0.6097 | val_loss=1.0104 | val_acc=0.6187
Epoch 016 | loss=0.9368 | train_acc=0.6099 | val_loss=1.0055 | val_acc=0.6187
Epoch 017 | loss=0.9570 | train_acc=0.6100 | val_loss=0.9926 | val_acc=0.6187
Epoch 018 | loss=0.9280 | train_acc=0.6096 | val_loss=0.9649 | val_acc=0.6187
Epoch 019 | loss=0.9792 | train_acc=0.6600 | val_loss=0.9998 | val_acc=0.4354
Epoch 020 | loss=1.1059 | train_acc=0.6397 | val_loss=6.3149 | val_acc=0.2121
Epoch 021 | loss=1.5225 | train_acc=0.5964 | val_loss=1.0212 | val_acc=0.6187
Epoch 022 | loss=1.2248 | train_acc=0.6257 | val_loss=1.0704 | val_acc=0.4513
Epoch 023 | loss=1.2321 | train_acc=0.5962 | val_loss=1.0217 | val_acc=0.6187
Epoch 024 | loss=1.0306 | train_acc=0.6067 | val_loss=1.0143 | val_acc=0.6187
Epoch 025 | loss=0.9928 | train_acc=0.6091 | val_loss=1.0108 | val_acc=0.6187
Epoch 026 | loss=0.9797 | train_acc=0.6092 | val_loss=1.0126 | val_acc=0.6187
Epoch 027 | loss=0.9713 | train_acc=0.6094 | val_loss=1.0136 | val_acc=0.6187
Epoch 028 | loss=0.9653 | train_acc=0.6094 | val_loss=1.0155 | val_acc=0.6187
Epoch 029 | loss=0.9603 | train_acc=0.6099 | val_loss=1.0176 | val_acc=0.6187
Epoch 030 | loss=0.9566 | train_acc=0.6100 | val_loss=1.0195 | val_acc=0.6187
Epoch 031 | loss=0.9547 | train_acc=0.6099 | val_loss=1.0209 | val_acc=0.6187
Epoch 032 | loss=0.9511 | train_acc=0.6101 | val_loss=1.0224 | val_acc=0.6187
Epoch 033 | loss=0.9475 | train_acc=0.6098 | val_loss=1.0241 | val_acc=0.6187
Epoch 034 | loss=0.9464 | train_acc=0.6093 | val_loss=1.0255 | val_acc=0.6187
Epoch 035 | loss=0.9419 | train_acc=0.6098 | val_loss=1.0272 | val_acc=0.6187
Epoch 036 | loss=0.9372 | train_acc=0.6098 | val_loss=1.0296 | val_acc=0.6187
Epoch 037 | loss=0.9313 | train_acc=0.6096 | val_loss=1.0329 | val_acc=0.6187
Epoch 038 | loss=0.9274 | train_acc=0.6099 | val_loss=1.0367 | val_acc=0.6187
Epoch 039 | loss=1.0195 | train_acc=0.6154 | val_loss=1.3915 | val_acc=0.4047
Epoch 040 | loss=1.0260 | train_acc=0.6041 | val_loss=0.9955 | val_acc=0.6187
Epoch 041 | loss=1.0034 | train_acc=0.6090 | val_loss=0.9919 | val_acc=0.6187
Epoch 042 | loss=0.9608 | train_acc=0.6094 | val_loss=0.9904 | val_acc=0.6187
Epoch 043 | loss=0.9616 | train_acc=0.6095 | val_loss=1.0038 | val_acc=0.6187
Epoch 044 | loss=0.9507 | train_acc=0.6096 | val_loss=1.0156 | val_acc=0.6187
Epoch 045 | loss=0.9434 | train_acc=0.6095 | val_loss=1.0135 | val_acc=0.6187
Epoch 046 | loss=0.9407 | train_acc=0.6100 | val_loss=1.0026 | val_acc=0.6187
Epoch 047 | loss=0.9326 | train_acc=0.6094 | val_loss=1.0083 | val_acc=0.6187
Epoch 048 | loss=0.9379 | train_acc=0.6098 | val_loss=0.9909 | val_acc=0.6187
Epoch 049 | loss=0.9180 | train_acc=0.6098 | val_loss=0.9665 | val_acc=0.6187
Final Test Loss: 0.8633 | Test Accuracy: 0.6322
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.0185 | train_acc=0.6706 | val_loss=1.9652 | val_acc=0.2099
Epoch 001 | loss=1.2772 | train_acc=0.5049 | val_loss=1.0509 | val_acc=0.6187
Epoch 002 | loss=1.0805 | train_acc=0.6009 | val_loss=1.0263 | val_acc=0.6187
Epoch 003 | loss=1.0191 | train_acc=0.6084 | val_loss=1.0157 | val_acc=0.6187
Epoch 004 | loss=1.0203 | train_acc=0.6093 | val_loss=1.0073 | val_acc=0.6187
Epoch 005 | loss=0.9877 | train_acc=0.6098 | val_loss=1.0029 | val_acc=0.6187
Epoch 006 | loss=0.9751 | train_acc=0.6098 | val_loss=1.0032 | val_acc=0.6187
Epoch 007 | loss=0.9667 | train_acc=0.6100 | val_loss=1.0031 | val_acc=0.6187
Epoch 008 | loss=0.9640 | train_acc=0.6099 | val_loss=1.0147 | val_acc=0.6187
Epoch 009 | loss=0.9543 | train_acc=0.6097 | val_loss=0.9806 | val_acc=0.6187
Epoch 010 | loss=0.9673 | train_acc=0.6100 | val_loss=1.0062 | val_acc=0.6187
Epoch 011 | loss=0.9515 | train_acc=0.6099 | val_loss=0.9756 | val_acc=0.6187
Epoch 012 | loss=0.9612 | train_acc=0.6099 | val_loss=0.9713 | val_acc=0.6187
Epoch 013 | loss=0.9514 | train_acc=0.6100 | val_loss=0.9756 | val_acc=0.6187
Epoch 014 | loss=0.9201 | train_acc=0.6100 | val_loss=0.9825 | val_acc=0.6187
Epoch 015 | loss=1.0764 | train_acc=0.6249 | val_loss=1.0561 | val_acc=0.4128
Epoch 016 | loss=1.0722 | train_acc=0.5992 | val_loss=1.0077 | val_acc=0.6187
Epoch 017 | loss=0.9949 | train_acc=0.6095 | val_loss=0.9961 | val_acc=0.6187
Epoch 018 | loss=0.9825 | train_acc=0.6098 | val_loss=0.9875 | val_acc=0.6187
Epoch 019 | loss=0.9637 | train_acc=0.6099 | val_loss=0.9786 | val_acc=0.6187
Epoch 020 | loss=0.9528 | train_acc=0.6099 | val_loss=0.9777 | val_acc=0.6187
Epoch 021 | loss=0.9570 | train_acc=0.6098 | val_loss=0.9588 | val_acc=0.6187
Epoch 022 | loss=0.9268 | train_acc=0.6096 | val_loss=0.9796 | val_acc=0.6187
Epoch 023 | loss=0.9569 | train_acc=0.6092 | val_loss=0.9580 | val_acc=0.6187
Epoch 024 | loss=0.8920 | train_acc=0.6095 | val_loss=0.9797 | val_acc=0.6187
Epoch 025 | loss=0.9516 | train_acc=0.6098 | val_loss=0.9566 | val_acc=0.6187
Epoch 026 | loss=0.9065 | train_acc=0.6099 | val_loss=0.9592 | val_acc=0.6183
Epoch 027 | loss=0.8798 | train_acc=0.6099 | val_loss=0.9269 | val_acc=0.6183
Epoch 028 | loss=0.8319 | train_acc=0.6092 | val_loss=0.9331 | val_acc=0.6183
Epoch 029 | loss=0.8355 | train_acc=0.6095 | val_loss=0.9229 | val_acc=0.6187
Epoch 030 | loss=0.7979 | train_acc=0.6095 | val_loss=0.9239 | val_acc=0.6187
Epoch 031 | loss=0.7873 | train_acc=0.6098 | val_loss=0.9165 | val_acc=0.6187
Epoch 032 | loss=0.7723 | train_acc=0.6098 | val_loss=0.9192 | val_acc=0.6187
Epoch 033 | loss=0.7749 | train_acc=0.6093 | val_loss=0.9106 | val_acc=0.6187
Epoch 034 | loss=0.7484 | train_acc=0.6098 | val_loss=0.8814 | val_acc=0.6187
Epoch 035 | loss=0.7467 | train_acc=0.6096 | val_loss=0.9421 | val_acc=0.6187
Epoch 036 | loss=0.7562 | train_acc=0.6098 | val_loss=0.9626 | val_acc=0.6187
Epoch 037 | loss=0.7609 | train_acc=0.6097 | val_loss=0.9176 | val_acc=0.6187
Epoch 038 | loss=0.7509 | train_acc=0.6100 | val_loss=0.9850 | val_acc=0.6187
Epoch 039 | loss=0.7610 | train_acc=0.6097 | val_loss=0.9613 | val_acc=0.6187
Epoch 040 | loss=0.7502 | train_acc=0.6099 | val_loss=0.9635 | val_acc=0.6187
Epoch 041 | loss=0.7462 | train_acc=0.6100 | val_loss=0.9921 | val_acc=0.6187
Epoch 042 | loss=0.7559 | train_acc=0.6098 | val_loss=0.9642 | val_acc=0.6187
Epoch 043 | loss=0.7548 | train_acc=0.6099 | val_loss=0.9486 | val_acc=0.6187
Epoch 044 | loss=0.7340 | train_acc=0.6136 | val_loss=0.9420 | val_acc=0.6076
Epoch 045 | loss=0.7439 | train_acc=0.6095 | val_loss=0.9917 | val_acc=0.6479
Epoch 046 | loss=0.7418 | train_acc=0.6057 | val_loss=0.9253 | val_acc=0.6187
Epoch 047 | loss=0.7263 | train_acc=0.6099 | val_loss=0.9557 | val_acc=0.6187
Epoch 048 | loss=0.7246 | train_acc=0.6097 | val_loss=0.9641 | val_acc=0.6187
Epoch 049 | loss=0.7258 | train_acc=0.6098 | val_loss=0.9602 | val_acc=0.3950
Final Test Loss: 0.9401 | Test Accuracy: 0.3919
