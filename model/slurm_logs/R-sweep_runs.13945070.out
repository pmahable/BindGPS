## SLURM PROLOG ###############################################################
##    Job ID : 13945070
##  Job Name : sweep_runs
##  Nodelist : gpu2005
##      CPUs : 1
##  Mem/Node : 32000 MB
## Directory : /oscar/data/larschan/shared_data/BindGPS/model
##   Job Started : Mon Nov 10 01:24:15 EST 2025
###############################################################################
Running model parameter sweep with GAT (NO SVM)
Create sweep with ID: n5hg56og
Sweep URL: https://wandb.ai/bind-gps/gps-gat-model-no-svm-parameter-test/sweeps/n5hg56og
Created sweep: n5hg56og
Starting 24 runs...
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=16.5936 | train_acc=0.5124 | val_loss=14.7390 | val_acc=0.1529
Epoch 001 | loss=8.4909 | train_acc=0.4577 | val_loss=1.0993 | val_acc=0.2951
Epoch 002 | loss=1.7910 | train_acc=0.4664 | val_loss=1.0738 | val_acc=0.6113
Epoch 003 | loss=1.2956 | train_acc=0.5490 | val_loss=1.0618 | val_acc=0.6187
Epoch 004 | loss=1.1803 | train_acc=0.5758 | val_loss=1.0497 | val_acc=0.6187
Epoch 005 | loss=1.1227 | train_acc=0.5896 | val_loss=1.0401 | val_acc=0.6187
Epoch 006 | loss=1.0725 | train_acc=0.5955 | val_loss=1.0324 | val_acc=0.6187
Epoch 007 | loss=1.0407 | train_acc=0.5982 | val_loss=1.0265 | val_acc=0.6187
Epoch 008 | loss=1.0291 | train_acc=0.6016 | val_loss=1.0217 | val_acc=0.6187
Epoch 009 | loss=1.0161 | train_acc=0.6018 | val_loss=1.0183 | val_acc=0.6187
Epoch 010 | loss=1.0057 | train_acc=0.6036 | val_loss=1.0154 | val_acc=0.6187
Epoch 011 | loss=0.9951 | train_acc=0.6040 | val_loss=1.0134 | val_acc=0.6187
Epoch 012 | loss=0.9908 | train_acc=0.6045 | val_loss=1.0118 | val_acc=0.6187
Epoch 013 | loss=0.9829 | train_acc=0.6050 | val_loss=1.0106 | val_acc=0.6187
Epoch 014 | loss=0.9802 | train_acc=0.6053 | val_loss=1.0096 | val_acc=0.6187
Epoch 015 | loss=0.9744 | train_acc=0.6057 | val_loss=1.0087 | val_acc=0.6187
Epoch 016 | loss=0.9733 | train_acc=0.6059 | val_loss=1.0080 | val_acc=0.6187
Epoch 017 | loss=0.9660 | train_acc=0.6062 | val_loss=1.0075 | val_acc=0.6187
Epoch 018 | loss=0.9632 | train_acc=0.6065 | val_loss=1.0071 | val_acc=0.6187
Epoch 019 | loss=0.9630 | train_acc=0.6064 | val_loss=1.0068 | val_acc=0.6187
Epoch 020 | loss=0.9610 | train_acc=0.6072 | val_loss=1.0067 | val_acc=0.6187
Epoch 021 | loss=0.9580 | train_acc=0.6071 | val_loss=1.0067 | val_acc=0.6187
Epoch 022 | loss=0.9574 | train_acc=0.6071 | val_loss=1.0066 | val_acc=0.6187
Epoch 023 | loss=0.9540 | train_acc=0.6071 | val_loss=1.0067 | val_acc=0.6187
Epoch 024 | loss=0.9524 | train_acc=0.6075 | val_loss=1.0068 | val_acc=0.6187
Epoch 025 | loss=0.9537 | train_acc=0.6077 | val_loss=1.0068 | val_acc=0.6187
Epoch 026 | loss=0.9495 | train_acc=0.6077 | val_loss=1.0070 | val_acc=0.6187
Epoch 027 | loss=0.9498 | train_acc=0.6068 | val_loss=1.0072 | val_acc=0.6187
Epoch 028 | loss=0.9439 | train_acc=0.6076 | val_loss=1.0074 | val_acc=0.6187
Epoch 029 | loss=0.9443 | train_acc=0.6078 | val_loss=1.0077 | val_acc=0.6187
Epoch 030 | loss=0.9411 | train_acc=0.6079 | val_loss=1.0078 | val_acc=0.6187
Epoch 031 | loss=0.9420 | train_acc=0.6074 | val_loss=1.0078 | val_acc=0.6187
Epoch 032 | loss=0.9403 | train_acc=0.6082 | val_loss=1.0081 | val_acc=0.6187
Epoch 033 | loss=0.9385 | train_acc=0.6066 | val_loss=1.0082 | val_acc=0.6187
Epoch 034 | loss=0.9352 | train_acc=0.6079 | val_loss=1.0077 | val_acc=0.6187
Epoch 035 | loss=0.9337 | train_acc=0.6074 | val_loss=1.0077 | val_acc=0.6187
Epoch 036 | loss=0.9339 | train_acc=0.6076 | val_loss=1.0081 | val_acc=0.6187
Epoch 037 | loss=0.9355 | train_acc=0.6078 | val_loss=1.0082 | val_acc=0.6187
Epoch 038 | loss=0.9323 | train_acc=0.6082 | val_loss=1.0072 | val_acc=0.6187
Epoch 039 | loss=0.9334 | train_acc=0.6074 | val_loss=1.0066 | val_acc=0.6187
Epoch 040 | loss=0.9295 | train_acc=0.6079 | val_loss=1.0077 | val_acc=0.6187
Epoch 041 | loss=0.9280 | train_acc=0.6082 | val_loss=1.0084 | val_acc=0.6187
Epoch 042 | loss=0.9271 | train_acc=0.6077 | val_loss=1.0052 | val_acc=0.6187
Epoch 043 | loss=0.9261 | train_acc=0.6077 | val_loss=1.0056 | val_acc=0.6187
Epoch 044 | loss=0.9281 | train_acc=0.6072 | val_loss=1.0042 | val_acc=0.6187
Epoch 045 | loss=0.9279 | train_acc=0.6074 | val_loss=1.0014 | val_acc=0.6187
Epoch 046 | loss=0.9232 | train_acc=0.6074 | val_loss=1.0111 | val_acc=0.6187
Epoch 047 | loss=0.9222 | train_acc=0.6078 | val_loss=1.0122 | val_acc=0.6187
Epoch 048 | loss=0.9217 | train_acc=0.6083 | val_loss=1.0129 | val_acc=0.6187
Epoch 049 | loss=0.9225 | train_acc=0.6079 | val_loss=1.0130 | val_acc=0.6187
Final Test Loss: 0.9203 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=6.6673 | train_acc=0.5812 | val_loss=2.5268 | val_acc=0.2099
Epoch 001 | loss=2.3960 | train_acc=0.4900 | val_loss=1.0468 | val_acc=0.6187
Epoch 002 | loss=1.1549 | train_acc=0.5514 | val_loss=1.0328 | val_acc=0.6187
Epoch 003 | loss=1.0689 | train_acc=0.5839 | val_loss=1.0229 | val_acc=0.6187
Epoch 004 | loss=1.0259 | train_acc=0.5947 | val_loss=1.0168 | val_acc=0.6187
Epoch 005 | loss=1.0026 | train_acc=0.6001 | val_loss=1.0135 | val_acc=0.6187
Epoch 006 | loss=0.9880 | train_acc=0.6031 | val_loss=1.0121 | val_acc=0.6187
Epoch 007 | loss=0.9772 | train_acc=0.6046 | val_loss=1.0121 | val_acc=0.6187
Epoch 008 | loss=0.9710 | train_acc=0.6057 | val_loss=1.0129 | val_acc=0.6187
Epoch 009 | loss=0.9652 | train_acc=0.6056 | val_loss=1.0140 | val_acc=0.6187
Epoch 010 | loss=0.9593 | train_acc=0.6061 | val_loss=1.0153 | val_acc=0.6187
Epoch 011 | loss=0.9565 | train_acc=0.6063 | val_loss=1.0166 | val_acc=0.6187
Epoch 012 | loss=0.9537 | train_acc=0.6073 | val_loss=1.0176 | val_acc=0.6187
Epoch 013 | loss=0.9520 | train_acc=0.6065 | val_loss=1.0183 | val_acc=0.6187
Epoch 014 | loss=0.9498 | train_acc=0.6073 | val_loss=1.0187 | val_acc=0.6187
Epoch 015 | loss=0.9486 | train_acc=0.6077 | val_loss=1.0182 | val_acc=0.6187
Epoch 016 | loss=0.9501 | train_acc=0.6079 | val_loss=1.0185 | val_acc=0.6187
Epoch 017 | loss=0.9470 | train_acc=0.6080 | val_loss=1.0185 | val_acc=0.6187
Epoch 018 | loss=0.9462 | train_acc=0.6075 | val_loss=1.0172 | val_acc=0.6187
Epoch 019 | loss=0.9447 | train_acc=0.6076 | val_loss=1.0151 | val_acc=0.6187
Epoch 020 | loss=0.9432 | train_acc=0.6083 | val_loss=1.0139 | val_acc=0.6187
Epoch 021 | loss=0.9445 | train_acc=0.6084 | val_loss=1.0114 | val_acc=0.6187
Epoch 022 | loss=0.9427 | train_acc=0.6082 | val_loss=1.0095 | val_acc=0.6187
Epoch 023 | loss=0.9428 | train_acc=0.6077 | val_loss=1.0091 | val_acc=0.6187
Epoch 024 | loss=0.9410 | train_acc=0.6084 | val_loss=1.0079 | val_acc=0.6187
Epoch 025 | loss=0.9403 | train_acc=0.6078 | val_loss=1.0109 | val_acc=0.6187
Epoch 026 | loss=0.9410 | train_acc=0.6075 | val_loss=1.0125 | val_acc=0.6187
Epoch 027 | loss=0.9383 | train_acc=0.6084 | val_loss=1.0133 | val_acc=0.6187
Epoch 028 | loss=0.9380 | train_acc=0.6087 | val_loss=1.0189 | val_acc=0.6187
Epoch 029 | loss=0.9393 | train_acc=0.6083 | val_loss=1.0171 | val_acc=0.6187
Epoch 030 | loss=0.9365 | train_acc=0.6073 | val_loss=1.0169 | val_acc=0.6187
Epoch 031 | loss=0.9336 | train_acc=0.6085 | val_loss=1.0162 | val_acc=0.6187
Epoch 032 | loss=0.9341 | train_acc=0.6080 | val_loss=1.0113 | val_acc=0.6187
Epoch 033 | loss=0.9331 | train_acc=0.6081 | val_loss=1.0092 | val_acc=0.6187
Epoch 034 | loss=0.9297 | train_acc=0.6080 | val_loss=1.0048 | val_acc=0.6187
Epoch 035 | loss=0.9294 | train_acc=0.6073 | val_loss=0.9918 | val_acc=0.6187
Epoch 036 | loss=0.9280 | train_acc=0.6077 | val_loss=0.9931 | val_acc=0.6187
Epoch 037 | loss=0.9262 | train_acc=0.6078 | val_loss=0.9771 | val_acc=0.6187
Epoch 038 | loss=0.9263 | train_acc=0.6082 | val_loss=1.0004 | val_acc=0.6187
Epoch 039 | loss=0.9318 | train_acc=0.6086 | val_loss=0.9968 | val_acc=0.6187
Epoch 040 | loss=0.9246 | train_acc=0.6081 | val_loss=0.9863 | val_acc=0.6187
Epoch 041 | loss=0.9246 | train_acc=0.6071 | val_loss=0.9682 | val_acc=0.6187
Epoch 042 | loss=0.9242 | train_acc=0.6080 | val_loss=0.9729 | val_acc=0.6187
Epoch 043 | loss=0.9240 | train_acc=0.6079 | val_loss=0.9633 | val_acc=0.6187
Epoch 044 | loss=0.9223 | train_acc=0.6076 | val_loss=0.9545 | val_acc=0.6187
Epoch 045 | loss=0.9247 | train_acc=0.6084 | val_loss=0.9893 | val_acc=0.6187
Epoch 046 | loss=0.9254 | train_acc=0.6077 | val_loss=0.9765 | val_acc=0.6187
Epoch 047 | loss=0.9220 | train_acc=0.6082 | val_loss=0.9639 | val_acc=0.6187
Epoch 048 | loss=0.9205 | train_acc=0.6078 | val_loss=0.9605 | val_acc=0.6187
Epoch 049 | loss=0.9159 | train_acc=0.6080 | val_loss=0.9529 | val_acc=0.6187
Final Test Loss: 0.8608 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=20.7667 | train_acc=0.6139 | val_loss=32.2571 | val_acc=0.1847
Epoch 001 | loss=14.7382 | train_acc=0.5145 | val_loss=1.3335 | val_acc=0.5583
Epoch 002 | loss=3.6800 | train_acc=0.5135 | val_loss=1.0548 | val_acc=0.6164
Epoch 003 | loss=1.6099 | train_acc=0.5222 | val_loss=1.0430 | val_acc=0.6187
Epoch 004 | loss=1.2620 | train_acc=0.5591 | val_loss=1.0308 | val_acc=0.6187
Epoch 005 | loss=1.1568 | train_acc=0.5752 | val_loss=1.0207 | val_acc=0.6187
Epoch 006 | loss=1.0844 | train_acc=0.5876 | val_loss=1.0118 | val_acc=0.6187
Epoch 007 | loss=1.0544 | train_acc=0.5934 | val_loss=1.0051 | val_acc=0.6187
Epoch 008 | loss=1.0260 | train_acc=0.5966 | val_loss=1.0003 | val_acc=0.6187
Epoch 009 | loss=1.0189 | train_acc=0.5993 | val_loss=0.9975 | val_acc=0.6187
Epoch 010 | loss=1.0075 | train_acc=0.6014 | val_loss=0.9951 | val_acc=0.6187
Epoch 011 | loss=0.9939 | train_acc=0.6028 | val_loss=0.9941 | val_acc=0.6187
Epoch 012 | loss=0.9847 | train_acc=0.6033 | val_loss=0.9939 | val_acc=0.6187
Epoch 013 | loss=0.9805 | train_acc=0.6045 | val_loss=0.9934 | val_acc=0.6187
Epoch 014 | loss=0.9774 | train_acc=0.6047 | val_loss=0.9934 | val_acc=0.6187
Epoch 015 | loss=0.9689 | train_acc=0.6053 | val_loss=0.9935 | val_acc=0.6187
Epoch 016 | loss=0.9695 | train_acc=0.6061 | val_loss=0.9994 | val_acc=0.6187
Epoch 017 | loss=0.9635 | train_acc=0.6062 | val_loss=1.0005 | val_acc=0.6187
Epoch 018 | loss=0.9608 | train_acc=0.6068 | val_loss=1.0006 | val_acc=0.6187
Epoch 019 | loss=0.9583 | train_acc=0.6069 | val_loss=0.9992 | val_acc=0.6187
Epoch 020 | loss=0.9556 | train_acc=0.6061 | val_loss=1.0006 | val_acc=0.6187
Epoch 021 | loss=0.9534 | train_acc=0.6066 | val_loss=1.0007 | val_acc=0.6187
Epoch 022 | loss=0.9507 | train_acc=0.6074 | val_loss=1.0036 | val_acc=0.6187
Epoch 023 | loss=0.9461 | train_acc=0.6067 | val_loss=1.0051 | val_acc=0.6187
Epoch 024 | loss=0.9470 | train_acc=0.6065 | val_loss=1.0058 | val_acc=0.6187
Epoch 025 | loss=0.9428 | train_acc=0.6075 | val_loss=1.0034 | val_acc=0.6187
Epoch 026 | loss=0.9372 | train_acc=0.6074 | val_loss=1.0080 | val_acc=0.6187
Epoch 027 | loss=0.9358 | train_acc=0.6071 | val_loss=0.9990 | val_acc=0.6187
Epoch 028 | loss=0.9305 | train_acc=0.6073 | val_loss=0.9944 | val_acc=0.6187
Epoch 029 | loss=0.9358 | train_acc=0.6072 | val_loss=0.9905 | val_acc=0.6187
Epoch 030 | loss=0.9263 | train_acc=0.6074 | val_loss=0.9888 | val_acc=0.6187
Epoch 031 | loss=0.9310 | train_acc=0.6071 | val_loss=0.9913 | val_acc=0.6187
Epoch 032 | loss=0.9290 | train_acc=0.6066 | val_loss=0.9918 | val_acc=0.6187
Epoch 033 | loss=0.9209 | train_acc=0.6073 | val_loss=0.9876 | val_acc=0.6187
Epoch 034 | loss=0.9239 | train_acc=0.6067 | val_loss=0.9915 | val_acc=0.6187
Epoch 035 | loss=0.9229 | train_acc=0.6076 | val_loss=0.9885 | val_acc=0.6187
Epoch 036 | loss=0.9270 | train_acc=0.6079 | val_loss=0.9929 | val_acc=0.6187
Epoch 037 | loss=0.9268 | train_acc=0.6072 | val_loss=0.9918 | val_acc=0.6187
Epoch 038 | loss=0.9197 | train_acc=0.6077 | val_loss=0.9825 | val_acc=0.6187
Epoch 039 | loss=0.9141 | train_acc=0.6074 | val_loss=0.9780 | val_acc=0.6187
Epoch 040 | loss=0.9167 | train_acc=0.6072 | val_loss=0.9857 | val_acc=0.6187
Epoch 041 | loss=0.9162 | train_acc=0.6075 | val_loss=0.9787 | val_acc=0.6187
Epoch 042 | loss=0.9150 | train_acc=0.6072 | val_loss=0.9825 | val_acc=0.6187
Epoch 043 | loss=0.9126 | train_acc=0.6065 | val_loss=0.9774 | val_acc=0.6187
Epoch 044 | loss=0.9136 | train_acc=0.6068 | val_loss=0.9776 | val_acc=0.6187
Epoch 045 | loss=0.9122 | train_acc=0.6066 | val_loss=0.9777 | val_acc=0.6187
Epoch 046 | loss=0.9124 | train_acc=0.6067 | val_loss=0.9842 | val_acc=0.6187
Epoch 047 | loss=0.9117 | train_acc=0.6070 | val_loss=0.9878 | val_acc=0.6187
Epoch 048 | loss=0.9126 | train_acc=0.6071 | val_loss=0.9913 | val_acc=0.6187
Epoch 049 | loss=0.9107 | train_acc=0.6071 | val_loss=0.9859 | val_acc=0.6187
Final Test Loss: 0.9073 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=11.0191 | train_acc=0.5863 | val_loss=5.6907 | val_acc=0.1548
Epoch 001 | loss=3.6385 | train_acc=0.4672 | val_loss=1.0456 | val_acc=0.6187
Epoch 002 | loss=1.3196 | train_acc=0.5138 | val_loss=1.0626 | val_acc=0.5990
Epoch 003 | loss=1.1428 | train_acc=0.5508 | val_loss=1.0542 | val_acc=0.6187
Epoch 004 | loss=1.0786 | train_acc=0.5686 | val_loss=1.0450 | val_acc=0.6187
Epoch 005 | loss=1.0456 | train_acc=0.5916 | val_loss=1.0376 | val_acc=0.6187
Epoch 006 | loss=1.0269 | train_acc=0.5969 | val_loss=1.0313 | val_acc=0.6187
Epoch 007 | loss=1.0100 | train_acc=0.6018 | val_loss=1.0257 | val_acc=0.6187
Epoch 008 | loss=1.0028 | train_acc=0.6029 | val_loss=1.0215 | val_acc=0.6187
Epoch 009 | loss=0.9917 | train_acc=0.6044 | val_loss=1.0179 | val_acc=0.6187
Epoch 010 | loss=0.9846 | train_acc=0.6047 | val_loss=1.0151 | val_acc=0.6187
Epoch 011 | loss=0.9781 | train_acc=0.6062 | val_loss=1.0129 | val_acc=0.6187
Epoch 012 | loss=0.9735 | train_acc=0.6062 | val_loss=1.0112 | val_acc=0.6187
Epoch 013 | loss=0.9698 | train_acc=0.6067 | val_loss=1.0098 | val_acc=0.6187
Epoch 014 | loss=0.9655 | train_acc=0.6072 | val_loss=1.0089 | val_acc=0.6187
Epoch 015 | loss=0.9623 | train_acc=0.6072 | val_loss=1.0082 | val_acc=0.6187
Epoch 016 | loss=0.9608 | train_acc=0.6072 | val_loss=1.0078 | val_acc=0.6187
Epoch 017 | loss=0.9570 | train_acc=0.6080 | val_loss=1.0075 | val_acc=0.6187
Epoch 018 | loss=0.9567 | train_acc=0.6077 | val_loss=1.0073 | val_acc=0.6187
Epoch 019 | loss=0.9538 | train_acc=0.6081 | val_loss=1.0074 | val_acc=0.6187
Epoch 020 | loss=0.9521 | train_acc=0.6077 | val_loss=1.0075 | val_acc=0.6187
Epoch 021 | loss=0.9490 | train_acc=0.6076 | val_loss=1.0077 | val_acc=0.6187
Epoch 022 | loss=0.9498 | train_acc=0.6074 | val_loss=1.0080 | val_acc=0.6187
Epoch 023 | loss=0.9486 | train_acc=0.6080 | val_loss=1.0083 | val_acc=0.6187
Epoch 024 | loss=0.9468 | train_acc=0.6079 | val_loss=1.0086 | val_acc=0.6187
Epoch 025 | loss=0.9467 | train_acc=0.6080 | val_loss=1.0088 | val_acc=0.6187
Epoch 026 | loss=0.9447 | train_acc=0.6080 | val_loss=1.0090 | val_acc=0.6187
Epoch 027 | loss=0.9443 | train_acc=0.6078 | val_loss=1.0093 | val_acc=0.6187
Epoch 028 | loss=0.9427 | train_acc=0.6081 | val_loss=1.0092 | val_acc=0.6187
Epoch 029 | loss=0.9416 | train_acc=0.6082 | val_loss=1.0089 | val_acc=0.6187
Epoch 030 | loss=0.9412 | train_acc=0.6079 | val_loss=1.0089 | val_acc=0.6187
Epoch 031 | loss=0.9400 | train_acc=0.6082 | val_loss=1.0085 | val_acc=0.6187
Epoch 032 | loss=0.9402 | train_acc=0.6080 | val_loss=1.0086 | val_acc=0.6187
Epoch 033 | loss=0.9397 | train_acc=0.6076 | val_loss=1.0086 | val_acc=0.6187
Epoch 034 | loss=0.9382 | train_acc=0.6083 | val_loss=1.0091 | val_acc=0.6187
Epoch 035 | loss=0.9377 | train_acc=0.6081 | val_loss=1.0093 | val_acc=0.6187
Epoch 036 | loss=0.9361 | train_acc=0.6085 | val_loss=1.0068 | val_acc=0.6187
Epoch 037 | loss=0.9359 | train_acc=0.6077 | val_loss=1.0053 | val_acc=0.6187
Epoch 038 | loss=0.9344 | train_acc=0.6085 | val_loss=1.0041 | val_acc=0.6187
Epoch 039 | loss=0.9340 | train_acc=0.6081 | val_loss=0.9920 | val_acc=0.6187
Epoch 040 | loss=0.9327 | train_acc=0.6078 | val_loss=0.9907 | val_acc=0.6187
Epoch 041 | loss=0.9314 | train_acc=0.6082 | val_loss=0.9874 | val_acc=0.6187
Epoch 042 | loss=0.9307 | train_acc=0.6084 | val_loss=0.9868 | val_acc=0.6187
Epoch 043 | loss=0.9302 | train_acc=0.6084 | val_loss=0.9839 | val_acc=0.6187
Epoch 044 | loss=0.9280 | train_acc=0.6085 | val_loss=0.9887 | val_acc=0.6187
Epoch 045 | loss=0.9266 | train_acc=0.6082 | val_loss=0.9829 | val_acc=0.6187
Epoch 046 | loss=0.9259 | train_acc=0.6084 | val_loss=0.9850 | val_acc=0.6187
Epoch 047 | loss=0.9272 | train_acc=0.6084 | val_loss=0.9831 | val_acc=0.6187
Epoch 048 | loss=0.9280 | train_acc=0.6075 | val_loss=1.0022 | val_acc=0.6187
Epoch 049 | loss=0.9277 | train_acc=0.6076 | val_loss=0.9801 | val_acc=0.6187
Final Test Loss: 0.8849 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=27.1949 | train_acc=0.5920 | val_loss=10.6615 | val_acc=0.1873
Epoch 001 | loss=18.7425 | train_acc=0.5243 | val_loss=2.1707 | val_acc=0.3184
Epoch 002 | loss=7.0259 | train_acc=0.5214 | val_loss=1.0647 | val_acc=0.5591
Epoch 003 | loss=2.2414 | train_acc=0.5255 | val_loss=1.0489 | val_acc=0.6187
Epoch 004 | loss=1.4803 | train_acc=0.5265 | val_loss=1.0339 | val_acc=0.6187
Epoch 005 | loss=1.2159 | train_acc=0.5598 | val_loss=1.0233 | val_acc=0.6187
Epoch 006 | loss=1.1404 | train_acc=0.5760 | val_loss=1.0135 | val_acc=0.6187
Epoch 007 | loss=1.0934 | train_acc=0.5833 | val_loss=1.0079 | val_acc=0.6187
Epoch 008 | loss=1.0554 | train_acc=0.5904 | val_loss=1.0036 | val_acc=0.6187
Epoch 009 | loss=1.0353 | train_acc=0.5939 | val_loss=1.0005 | val_acc=0.6187
Epoch 010 | loss=1.0176 | train_acc=0.5970 | val_loss=0.9988 | val_acc=0.6187
Epoch 011 | loss=1.0118 | train_acc=0.5996 | val_loss=0.9982 | val_acc=0.6187
Epoch 012 | loss=1.0040 | train_acc=0.6004 | val_loss=0.9973 | val_acc=0.6187
Epoch 013 | loss=0.9943 | train_acc=0.6013 | val_loss=0.9966 | val_acc=0.6187
Epoch 014 | loss=0.9853 | train_acc=0.6028 | val_loss=0.9957 | val_acc=0.6187
Epoch 015 | loss=0.9811 | train_acc=0.6032 | val_loss=0.9955 | val_acc=0.6187
Epoch 016 | loss=0.9780 | train_acc=0.6041 | val_loss=0.9955 | val_acc=0.6187
Epoch 017 | loss=0.9797 | train_acc=0.6044 | val_loss=0.9955 | val_acc=0.6187
Epoch 018 | loss=0.9741 | train_acc=0.6051 | val_loss=0.9950 | val_acc=0.6187
Epoch 019 | loss=0.9699 | train_acc=0.6054 | val_loss=0.9948 | val_acc=0.6187
Epoch 020 | loss=0.9700 | train_acc=0.6055 | val_loss=0.9969 | val_acc=0.6187
Epoch 021 | loss=0.9674 | train_acc=0.6053 | val_loss=0.9974 | val_acc=0.6187
Epoch 022 | loss=0.9627 | train_acc=0.6065 | val_loss=0.9990 | val_acc=0.6187
Epoch 023 | loss=0.9620 | train_acc=0.6067 | val_loss=0.9991 | val_acc=0.6187
Epoch 024 | loss=0.9630 | train_acc=0.6067 | val_loss=0.9997 | val_acc=0.6187
Epoch 025 | loss=0.9579 | train_acc=0.6070 | val_loss=1.0000 | val_acc=0.6187
Epoch 026 | loss=0.9572 | train_acc=0.6071 | val_loss=1.0000 | val_acc=0.6187
Epoch 027 | loss=0.9571 | train_acc=0.6070 | val_loss=1.0001 | val_acc=0.6187
Epoch 028 | loss=0.9548 | train_acc=0.6069 | val_loss=1.0018 | val_acc=0.6187
Epoch 029 | loss=0.9551 | train_acc=0.6075 | val_loss=1.0029 | val_acc=0.6187
Epoch 030 | loss=0.9536 | train_acc=0.6071 | val_loss=1.0035 | val_acc=0.6187
Epoch 031 | loss=0.9510 | train_acc=0.6076 | val_loss=1.0046 | val_acc=0.6187
Epoch 032 | loss=0.9512 | train_acc=0.6077 | val_loss=1.0056 | val_acc=0.6187
Epoch 033 | loss=0.9502 | train_acc=0.6076 | val_loss=1.0061 | val_acc=0.6187
Epoch 034 | loss=0.9488 | train_acc=0.6079 | val_loss=1.0057 | val_acc=0.6187
Epoch 035 | loss=0.9482 | train_acc=0.6074 | val_loss=1.0059 | val_acc=0.6187
Epoch 036 | loss=0.9479 | train_acc=0.6078 | val_loss=1.0069 | val_acc=0.6187
Epoch 037 | loss=0.9483 | train_acc=0.6079 | val_loss=1.0072 | val_acc=0.6187
Epoch 038 | loss=0.9471 | train_acc=0.6073 | val_loss=1.0081 | val_acc=0.6187
Epoch 039 | loss=0.9475 | train_acc=0.6077 | val_loss=1.0090 | val_acc=0.6187
Epoch 040 | loss=0.9450 | train_acc=0.6083 | val_loss=1.0101 | val_acc=0.6187
Epoch 041 | loss=0.9456 | train_acc=0.6079 | val_loss=1.0105 | val_acc=0.6187
Epoch 042 | loss=0.9449 | train_acc=0.6077 | val_loss=1.0117 | val_acc=0.6187
Epoch 043 | loss=0.9446 | train_acc=0.6081 | val_loss=1.0127 | val_acc=0.6187
Epoch 044 | loss=0.9442 | train_acc=0.6082 | val_loss=1.0134 | val_acc=0.6187
Epoch 045 | loss=0.9428 | train_acc=0.6080 | val_loss=1.0144 | val_acc=0.6187
Epoch 046 | loss=0.9429 | train_acc=0.6082 | val_loss=1.0144 | val_acc=0.6187
Epoch 047 | loss=0.9413 | train_acc=0.6081 | val_loss=1.0157 | val_acc=0.6187
Epoch 048 | loss=0.9415 | train_acc=0.6082 | val_loss=1.0164 | val_acc=0.6187
Epoch 049 | loss=0.9409 | train_acc=0.6085 | val_loss=1.0168 | val_acc=0.6187
Final Test Loss: 0.9237 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=8.0500 | train_acc=0.6302 | val_loss=2.6770 | val_acc=0.1925
Epoch 001 | loss=2.3494 | train_acc=0.5150 | val_loss=1.0623 | val_acc=0.6187
Epoch 002 | loss=1.0509 | train_acc=0.5843 | val_loss=1.0468 | val_acc=0.6187
Epoch 003 | loss=1.0197 | train_acc=0.6009 | val_loss=1.0358 | val_acc=0.6187
Epoch 004 | loss=1.0041 | train_acc=0.6031 | val_loss=1.0276 | val_acc=0.6187
Epoch 005 | loss=0.9909 | train_acc=0.6054 | val_loss=1.0215 | val_acc=0.6187
Epoch 006 | loss=0.9829 | train_acc=0.6059 | val_loss=1.0171 | val_acc=0.6187
Epoch 007 | loss=0.9752 | train_acc=0.6068 | val_loss=1.0140 | val_acc=0.6187
Epoch 008 | loss=0.9688 | train_acc=0.6075 | val_loss=1.0117 | val_acc=0.6187
Epoch 009 | loss=0.9661 | train_acc=0.6071 | val_loss=1.0102 | val_acc=0.6187
Epoch 010 | loss=0.9618 | train_acc=0.6077 | val_loss=1.0091 | val_acc=0.6187
Epoch 011 | loss=0.9594 | train_acc=0.6079 | val_loss=1.0085 | val_acc=0.6187
Epoch 012 | loss=0.9575 | train_acc=0.6079 | val_loss=1.0081 | val_acc=0.6187
Epoch 013 | loss=0.9544 | train_acc=0.6076 | val_loss=1.0080 | val_acc=0.6187
Epoch 014 | loss=0.9536 | train_acc=0.6075 | val_loss=1.0081 | val_acc=0.6187
Epoch 015 | loss=0.9512 | train_acc=0.6080 | val_loss=1.0083 | val_acc=0.6187
Epoch 016 | loss=0.9489 | train_acc=0.6081 | val_loss=1.0085 | val_acc=0.6187
Epoch 017 | loss=0.9475 | train_acc=0.6080 | val_loss=1.0088 | val_acc=0.6187
Epoch 018 | loss=0.9470 | train_acc=0.6082 | val_loss=1.0092 | val_acc=0.6187
Epoch 019 | loss=0.9466 | train_acc=0.6082 | val_loss=1.0096 | val_acc=0.6187
Epoch 020 | loss=0.9452 | train_acc=0.6085 | val_loss=1.0101 | val_acc=0.6187
Epoch 021 | loss=0.9439 | train_acc=0.6082 | val_loss=1.0106 | val_acc=0.6187
Epoch 022 | loss=0.9435 | train_acc=0.6084 | val_loss=1.0111 | val_acc=0.6187
Epoch 023 | loss=0.9424 | train_acc=0.6080 | val_loss=1.0116 | val_acc=0.6187
Epoch 024 | loss=0.9424 | train_acc=0.6084 | val_loss=1.0120 | val_acc=0.6187
Epoch 025 | loss=0.9415 | train_acc=0.6084 | val_loss=1.0126 | val_acc=0.6187
Epoch 026 | loss=0.9421 | train_acc=0.6078 | val_loss=1.0131 | val_acc=0.6187
Epoch 027 | loss=0.9407 | train_acc=0.6086 | val_loss=1.0136 | val_acc=0.6187
Epoch 028 | loss=0.9410 | train_acc=0.6083 | val_loss=1.0141 | val_acc=0.6187
Epoch 029 | loss=0.9406 | train_acc=0.6080 | val_loss=1.0146 | val_acc=0.6187
Epoch 030 | loss=0.9404 | train_acc=0.6087 | val_loss=1.0151 | val_acc=0.6187
Epoch 031 | loss=0.9394 | train_acc=0.6086 | val_loss=1.0156 | val_acc=0.6187
Epoch 032 | loss=0.9389 | train_acc=0.6084 | val_loss=1.0160 | val_acc=0.6187
Epoch 033 | loss=0.9386 | train_acc=0.6084 | val_loss=1.0165 | val_acc=0.6187
Epoch 034 | loss=0.9390 | train_acc=0.6079 | val_loss=1.0169 | val_acc=0.6187
Epoch 035 | loss=0.9392 | train_acc=0.6083 | val_loss=1.0174 | val_acc=0.6187
Epoch 036 | loss=0.9388 | train_acc=0.6082 | val_loss=1.0177 | val_acc=0.6187
Epoch 037 | loss=0.9384 | train_acc=0.6082 | val_loss=1.0181 | val_acc=0.6187
Epoch 038 | loss=0.9383 | train_acc=0.6083 | val_loss=1.0185 | val_acc=0.6187
Epoch 039 | loss=0.9374 | train_acc=0.6086 | val_loss=1.0189 | val_acc=0.6187
Epoch 040 | loss=0.9385 | train_acc=0.6080 | val_loss=1.0192 | val_acc=0.6187
Epoch 041 | loss=0.9377 | train_acc=0.6082 | val_loss=1.0195 | val_acc=0.6187
Epoch 042 | loss=0.9377 | train_acc=0.6082 | val_loss=1.0199 | val_acc=0.6187
Epoch 043 | loss=0.9378 | train_acc=0.6080 | val_loss=1.0202 | val_acc=0.6187
Epoch 044 | loss=0.9377 | train_acc=0.6081 | val_loss=1.0205 | val_acc=0.6187
Epoch 045 | loss=0.9374 | train_acc=0.6082 | val_loss=1.0208 | val_acc=0.6187
Epoch 046 | loss=0.9371 | train_acc=0.6083 | val_loss=1.0210 | val_acc=0.6187
Epoch 047 | loss=0.9374 | train_acc=0.6078 | val_loss=1.0213 | val_acc=0.6187
Epoch 048 | loss=0.9369 | train_acc=0.6084 | val_loss=1.0216 | val_acc=0.6187
Epoch 049 | loss=0.9368 | train_acc=0.6083 | val_loss=1.0219 | val_acc=0.6187
Final Test Loss: 0.9182 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=10.1230 | train_acc=0.5286 | val_loss=5.4809 | val_acc=0.1659
Epoch 001 | loss=2.4750 | train_acc=0.4386 | val_loss=1.1018 | val_acc=0.2532
Epoch 002 | loss=1.1852 | train_acc=0.4781 | val_loss=1.0758 | val_acc=0.6187
Epoch 003 | loss=1.0991 | train_acc=0.5992 | val_loss=1.0560 | val_acc=0.6187
Epoch 004 | loss=1.0571 | train_acc=0.6028 | val_loss=1.0414 | val_acc=0.6187
Epoch 005 | loss=1.0312 | train_acc=0.6049 | val_loss=1.0308 | val_acc=0.6187
Epoch 006 | loss=1.0154 | train_acc=0.6054 | val_loss=1.0233 | val_acc=0.6187
Epoch 007 | loss=1.0024 | train_acc=0.6069 | val_loss=1.0181 | val_acc=0.6187
Epoch 008 | loss=0.9928 | train_acc=0.6068 | val_loss=1.0141 | val_acc=0.6187
Epoch 009 | loss=0.9862 | train_acc=0.6067 | val_loss=1.0112 | val_acc=0.6187
Epoch 010 | loss=0.9807 | train_acc=0.6072 | val_loss=1.0091 | val_acc=0.6187
Epoch 011 | loss=0.9753 | train_acc=0.6076 | val_loss=1.0077 | val_acc=0.6187
Epoch 012 | loss=0.9693 | train_acc=0.6076 | val_loss=1.0068 | val_acc=0.6187
Epoch 013 | loss=0.9664 | train_acc=0.6077 | val_loss=1.0063 | val_acc=0.6187
Epoch 014 | loss=0.9616 | train_acc=0.6077 | val_loss=1.0059 | val_acc=0.6187
Epoch 015 | loss=0.9586 | train_acc=0.6079 | val_loss=1.0057 | val_acc=0.6187
Epoch 016 | loss=0.9583 | train_acc=0.6078 | val_loss=1.0057 | val_acc=0.6187
Epoch 017 | loss=0.9546 | train_acc=0.6079 | val_loss=1.0061 | val_acc=0.6187
Epoch 018 | loss=0.9532 | train_acc=0.6083 | val_loss=1.0065 | val_acc=0.6187
Epoch 019 | loss=0.9517 | train_acc=0.6083 | val_loss=1.0070 | val_acc=0.6187
Epoch 020 | loss=0.9509 | train_acc=0.6082 | val_loss=1.0075 | val_acc=0.6187
Epoch 021 | loss=0.9491 | train_acc=0.6086 | val_loss=1.0082 | val_acc=0.6187
Epoch 022 | loss=0.9477 | train_acc=0.6078 | val_loss=1.0088 | val_acc=0.6187
Epoch 023 | loss=0.9466 | train_acc=0.6082 | val_loss=1.0093 | val_acc=0.6187
Epoch 024 | loss=0.9458 | train_acc=0.6080 | val_loss=1.0098 | val_acc=0.6187
Epoch 025 | loss=0.9444 | train_acc=0.6082 | val_loss=1.0105 | val_acc=0.6187
Epoch 026 | loss=0.9434 | train_acc=0.6079 | val_loss=1.0113 | val_acc=0.6187
Epoch 027 | loss=0.9429 | train_acc=0.6082 | val_loss=1.0118 | val_acc=0.6187
Epoch 028 | loss=0.9426 | train_acc=0.6081 | val_loss=1.0122 | val_acc=0.6187
Epoch 029 | loss=0.9424 | train_acc=0.6077 | val_loss=1.0129 | val_acc=0.6187
Epoch 030 | loss=0.9429 | train_acc=0.6080 | val_loss=1.0135 | val_acc=0.6187
Epoch 031 | loss=0.9414 | train_acc=0.6079 | val_loss=1.0142 | val_acc=0.6187
Epoch 032 | loss=0.9412 | train_acc=0.6081 | val_loss=1.0147 | val_acc=0.6187
Epoch 033 | loss=0.9404 | train_acc=0.6086 | val_loss=1.0153 | val_acc=0.6187
Epoch 034 | loss=0.9398 | train_acc=0.6084 | val_loss=1.0158 | val_acc=0.6187
Epoch 035 | loss=0.9397 | train_acc=0.6083 | val_loss=1.0163 | val_acc=0.6187
Epoch 036 | loss=0.9397 | train_acc=0.6079 | val_loss=1.0169 | val_acc=0.6187
Epoch 037 | loss=0.9389 | train_acc=0.6084 | val_loss=1.0172 | val_acc=0.6187
Epoch 038 | loss=0.9388 | train_acc=0.6083 | val_loss=1.0178 | val_acc=0.6187
Epoch 039 | loss=0.9385 | train_acc=0.6079 | val_loss=1.0185 | val_acc=0.6187
Epoch 040 | loss=0.9382 | train_acc=0.6082 | val_loss=1.0193 | val_acc=0.6187
Epoch 041 | loss=0.9390 | train_acc=0.6083 | val_loss=1.0200 | val_acc=0.6187
Epoch 042 | loss=0.9393 | train_acc=0.6075 | val_loss=1.0203 | val_acc=0.6187
Epoch 043 | loss=0.9384 | train_acc=0.6085 | val_loss=1.0208 | val_acc=0.6187
Epoch 044 | loss=0.9386 | train_acc=0.6078 | val_loss=1.0214 | val_acc=0.6187
Epoch 045 | loss=0.9382 | train_acc=0.6081 | val_loss=1.0219 | val_acc=0.6187
Epoch 046 | loss=0.9380 | train_acc=0.6081 | val_loss=1.0224 | val_acc=0.6187
Epoch 047 | loss=0.9381 | train_acc=0.6077 | val_loss=1.0227 | val_acc=0.6187
Epoch 048 | loss=0.9375 | train_acc=0.6080 | val_loss=1.0232 | val_acc=0.6187
Epoch 049 | loss=0.9377 | train_acc=0.6079 | val_loss=1.0232 | val_acc=0.6187
Final Test Loss: 0.9185 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=2.9860 | train_acc=0.5751 | val_loss=1.1009 | val_acc=0.5861
Epoch 001 | loss=1.1801 | train_acc=0.5763 | val_loss=1.0290 | val_acc=0.6187
Epoch 002 | loss=1.0001 | train_acc=0.6044 | val_loss=1.0190 | val_acc=0.6187
Epoch 003 | loss=0.9819 | train_acc=0.6066 | val_loss=1.0148 | val_acc=0.6187
Epoch 004 | loss=0.9699 | train_acc=0.6074 | val_loss=1.0142 | val_acc=0.6187
Epoch 005 | loss=0.9622 | train_acc=0.6080 | val_loss=1.0153 | val_acc=0.6187
Epoch 006 | loss=0.9578 | train_acc=0.6075 | val_loss=1.0170 | val_acc=0.6187
Epoch 007 | loss=0.9554 | train_acc=0.6076 | val_loss=1.0188 | val_acc=0.6187
Epoch 008 | loss=0.9526 | train_acc=0.6082 | val_loss=1.0203 | val_acc=0.6187
Epoch 009 | loss=0.9524 | train_acc=0.6081 | val_loss=1.0216 | val_acc=0.6187
Epoch 010 | loss=0.9514 | train_acc=0.6080 | val_loss=1.0226 | val_acc=0.6187
Epoch 011 | loss=0.9514 | train_acc=0.6079 | val_loss=1.0233 | val_acc=0.6187
Epoch 012 | loss=0.9509 | train_acc=0.6080 | val_loss=1.0239 | val_acc=0.6187
Epoch 013 | loss=0.9507 | train_acc=0.6078 | val_loss=1.0243 | val_acc=0.6187
Epoch 014 | loss=0.9515 | train_acc=0.6082 | val_loss=1.0246 | val_acc=0.6187
Epoch 015 | loss=0.9504 | train_acc=0.6081 | val_loss=1.0248 | val_acc=0.6187
Epoch 016 | loss=0.9502 | train_acc=0.6079 | val_loss=1.0251 | val_acc=0.6187
Epoch 017 | loss=0.9501 | train_acc=0.6084 | val_loss=1.0254 | val_acc=0.6187
Epoch 018 | loss=0.9501 | train_acc=0.6077 | val_loss=1.0255 | val_acc=0.6187
Epoch 019 | loss=0.9495 | train_acc=0.6087 | val_loss=1.0256 | val_acc=0.6187
Epoch 020 | loss=0.9501 | train_acc=0.6083 | val_loss=1.0256 | val_acc=0.6187
Epoch 021 | loss=0.9494 | train_acc=0.6085 | val_loss=1.0257 | val_acc=0.6187
Epoch 022 | loss=0.9497 | train_acc=0.6082 | val_loss=1.0258 | val_acc=0.6187
Epoch 023 | loss=0.9496 | train_acc=0.6088 | val_loss=1.0258 | val_acc=0.6187
Epoch 024 | loss=0.9493 | train_acc=0.6084 | val_loss=1.0259 | val_acc=0.6187
Epoch 025 | loss=0.9496 | train_acc=0.6086 | val_loss=1.0260 | val_acc=0.6187
Epoch 026 | loss=0.9520 | train_acc=0.6080 | val_loss=1.0262 | val_acc=0.6187
Epoch 027 | loss=0.9498 | train_acc=0.6088 | val_loss=1.0261 | val_acc=0.6187
Epoch 028 | loss=0.9497 | train_acc=0.6085 | val_loss=1.0260 | val_acc=0.6187
Epoch 029 | loss=0.9499 | train_acc=0.6086 | val_loss=1.0260 | val_acc=0.6187
Epoch 030 | loss=0.9502 | train_acc=0.6083 | val_loss=1.0258 | val_acc=0.6187
Epoch 031 | loss=0.9492 | train_acc=0.6089 | val_loss=1.0259 | val_acc=0.6187
Epoch 032 | loss=0.9502 | train_acc=0.6079 | val_loss=1.0259 | val_acc=0.6187
Epoch 033 | loss=0.9498 | train_acc=0.6083 | val_loss=1.0259 | val_acc=0.6187
Epoch 034 | loss=0.9506 | train_acc=0.6078 | val_loss=1.0258 | val_acc=0.6187
Epoch 035 | loss=0.9498 | train_acc=0.6082 | val_loss=1.0258 | val_acc=0.6187
Epoch 036 | loss=0.9497 | train_acc=0.6083 | val_loss=1.0258 | val_acc=0.6187
Epoch 037 | loss=0.9495 | train_acc=0.6081 | val_loss=1.0259 | val_acc=0.6187
Epoch 038 | loss=0.9501 | train_acc=0.6082 | val_loss=1.0264 | val_acc=0.6187
Epoch 039 | loss=0.9504 | train_acc=0.6087 | val_loss=1.0258 | val_acc=0.6187
Epoch 040 | loss=0.9501 | train_acc=0.6084 | val_loss=1.0258 | val_acc=0.6187
Epoch 041 | loss=0.9492 | train_acc=0.6087 | val_loss=1.0259 | val_acc=0.6187
Epoch 042 | loss=0.9498 | train_acc=0.6084 | val_loss=1.0259 | val_acc=0.6187
Epoch 043 | loss=0.9494 | train_acc=0.6084 | val_loss=1.0259 | val_acc=0.6187
Epoch 044 | loss=0.9494 | train_acc=0.6087 | val_loss=1.0259 | val_acc=0.6187
Epoch 045 | loss=0.9494 | train_acc=0.6083 | val_loss=1.0259 | val_acc=0.6187
Epoch 046 | loss=0.9492 | train_acc=0.6083 | val_loss=1.0259 | val_acc=0.6187
Epoch 047 | loss=0.9491 | train_acc=0.6085 | val_loss=1.0260 | val_acc=0.6187
Epoch 048 | loss=0.9490 | train_acc=0.6079 | val_loss=1.0260 | val_acc=0.6187
Epoch 049 | loss=0.9487 | train_acc=0.6086 | val_loss=1.0267 | val_acc=0.6187
Final Test Loss: 0.9193 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=7.5809 | train_acc=0.6180 | val_loss=21.8283 | val_acc=0.1884
Epoch 001 | loss=2.5642 | train_acc=0.5520 | val_loss=1.0647 | val_acc=0.6187
Epoch 002 | loss=1.0649 | train_acc=0.5902 | val_loss=1.0448 | val_acc=0.6187
Epoch 003 | loss=1.0215 | train_acc=0.6022 | val_loss=1.0322 | val_acc=0.6187
Epoch 004 | loss=1.0023 | train_acc=0.6048 | val_loss=1.0242 | val_acc=0.6187
Epoch 005 | loss=0.9904 | train_acc=0.6060 | val_loss=1.0190 | val_acc=0.6187
Epoch 006 | loss=0.9819 | train_acc=0.6070 | val_loss=1.0152 | val_acc=0.6187
Epoch 007 | loss=0.9750 | train_acc=0.6071 | val_loss=1.0125 | val_acc=0.6187
Epoch 008 | loss=0.9726 | train_acc=0.6075 | val_loss=1.0107 | val_acc=0.6187
Epoch 009 | loss=0.9685 | train_acc=0.6068 | val_loss=1.0090 | val_acc=0.6187
Epoch 010 | loss=0.9636 | train_acc=0.6069 | val_loss=1.0078 | val_acc=0.6187
Epoch 011 | loss=0.9612 | train_acc=0.6076 | val_loss=1.0069 | val_acc=0.6187
Epoch 012 | loss=0.9590 | train_acc=0.6079 | val_loss=1.0064 | val_acc=0.6187
Epoch 013 | loss=0.9565 | train_acc=0.6080 | val_loss=1.0060 | val_acc=0.6187
Epoch 014 | loss=0.9548 | train_acc=0.6075 | val_loss=1.0058 | val_acc=0.6187
Epoch 015 | loss=0.9511 | train_acc=0.6083 | val_loss=1.0055 | val_acc=0.6187
Epoch 016 | loss=0.9512 | train_acc=0.6081 | val_loss=1.0055 | val_acc=0.6187
Epoch 017 | loss=0.9489 | train_acc=0.6078 | val_loss=1.0055 | val_acc=0.6187
Epoch 018 | loss=0.9480 | train_acc=0.6080 | val_loss=1.0055 | val_acc=0.6187
Epoch 019 | loss=0.9458 | train_acc=0.6079 | val_loss=1.0056 | val_acc=0.6187
Epoch 020 | loss=0.9448 | train_acc=0.6080 | val_loss=1.0056 | val_acc=0.6187
Epoch 021 | loss=0.9438 | train_acc=0.6080 | val_loss=1.0057 | val_acc=0.6187
Epoch 022 | loss=0.9420 | train_acc=0.6078 | val_loss=1.0057 | val_acc=0.6187
Epoch 023 | loss=0.9407 | train_acc=0.6084 | val_loss=1.0059 | val_acc=0.6187
Epoch 024 | loss=0.9398 | train_acc=0.6082 | val_loss=1.0062 | val_acc=0.6187
Epoch 025 | loss=0.9394 | train_acc=0.6080 | val_loss=1.0065 | val_acc=0.6187
Epoch 026 | loss=0.9394 | train_acc=0.6083 | val_loss=1.0068 | val_acc=0.6187
Epoch 027 | loss=0.9389 | train_acc=0.6088 | val_loss=1.0072 | val_acc=0.6187
Epoch 028 | loss=0.9391 | train_acc=0.6082 | val_loss=1.0076 | val_acc=0.6187
Epoch 029 | loss=0.9380 | train_acc=0.6082 | val_loss=1.0080 | val_acc=0.6187
Epoch 030 | loss=0.9372 | train_acc=0.6088 | val_loss=1.0082 | val_acc=0.6187
Epoch 031 | loss=0.9380 | train_acc=0.6088 | val_loss=1.0087 | val_acc=0.6187
Epoch 032 | loss=0.9369 | train_acc=0.6078 | val_loss=1.0092 | val_acc=0.6187
Epoch 033 | loss=0.9369 | train_acc=0.6083 | val_loss=1.0099 | val_acc=0.6187
Epoch 034 | loss=0.9358 | train_acc=0.6080 | val_loss=1.0087 | val_acc=0.6187
Epoch 035 | loss=0.9352 | train_acc=0.6082 | val_loss=1.0089 | val_acc=0.6187
Epoch 036 | loss=0.9346 | train_acc=0.6076 | val_loss=1.0078 | val_acc=0.6187
Epoch 037 | loss=0.9343 | train_acc=0.6083 | val_loss=1.0073 | val_acc=0.6187
Epoch 038 | loss=0.9331 | train_acc=0.6084 | val_loss=1.0137 | val_acc=0.6187
Epoch 039 | loss=0.9345 | train_acc=0.6080 | val_loss=1.0145 | val_acc=0.6187
Epoch 040 | loss=0.9363 | train_acc=0.6083 | val_loss=1.0140 | val_acc=0.6187
Epoch 041 | loss=0.9330 | train_acc=0.6085 | val_loss=1.0157 | val_acc=0.6187
Epoch 042 | loss=0.9466 | train_acc=0.6083 | val_loss=1.0174 | val_acc=0.6187
Epoch 043 | loss=0.9382 | train_acc=0.6082 | val_loss=1.0173 | val_acc=0.6187
Epoch 044 | loss=0.9362 | train_acc=0.6083 | val_loss=1.0188 | val_acc=0.6187
Epoch 045 | loss=0.9353 | train_acc=0.6083 | val_loss=1.0189 | val_acc=0.6187
Epoch 046 | loss=0.9357 | train_acc=0.6083 | val_loss=1.0189 | val_acc=0.6187
Epoch 047 | loss=0.9339 | train_acc=0.6081 | val_loss=1.0191 | val_acc=0.6187
Epoch 048 | loss=0.9351 | train_acc=0.6080 | val_loss=1.0196 | val_acc=0.6187
Epoch 049 | loss=0.9336 | train_acc=0.6078 | val_loss=1.0201 | val_acc=0.6187
Final Test Loss: 0.9215 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=4.4038 | train_acc=0.5975 | val_loss=5.6496 | val_acc=0.1870
Epoch 001 | loss=1.8860 | train_acc=0.5361 | val_loss=1.0510 | val_acc=0.6187
Epoch 002 | loss=1.0705 | train_acc=0.5754 | val_loss=1.0279 | val_acc=0.6187
Epoch 003 | loss=1.0164 | train_acc=0.6006 | val_loss=1.0167 | val_acc=0.6187
Epoch 004 | loss=0.9917 | train_acc=0.6050 | val_loss=1.0124 | val_acc=0.6187
Epoch 005 | loss=0.9789 | train_acc=0.6056 | val_loss=1.0115 | val_acc=0.6187
Epoch 006 | loss=0.9709 | train_acc=0.6063 | val_loss=1.0123 | val_acc=0.6187
Epoch 007 | loss=0.9650 | train_acc=0.6071 | val_loss=1.0135 | val_acc=0.6187
Epoch 008 | loss=0.9620 | train_acc=0.6074 | val_loss=1.0150 | val_acc=0.6187
Epoch 009 | loss=0.9595 | train_acc=0.6072 | val_loss=1.0204 | val_acc=0.6187
Epoch 010 | loss=0.9623 | train_acc=0.6072 | val_loss=1.0179 | val_acc=0.6187
Epoch 011 | loss=0.9559 | train_acc=0.6077 | val_loss=1.0193 | val_acc=0.6187
Epoch 012 | loss=0.9560 | train_acc=0.6077 | val_loss=1.0204 | val_acc=0.6187
Epoch 013 | loss=0.9545 | train_acc=0.6075 | val_loss=1.0214 | val_acc=0.6187
Epoch 014 | loss=0.9537 | train_acc=0.6074 | val_loss=1.0223 | val_acc=0.6187
Epoch 015 | loss=0.9531 | train_acc=0.6079 | val_loss=1.0230 | val_acc=0.6187
Epoch 016 | loss=0.9528 | train_acc=0.6076 | val_loss=1.0236 | val_acc=0.6187
Epoch 017 | loss=0.9519 | train_acc=0.6080 | val_loss=1.0242 | val_acc=0.6187
Epoch 018 | loss=0.9520 | train_acc=0.6077 | val_loss=1.0247 | val_acc=0.6187
Epoch 019 | loss=0.9515 | train_acc=0.6082 | val_loss=1.0251 | val_acc=0.6187
Epoch 020 | loss=0.9514 | train_acc=0.6079 | val_loss=1.0254 | val_acc=0.6187
Epoch 021 | loss=0.9513 | train_acc=0.6080 | val_loss=1.0256 | val_acc=0.6187
Epoch 022 | loss=0.9513 | train_acc=0.6084 | val_loss=1.0258 | val_acc=0.6187
Epoch 023 | loss=0.9517 | train_acc=0.6082 | val_loss=1.0258 | val_acc=0.6187
Epoch 024 | loss=0.9521 | train_acc=0.6082 | val_loss=1.0259 | val_acc=0.6187
Epoch 025 | loss=0.9518 | train_acc=0.6087 | val_loss=1.0259 | val_acc=0.6187
Epoch 026 | loss=0.9513 | train_acc=0.6084 | val_loss=1.0260 | val_acc=0.6187
Epoch 027 | loss=0.9519 | train_acc=0.6080 | val_loss=1.0260 | val_acc=0.6187
Epoch 028 | loss=0.9517 | train_acc=0.6083 | val_loss=1.0260 | val_acc=0.6187
Epoch 029 | loss=0.9517 | train_acc=0.6083 | val_loss=1.0260 | val_acc=0.6187
Epoch 030 | loss=0.9522 | train_acc=0.6084 | val_loss=1.0260 | val_acc=0.6187
Epoch 031 | loss=0.9523 | train_acc=0.6084 | val_loss=1.0258 | val_acc=0.6187
Epoch 032 | loss=0.9524 | train_acc=0.6075 | val_loss=1.0259 | val_acc=0.6187
Epoch 033 | loss=0.9524 | train_acc=0.6081 | val_loss=1.0258 | val_acc=0.6187
Epoch 034 | loss=0.9521 | train_acc=0.6088 | val_loss=1.0258 | val_acc=0.6187
Epoch 035 | loss=0.9526 | train_acc=0.6078 | val_loss=1.0259 | val_acc=0.6187
Epoch 036 | loss=0.9521 | train_acc=0.6088 | val_loss=1.0258 | val_acc=0.6187
Epoch 037 | loss=0.9524 | train_acc=0.6083 | val_loss=1.0257 | val_acc=0.6187
Epoch 038 | loss=0.9531 | train_acc=0.6080 | val_loss=1.0256 | val_acc=0.6187
Epoch 039 | loss=0.9528 | train_acc=0.6084 | val_loss=1.0255 | val_acc=0.6187
Epoch 040 | loss=0.9526 | train_acc=0.6080 | val_loss=1.0277 | val_acc=0.6187
Epoch 041 | loss=0.9531 | train_acc=0.6081 | val_loss=1.0254 | val_acc=0.6187
Epoch 042 | loss=0.9531 | train_acc=0.6083 | val_loss=1.0256 | val_acc=0.6187
Epoch 043 | loss=0.9527 | train_acc=0.6086 | val_loss=1.0255 | val_acc=0.6187
Epoch 044 | loss=0.9529 | train_acc=0.6080 | val_loss=1.0255 | val_acc=0.6187
Epoch 045 | loss=0.9527 | train_acc=0.6087 | val_loss=1.0254 | val_acc=0.6187
Epoch 046 | loss=0.9530 | train_acc=0.6086 | val_loss=1.0253 | val_acc=0.6187
Epoch 047 | loss=0.9528 | train_acc=0.6083 | val_loss=1.0253 | val_acc=0.6187
Epoch 048 | loss=0.9537 | train_acc=0.6084 | val_loss=1.0253 | val_acc=0.6187
Epoch 049 | loss=0.9533 | train_acc=0.6080 | val_loss=1.0252 | val_acc=0.6187
Final Test Loss: 0.9194 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=11.5048 | train_acc=0.6260 | val_loss=88.7341 | val_acc=0.1873
Epoch 001 | loss=6.5090 | train_acc=0.5670 | val_loss=2.8396 | val_acc=0.2610
Epoch 002 | loss=1.3509 | train_acc=0.5496 | val_loss=1.0457 | val_acc=0.6187
Epoch 003 | loss=1.0668 | train_acc=0.5924 | val_loss=1.0354 | val_acc=0.6187
Epoch 004 | loss=1.0348 | train_acc=0.5975 | val_loss=1.0288 | val_acc=0.6187
Epoch 005 | loss=1.0143 | train_acc=0.6009 | val_loss=1.0242 | val_acc=0.6187
Epoch 006 | loss=1.0051 | train_acc=0.6024 | val_loss=1.0208 | val_acc=0.6187
Epoch 007 | loss=0.9959 | train_acc=0.6044 | val_loss=1.0181 | val_acc=0.6187
Epoch 008 | loss=0.9864 | train_acc=0.6055 | val_loss=1.0159 | val_acc=0.6187
Epoch 009 | loss=0.9845 | train_acc=0.6061 | val_loss=1.0141 | val_acc=0.6187
Epoch 010 | loss=0.9784 | train_acc=0.6061 | val_loss=1.0126 | val_acc=0.6187
Epoch 011 | loss=0.9739 | train_acc=0.6060 | val_loss=1.0114 | val_acc=0.6187
Epoch 012 | loss=0.9720 | train_acc=0.6068 | val_loss=1.0105 | val_acc=0.6187
Epoch 013 | loss=0.9694 | train_acc=0.6075 | val_loss=1.0097 | val_acc=0.6187
Epoch 014 | loss=0.9661 | train_acc=0.6073 | val_loss=1.0090 | val_acc=0.6187
Epoch 015 | loss=0.9643 | train_acc=0.6070 | val_loss=1.0085 | val_acc=0.6187
Epoch 016 | loss=0.9624 | train_acc=0.6074 | val_loss=1.0083 | val_acc=0.6187
Epoch 017 | loss=0.9619 | train_acc=0.6076 | val_loss=1.0082 | val_acc=0.6187
Epoch 018 | loss=0.9585 | train_acc=0.6073 | val_loss=1.0080 | val_acc=0.6187
Epoch 019 | loss=0.9568 | train_acc=0.6076 | val_loss=1.0079 | val_acc=0.6187
Epoch 020 | loss=0.9547 | train_acc=0.6073 | val_loss=1.0079 | val_acc=0.6187
Epoch 021 | loss=0.9557 | train_acc=0.6079 | val_loss=1.0080 | val_acc=0.6187
Epoch 022 | loss=0.9533 | train_acc=0.6083 | val_loss=1.0080 | val_acc=0.6187
Epoch 023 | loss=0.9529 | train_acc=0.6084 | val_loss=1.0083 | val_acc=0.6187
Epoch 024 | loss=0.9523 | train_acc=0.6083 | val_loss=1.0083 | val_acc=0.6187
Epoch 025 | loss=0.9504 | train_acc=0.6076 | val_loss=1.0087 | val_acc=0.6187
Epoch 026 | loss=0.9508 | train_acc=0.6081 | val_loss=1.0089 | val_acc=0.6187
Epoch 027 | loss=0.9495 | train_acc=0.6080 | val_loss=1.0093 | val_acc=0.6187
Epoch 028 | loss=0.9498 | train_acc=0.6079 | val_loss=1.0094 | val_acc=0.6187
Epoch 029 | loss=0.9483 | train_acc=0.6081 | val_loss=1.0097 | val_acc=0.6187
Epoch 030 | loss=0.9466 | train_acc=0.6083 | val_loss=1.0102 | val_acc=0.6187
Epoch 031 | loss=0.9473 | train_acc=0.6079 | val_loss=1.0104 | val_acc=0.6187
Epoch 032 | loss=0.9466 | train_acc=0.6081 | val_loss=1.0108 | val_acc=0.6187
Epoch 033 | loss=0.9483 | train_acc=0.6084 | val_loss=1.0113 | val_acc=0.6187
Epoch 034 | loss=0.9465 | train_acc=0.6085 | val_loss=1.0116 | val_acc=0.6187
Epoch 035 | loss=0.9447 | train_acc=0.6078 | val_loss=1.0120 | val_acc=0.6187
Epoch 036 | loss=0.9442 | train_acc=0.6079 | val_loss=1.0118 | val_acc=0.6187
Epoch 037 | loss=0.9436 | train_acc=0.6080 | val_loss=1.0120 | val_acc=0.6187
Epoch 038 | loss=0.9449 | train_acc=0.6083 | val_loss=1.0122 | val_acc=0.6187
Epoch 039 | loss=0.9445 | train_acc=0.6080 | val_loss=1.0132 | val_acc=0.6187
Epoch 040 | loss=0.9439 | train_acc=0.6080 | val_loss=1.0137 | val_acc=0.6187
Epoch 041 | loss=0.9426 | train_acc=0.6080 | val_loss=1.0138 | val_acc=0.6187
Epoch 042 | loss=0.9415 | train_acc=0.6083 | val_loss=1.0141 | val_acc=0.6187
Epoch 043 | loss=0.9416 | train_acc=0.6085 | val_loss=1.0139 | val_acc=0.6187
Epoch 044 | loss=0.9416 | train_acc=0.6083 | val_loss=1.0144 | val_acc=0.6187
Epoch 045 | loss=0.9419 | train_acc=0.6082 | val_loss=1.0146 | val_acc=0.6187
Epoch 046 | loss=0.9406 | train_acc=0.6079 | val_loss=1.0150 | val_acc=0.6187
Epoch 047 | loss=0.9400 | train_acc=0.6084 | val_loss=1.0156 | val_acc=0.6187
Epoch 048 | loss=0.9405 | train_acc=0.6082 | val_loss=1.0163 | val_acc=0.6187
Epoch 049 | loss=0.9391 | train_acc=0.6083 | val_loss=1.0167 | val_acc=0.6187
Final Test Loss: 0.9206 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=5.7994 | train_acc=0.6472 | val_loss=13.5800 | val_acc=0.2277
Epoch 001 | loss=3.0668 | train_acc=0.6019 | val_loss=5.0591 | val_acc=0.2025
Epoch 002 | loss=1.5721 | train_acc=0.5546 | val_loss=1.0342 | val_acc=0.6190
Epoch 003 | loss=1.0670 | train_acc=0.5728 | val_loss=1.0209 | val_acc=0.6187
Epoch 004 | loss=1.0214 | train_acc=0.5947 | val_loss=1.0151 | val_acc=0.6187
Epoch 005 | loss=1.0005 | train_acc=0.5995 | val_loss=1.0138 | val_acc=0.6187
Epoch 006 | loss=0.9876 | train_acc=0.6021 | val_loss=1.0146 | val_acc=0.6187
Epoch 007 | loss=0.9779 | train_acc=0.6045 | val_loss=1.0163 | val_acc=0.6187
Epoch 008 | loss=0.9721 | train_acc=0.6049 | val_loss=1.0180 | val_acc=0.6187
Epoch 009 | loss=0.9679 | train_acc=0.6059 | val_loss=1.0195 | val_acc=0.6187
Epoch 010 | loss=0.9646 | train_acc=0.6061 | val_loss=1.0208 | val_acc=0.6187
Epoch 011 | loss=0.9628 | train_acc=0.6069 | val_loss=1.0218 | val_acc=0.6187
Epoch 012 | loss=0.9611 | train_acc=0.6065 | val_loss=1.0226 | val_acc=0.6187
Epoch 013 | loss=0.9599 | train_acc=0.6071 | val_loss=1.0232 | val_acc=0.6187
Epoch 014 | loss=0.9587 | train_acc=0.6072 | val_loss=1.0237 | val_acc=0.6187
Epoch 015 | loss=0.9584 | train_acc=0.6077 | val_loss=1.0239 | val_acc=0.6187
Epoch 016 | loss=0.9570 | train_acc=0.6079 | val_loss=1.0241 | val_acc=0.6187
Epoch 017 | loss=0.9569 | train_acc=0.6081 | val_loss=1.0243 | val_acc=0.6187
Epoch 018 | loss=0.9564 | train_acc=0.6077 | val_loss=1.0244 | val_acc=0.6187
Epoch 019 | loss=0.9566 | train_acc=0.6072 | val_loss=1.0246 | val_acc=0.6187
Epoch 020 | loss=0.9556 | train_acc=0.6079 | val_loss=1.0248 | val_acc=0.6187
Epoch 021 | loss=0.9553 | train_acc=0.6080 | val_loss=1.0249 | val_acc=0.6187
Epoch 022 | loss=0.9555 | train_acc=0.6075 | val_loss=1.0249 | val_acc=0.6187
Epoch 023 | loss=0.9555 | train_acc=0.6076 | val_loss=1.0249 | val_acc=0.6187
Epoch 024 | loss=0.9550 | train_acc=0.6079 | val_loss=1.0250 | val_acc=0.6187
Epoch 025 | loss=0.9552 | train_acc=0.6081 | val_loss=1.0250 | val_acc=0.6187
Epoch 026 | loss=0.9548 | train_acc=0.6079 | val_loss=1.0249 | val_acc=0.6187
Epoch 027 | loss=0.9556 | train_acc=0.6076 | val_loss=1.0249 | val_acc=0.6187
Epoch 028 | loss=0.9546 | train_acc=0.6080 | val_loss=1.0249 | val_acc=0.6187
Epoch 029 | loss=0.9540 | train_acc=0.6086 | val_loss=1.0249 | val_acc=0.6187
Epoch 030 | loss=0.9537 | train_acc=0.6086 | val_loss=1.0250 | val_acc=0.6187
Epoch 031 | loss=0.9546 | train_acc=0.6079 | val_loss=1.0249 | val_acc=0.6187
Epoch 032 | loss=0.9544 | train_acc=0.6081 | val_loss=1.0249 | val_acc=0.6187
Epoch 033 | loss=0.9535 | train_acc=0.6086 | val_loss=1.0249 | val_acc=0.6187
Epoch 034 | loss=0.9537 | train_acc=0.6084 | val_loss=1.0249 | val_acc=0.6187
Epoch 035 | loss=0.9539 | train_acc=0.6081 | val_loss=1.0249 | val_acc=0.6187
Epoch 036 | loss=0.9540 | train_acc=0.6079 | val_loss=1.0249 | val_acc=0.6187
Epoch 037 | loss=0.9529 | train_acc=0.6089 | val_loss=1.0249 | val_acc=0.6187
Epoch 038 | loss=0.9536 | train_acc=0.6081 | val_loss=1.0250 | val_acc=0.6187
Epoch 039 | loss=0.9533 | train_acc=0.6081 | val_loss=1.0250 | val_acc=0.6187
Epoch 040 | loss=0.9533 | train_acc=0.6082 | val_loss=1.0250 | val_acc=0.6187
Epoch 041 | loss=0.9538 | train_acc=0.6078 | val_loss=1.0249 | val_acc=0.6187
Epoch 042 | loss=0.9527 | train_acc=0.6083 | val_loss=1.0249 | val_acc=0.6187
Epoch 043 | loss=0.9530 | train_acc=0.6079 | val_loss=1.0249 | val_acc=0.6187
Epoch 044 | loss=0.9528 | train_acc=0.6084 | val_loss=1.0249 | val_acc=0.6187
Epoch 045 | loss=0.9446 | train_acc=0.6082 | val_loss=1.0332 | val_acc=0.6187
Epoch 046 | loss=0.9585 | train_acc=0.6085 | val_loss=1.0325 | val_acc=0.6187
Epoch 047 | loss=0.9579 | train_acc=0.6075 | val_loss=1.0324 | val_acc=0.6187
Epoch 048 | loss=0.9566 | train_acc=0.6085 | val_loss=1.0310 | val_acc=0.6187
Epoch 049 | loss=0.9555 | train_acc=0.6086 | val_loss=1.0300 | val_acc=0.6187
Final Test Loss: 0.9212 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=5.0302 | train_acc=0.5614 | val_loss=1.0710 | val_acc=0.6068
Epoch 001 | loss=1.1284 | train_acc=0.5888 | val_loss=1.0442 | val_acc=0.6187
Epoch 002 | loss=1.0188 | train_acc=0.6056 | val_loss=1.0337 | val_acc=0.6187
Epoch 003 | loss=0.9977 | train_acc=0.6063 | val_loss=1.0275 | val_acc=0.6187
Epoch 004 | loss=0.9907 | train_acc=0.6075 | val_loss=1.0234 | val_acc=0.6187
Epoch 005 | loss=0.9863 | train_acc=0.6073 | val_loss=1.0201 | val_acc=0.6187
Epoch 006 | loss=0.9828 | train_acc=0.6074 | val_loss=1.0168 | val_acc=0.6187
Epoch 007 | loss=0.9776 | train_acc=0.6075 | val_loss=1.0148 | val_acc=0.6187
Epoch 008 | loss=0.9739 | train_acc=0.6080 | val_loss=1.0133 | val_acc=0.6187
Epoch 009 | loss=0.9725 | train_acc=0.6076 | val_loss=1.0115 | val_acc=0.6187
Epoch 010 | loss=0.9665 | train_acc=0.6085 | val_loss=1.0109 | val_acc=0.6187
Epoch 011 | loss=0.9664 | train_acc=0.6084 | val_loss=1.0097 | val_acc=0.6187
Epoch 012 | loss=0.9643 | train_acc=0.6081 | val_loss=1.0089 | val_acc=0.6187
Epoch 013 | loss=0.9613 | train_acc=0.6078 | val_loss=1.0083 | val_acc=0.6187
Epoch 014 | loss=0.9599 | train_acc=0.6084 | val_loss=1.0082 | val_acc=0.6187
Epoch 015 | loss=0.9593 | train_acc=0.6081 | val_loss=1.0075 | val_acc=0.6187
Epoch 016 | loss=0.9557 | train_acc=0.6078 | val_loss=1.0080 | val_acc=0.6187
Epoch 017 | loss=0.9555 | train_acc=0.6083 | val_loss=1.0073 | val_acc=0.6187
Epoch 018 | loss=0.9527 | train_acc=0.6078 | val_loss=1.0077 | val_acc=0.6187
Epoch 019 | loss=0.9514 | train_acc=0.6078 | val_loss=1.0078 | val_acc=0.6187
Epoch 020 | loss=0.9502 | train_acc=0.6081 | val_loss=1.0077 | val_acc=0.6187
Epoch 021 | loss=0.9479 | train_acc=0.6079 | val_loss=1.0081 | val_acc=0.6187
Epoch 022 | loss=0.9480 | train_acc=0.6082 | val_loss=1.0079 | val_acc=0.6187
Epoch 023 | loss=0.9451 | train_acc=0.6074 | val_loss=1.0081 | val_acc=0.6187
Epoch 024 | loss=0.9454 | train_acc=0.6074 | val_loss=1.0082 | val_acc=0.6187
Epoch 025 | loss=0.9419 | train_acc=0.6079 | val_loss=1.0074 | val_acc=0.6187
Epoch 026 | loss=0.9423 | train_acc=0.6083 | val_loss=1.0087 | val_acc=0.6187
Epoch 027 | loss=0.9404 | train_acc=0.6083 | val_loss=1.0094 | val_acc=0.6187
Epoch 028 | loss=0.9393 | train_acc=0.6078 | val_loss=1.0059 | val_acc=0.6187
Epoch 029 | loss=0.9388 | train_acc=0.6077 | val_loss=1.0031 | val_acc=0.6187
Epoch 030 | loss=0.9362 | train_acc=0.6082 | val_loss=1.0110 | val_acc=0.6187
Epoch 031 | loss=0.9338 | train_acc=0.6073 | val_loss=1.0113 | val_acc=0.6187
Epoch 032 | loss=0.9291 | train_acc=0.6074 | val_loss=1.0090 | val_acc=0.6187
Epoch 033 | loss=0.9593 | train_acc=0.6274 | val_loss=1.9718 | val_acc=0.5961
Epoch 034 | loss=1.1909 | train_acc=0.6032 | val_loss=1.0148 | val_acc=0.6187
Epoch 035 | loss=0.9453 | train_acc=0.6087 | val_loss=1.0154 | val_acc=0.6187
Epoch 036 | loss=0.9453 | train_acc=0.6080 | val_loss=1.0161 | val_acc=0.6187
Epoch 037 | loss=0.9435 | train_acc=0.6087 | val_loss=1.0168 | val_acc=0.6187
Epoch 038 | loss=0.9428 | train_acc=0.6080 | val_loss=1.0174 | val_acc=0.6187
Epoch 039 | loss=0.9402 | train_acc=0.6081 | val_loss=1.0176 | val_acc=0.6187
Epoch 040 | loss=0.9408 | train_acc=0.6086 | val_loss=1.0185 | val_acc=0.6187
Epoch 041 | loss=0.9409 | train_acc=0.6086 | val_loss=1.0192 | val_acc=0.6187
Epoch 042 | loss=0.9401 | train_acc=0.6078 | val_loss=1.0201 | val_acc=0.6187
Epoch 043 | loss=0.9370 | train_acc=0.6075 | val_loss=1.0204 | val_acc=0.6187
Epoch 044 | loss=0.9443 | train_acc=0.6086 | val_loss=1.0210 | val_acc=0.6187
Epoch 045 | loss=0.9394 | train_acc=0.6080 | val_loss=1.0224 | val_acc=0.6187
Epoch 046 | loss=0.9410 | train_acc=0.6078 | val_loss=1.0219 | val_acc=0.6187
Epoch 047 | loss=0.9387 | train_acc=0.6088 | val_loss=1.0225 | val_acc=0.6187
Epoch 048 | loss=0.9387 | train_acc=0.6083 | val_loss=1.0229 | val_acc=0.6187
Epoch 049 | loss=0.9381 | train_acc=0.6077 | val_loss=1.0217 | val_acc=0.6187
Final Test Loss: 0.9089 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.8773 | train_acc=0.5703 | val_loss=1.0526 | val_acc=0.6187
Epoch 001 | loss=1.0277 | train_acc=0.6070 | val_loss=1.0279 | val_acc=0.6187
Epoch 002 | loss=0.9954 | train_acc=0.6079 | val_loss=1.0152 | val_acc=0.6187
Epoch 003 | loss=0.9780 | train_acc=0.6085 | val_loss=1.0105 | val_acc=0.6187
Epoch 004 | loss=0.9670 | train_acc=0.6082 | val_loss=1.0104 | val_acc=0.6187
Epoch 005 | loss=0.9620 | train_acc=0.6085 | val_loss=1.0121 | val_acc=0.6187
Epoch 006 | loss=0.9583 | train_acc=0.6081 | val_loss=1.0141 | val_acc=0.6187
Epoch 007 | loss=0.9559 | train_acc=0.6081 | val_loss=1.0160 | val_acc=0.6187
Epoch 008 | loss=0.9538 | train_acc=0.6088 | val_loss=1.0177 | val_acc=0.6187
Epoch 009 | loss=0.9536 | train_acc=0.6081 | val_loss=1.0191 | val_acc=0.6187
Epoch 010 | loss=0.9531 | train_acc=0.6080 | val_loss=1.0202 | val_acc=0.6187
Epoch 011 | loss=0.9524 | train_acc=0.6080 | val_loss=1.0209 | val_acc=0.6187
Epoch 012 | loss=0.9524 | train_acc=0.6080 | val_loss=1.0216 | val_acc=0.6187
Epoch 013 | loss=0.9518 | train_acc=0.6081 | val_loss=1.0221 | val_acc=0.6187
Epoch 014 | loss=0.9518 | train_acc=0.6081 | val_loss=1.0225 | val_acc=0.6187
Epoch 015 | loss=0.9516 | train_acc=0.6085 | val_loss=1.0228 | val_acc=0.6187
Epoch 016 | loss=0.9520 | train_acc=0.6082 | val_loss=1.0230 | val_acc=0.6187
Epoch 017 | loss=0.9512 | train_acc=0.6086 | val_loss=1.0231 | val_acc=0.6187
Epoch 018 | loss=0.9510 | train_acc=0.6083 | val_loss=1.0233 | val_acc=0.6187
Epoch 019 | loss=0.9510 | train_acc=0.6087 | val_loss=1.0235 | val_acc=0.6187
Epoch 020 | loss=0.9509 | train_acc=0.6082 | val_loss=1.0241 | val_acc=0.6187
Epoch 021 | loss=0.9504 | train_acc=0.6079 | val_loss=1.0242 | val_acc=0.6187
Epoch 022 | loss=0.9504 | train_acc=0.6083 | val_loss=1.0239 | val_acc=0.6187
Epoch 023 | loss=0.9499 | train_acc=0.6080 | val_loss=1.0251 | val_acc=0.6187
Epoch 024 | loss=0.9508 | train_acc=0.6083 | val_loss=1.0253 | val_acc=0.6187
Epoch 025 | loss=0.9502 | train_acc=0.6082 | val_loss=1.0260 | val_acc=0.6187
Epoch 026 | loss=0.9498 | train_acc=0.6083 | val_loss=1.0260 | val_acc=0.6187
Epoch 027 | loss=0.9508 | train_acc=0.6086 | val_loss=1.0252 | val_acc=0.6187
Epoch 028 | loss=0.9496 | train_acc=0.6078 | val_loss=1.0253 | val_acc=0.6187
Epoch 029 | loss=0.9501 | train_acc=0.6082 | val_loss=1.0261 | val_acc=0.6187
Epoch 030 | loss=0.9507 | train_acc=0.6081 | val_loss=1.0245 | val_acc=0.6187
Epoch 031 | loss=0.9497 | train_acc=0.6080 | val_loss=1.0239 | val_acc=0.6187
Epoch 032 | loss=0.9500 | train_acc=0.6085 | val_loss=1.0237 | val_acc=0.6187
Epoch 033 | loss=0.9495 | train_acc=0.6086 | val_loss=1.0241 | val_acc=0.6187
Epoch 034 | loss=0.9504 | train_acc=0.6087 | val_loss=1.0248 | val_acc=0.6187
Epoch 035 | loss=0.9497 | train_acc=0.6082 | val_loss=1.0247 | val_acc=0.6187
Epoch 036 | loss=0.9492 | train_acc=0.6083 | val_loss=1.0254 | val_acc=0.6187
Epoch 037 | loss=0.9493 | train_acc=0.6077 | val_loss=1.0274 | val_acc=0.6187
Epoch 038 | loss=0.9488 | train_acc=0.6081 | val_loss=1.0269 | val_acc=0.6187
Epoch 039 | loss=0.9497 | train_acc=0.6084 | val_loss=1.0268 | val_acc=0.6187
Epoch 040 | loss=0.9486 | train_acc=0.6086 | val_loss=1.0268 | val_acc=0.6187
Epoch 041 | loss=0.9495 | train_acc=0.6084 | val_loss=1.0274 | val_acc=0.6187
Epoch 042 | loss=0.9504 | train_acc=0.6082 | val_loss=1.0270 | val_acc=0.6187
Epoch 043 | loss=0.9486 | train_acc=0.6075 | val_loss=1.0244 | val_acc=0.6187
Epoch 044 | loss=0.9762 | train_acc=0.6083 | val_loss=1.0246 | val_acc=0.6183
Epoch 045 | loss=0.9538 | train_acc=0.6077 | val_loss=1.0231 | val_acc=0.6187
Epoch 046 | loss=0.9499 | train_acc=0.6082 | val_loss=1.0231 | val_acc=0.6187
Epoch 047 | loss=0.9496 | train_acc=0.6087 | val_loss=1.0240 | val_acc=0.6187
Epoch 048 | loss=0.9493 | train_acc=0.6079 | val_loss=1.0249 | val_acc=0.6187
Epoch 049 | loss=0.9486 | train_acc=0.6084 | val_loss=1.0253 | val_acc=0.6187
Final Test Loss: 0.9208 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=7.1111 | train_acc=0.6331 | val_loss=41.0200 | val_acc=0.1984
Epoch 001 | loss=2.3015 | train_acc=0.5847 | val_loss=1.0455 | val_acc=0.6187
Epoch 002 | loss=1.0238 | train_acc=0.6028 | val_loss=1.0279 | val_acc=0.6187
Epoch 003 | loss=1.0027 | train_acc=0.6060 | val_loss=1.0191 | val_acc=0.6187
Epoch 004 | loss=0.9927 | train_acc=0.6062 | val_loss=1.0143 | val_acc=0.6187
Epoch 005 | loss=0.9866 | train_acc=0.6073 | val_loss=1.0116 | val_acc=0.6187
Epoch 006 | loss=0.9809 | train_acc=0.6073 | val_loss=1.0087 | val_acc=0.6187
Epoch 007 | loss=0.9750 | train_acc=0.6078 | val_loss=1.0074 | val_acc=0.6187
Epoch 008 | loss=0.9731 | train_acc=0.6076 | val_loss=1.0074 | val_acc=0.6187
Epoch 009 | loss=0.9689 | train_acc=0.6075 | val_loss=1.0063 | val_acc=0.6187
Epoch 010 | loss=0.9677 | train_acc=0.6078 | val_loss=1.0047 | val_acc=0.6187
Epoch 011 | loss=0.9640 | train_acc=0.6079 | val_loss=1.0034 | val_acc=0.6187
Epoch 012 | loss=0.9615 | train_acc=0.6081 | val_loss=1.0036 | val_acc=0.6187
Epoch 013 | loss=0.9587 | train_acc=0.6081 | val_loss=1.0037 | val_acc=0.6187
Epoch 014 | loss=0.9569 | train_acc=0.6080 | val_loss=1.0048 | val_acc=0.6187
Epoch 015 | loss=0.9550 | train_acc=0.6084 | val_loss=1.0048 | val_acc=0.6187
Epoch 016 | loss=0.9553 | train_acc=0.6082 | val_loss=1.0045 | val_acc=0.6187
Epoch 017 | loss=0.9541 | train_acc=0.6080 | val_loss=1.0036 | val_acc=0.6187
Epoch 018 | loss=0.9527 | train_acc=0.6080 | val_loss=1.0031 | val_acc=0.6187
Epoch 019 | loss=0.9499 | train_acc=0.6083 | val_loss=1.0038 | val_acc=0.6187
Epoch 020 | loss=0.9501 | train_acc=0.6083 | val_loss=1.0049 | val_acc=0.6187
Epoch 021 | loss=0.9492 | train_acc=0.6078 | val_loss=1.0060 | val_acc=0.6187
Epoch 022 | loss=0.9462 | train_acc=0.6081 | val_loss=1.0063 | val_acc=0.6187
Epoch 023 | loss=0.9446 | train_acc=0.6075 | val_loss=1.0064 | val_acc=0.6187
Epoch 024 | loss=0.9439 | train_acc=0.6076 | val_loss=1.0073 | val_acc=0.6187
Epoch 025 | loss=0.9440 | train_acc=0.6080 | val_loss=1.0084 | val_acc=0.6187
Epoch 026 | loss=0.9392 | train_acc=0.6081 | val_loss=1.0087 | val_acc=0.6187
Epoch 027 | loss=0.9408 | train_acc=0.6082 | val_loss=1.0081 | val_acc=0.6187
Epoch 028 | loss=0.9381 | train_acc=0.6085 | val_loss=1.0089 | val_acc=0.6187
Epoch 029 | loss=0.9521 | train_acc=0.6082 | val_loss=1.0087 | val_acc=0.6187
Epoch 030 | loss=0.9400 | train_acc=0.6082 | val_loss=1.0089 | val_acc=0.6187
Epoch 031 | loss=0.9430 | train_acc=0.6082 | val_loss=1.0088 | val_acc=0.6187
Epoch 032 | loss=0.9385 | train_acc=0.6083 | val_loss=1.0120 | val_acc=0.6187
Epoch 033 | loss=0.9381 | train_acc=0.6083 | val_loss=1.0094 | val_acc=0.6187
Epoch 034 | loss=0.9354 | train_acc=0.6083 | val_loss=1.0090 | val_acc=0.6187
Epoch 035 | loss=0.9357 | train_acc=0.6081 | val_loss=1.0116 | val_acc=0.6187
Epoch 036 | loss=0.9360 | train_acc=0.6083 | val_loss=1.0181 | val_acc=0.6187
Epoch 037 | loss=0.9379 | train_acc=0.6084 | val_loss=1.0150 | val_acc=0.6187
Epoch 038 | loss=0.9349 | train_acc=0.6085 | val_loss=1.0128 | val_acc=0.6187
Epoch 039 | loss=0.9376 | train_acc=0.6088 | val_loss=1.0139 | val_acc=0.6187
Epoch 040 | loss=0.9379 | train_acc=0.6083 | val_loss=1.0134 | val_acc=0.6187
Epoch 041 | loss=0.9347 | train_acc=0.6081 | val_loss=1.0186 | val_acc=0.6187
Epoch 042 | loss=0.9338 | train_acc=0.6086 | val_loss=1.0194 | val_acc=0.6187
Epoch 043 | loss=0.9360 | train_acc=0.6074 | val_loss=1.0159 | val_acc=0.6187
Epoch 044 | loss=0.9345 | train_acc=0.6081 | val_loss=1.0166 | val_acc=0.6187
Epoch 045 | loss=0.9337 | train_acc=0.6083 | val_loss=1.0170 | val_acc=0.6187
Epoch 046 | loss=0.9345 | train_acc=0.6079 | val_loss=1.0215 | val_acc=0.6187
Epoch 047 | loss=0.9355 | train_acc=0.6080 | val_loss=1.0143 | val_acc=0.6187
Epoch 048 | loss=0.9373 | train_acc=0.6072 | val_loss=1.0167 | val_acc=0.6187
Epoch 049 | loss=0.9362 | train_acc=0.6081 | val_loss=1.0168 | val_acc=0.6187
Final Test Loss: 0.9227 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.9851 | train_acc=0.5699 | val_loss=2.2151 | val_acc=0.2136
Epoch 001 | loss=1.0519 | train_acc=0.6029 | val_loss=1.0280 | val_acc=0.6187
Epoch 002 | loss=1.0047 | train_acc=0.6078 | val_loss=1.0152 | val_acc=0.6187
Epoch 003 | loss=0.9841 | train_acc=0.6081 | val_loss=1.0124 | val_acc=0.6187
Epoch 004 | loss=0.9738 | train_acc=0.6080 | val_loss=1.0137 | val_acc=0.6187
Epoch 005 | loss=0.9666 | train_acc=0.6077 | val_loss=1.0161 | val_acc=0.6187
Epoch 006 | loss=0.9627 | train_acc=0.6083 | val_loss=1.0185 | val_acc=0.6187
Epoch 007 | loss=0.9598 | train_acc=0.6082 | val_loss=1.0205 | val_acc=0.6187
Epoch 008 | loss=0.9595 | train_acc=0.6083 | val_loss=1.0217 | val_acc=0.6187
Epoch 009 | loss=0.9571 | train_acc=0.6085 | val_loss=1.0226 | val_acc=0.6187
Epoch 010 | loss=0.9573 | train_acc=0.6083 | val_loss=1.0232 | val_acc=0.6187
Epoch 011 | loss=0.9568 | train_acc=0.6084 | val_loss=1.0235 | val_acc=0.6187
Epoch 012 | loss=0.9565 | train_acc=0.6088 | val_loss=1.0238 | val_acc=0.6187
Epoch 013 | loss=0.9573 | train_acc=0.6083 | val_loss=1.0238 | val_acc=0.6187
Epoch 014 | loss=0.9565 | train_acc=0.6080 | val_loss=1.0239 | val_acc=0.6187
Epoch 015 | loss=0.9573 | train_acc=0.6080 | val_loss=1.0239 | val_acc=0.6187
Epoch 016 | loss=0.9569 | train_acc=0.6084 | val_loss=1.0239 | val_acc=0.6187
Epoch 017 | loss=0.9564 | train_acc=0.6081 | val_loss=1.0240 | val_acc=0.6187
Epoch 018 | loss=0.9564 | train_acc=0.6081 | val_loss=1.0240 | val_acc=0.6187
Epoch 019 | loss=0.9569 | train_acc=0.6080 | val_loss=1.0237 | val_acc=0.6187
Epoch 020 | loss=0.9574 | train_acc=0.6083 | val_loss=1.0239 | val_acc=0.6187
Epoch 021 | loss=0.9562 | train_acc=0.6084 | val_loss=1.0239 | val_acc=0.6187
Epoch 022 | loss=0.9567 | train_acc=0.6079 | val_loss=1.0240 | val_acc=0.6187
Epoch 023 | loss=0.9563 | train_acc=0.6083 | val_loss=1.0240 | val_acc=0.6187
Epoch 024 | loss=0.9559 | train_acc=0.6084 | val_loss=1.0240 | val_acc=0.6187
Epoch 025 | loss=0.9560 | train_acc=0.6079 | val_loss=1.0240 | val_acc=0.6187
Epoch 026 | loss=0.9553 | train_acc=0.6084 | val_loss=1.0241 | val_acc=0.6187
Epoch 027 | loss=0.9567 | train_acc=0.6084 | val_loss=1.0241 | val_acc=0.6187
Epoch 028 | loss=0.9559 | train_acc=0.6085 | val_loss=1.0239 | val_acc=0.6187
Epoch 029 | loss=0.9558 | train_acc=0.6080 | val_loss=1.0239 | val_acc=0.6187
Epoch 030 | loss=0.9554 | train_acc=0.6084 | val_loss=1.0240 | val_acc=0.6187
Epoch 031 | loss=0.9554 | train_acc=0.6085 | val_loss=1.0240 | val_acc=0.6187
Epoch 032 | loss=0.9560 | train_acc=0.6079 | val_loss=1.0241 | val_acc=0.6187
Epoch 033 | loss=0.9555 | train_acc=0.6078 | val_loss=1.0241 | val_acc=0.6187
Epoch 034 | loss=0.9544 | train_acc=0.6083 | val_loss=1.1380 | val_acc=0.4254
Epoch 035 | loss=0.9578 | train_acc=0.6081 | val_loss=1.0246 | val_acc=0.6187
Epoch 036 | loss=0.9548 | train_acc=0.6081 | val_loss=1.0244 | val_acc=0.6187
Epoch 037 | loss=0.9550 | train_acc=0.6088 | val_loss=1.0243 | val_acc=0.6187
Epoch 038 | loss=0.9545 | train_acc=0.6080 | val_loss=1.0242 | val_acc=0.6187
Epoch 039 | loss=0.9546 | train_acc=0.6082 | val_loss=1.0242 | val_acc=0.6187
Epoch 040 | loss=0.9548 | train_acc=0.6080 | val_loss=1.0241 | val_acc=0.6187
Epoch 041 | loss=0.9540 | train_acc=0.6088 | val_loss=1.0239 | val_acc=0.6187
Epoch 042 | loss=0.9546 | train_acc=0.6086 | val_loss=1.0241 | val_acc=0.6187
Epoch 043 | loss=0.9525 | train_acc=0.6179 | val_loss=3.2868 | val_acc=0.2703
Epoch 044 | loss=0.9756 | train_acc=0.6020 | val_loss=1.0215 | val_acc=0.6187
Epoch 045 | loss=0.9595 | train_acc=0.6087 | val_loss=1.0216 | val_acc=0.6187
Epoch 046 | loss=0.9564 | train_acc=0.6085 | val_loss=1.0225 | val_acc=0.6187
Epoch 047 | loss=0.9554 | train_acc=0.6084 | val_loss=1.0231 | val_acc=0.6187
Epoch 048 | loss=0.9549 | train_acc=0.6080 | val_loss=1.0234 | val_acc=0.6187
Epoch 049 | loss=0.9547 | train_acc=0.6082 | val_loss=1.0236 | val_acc=0.6187
Final Test Loss: 0.9203 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=6.7599 | train_acc=0.6209 | val_loss=1.8398 | val_acc=0.2503
Epoch 001 | loss=1.5471 | train_acc=0.5752 | val_loss=1.0493 | val_acc=0.6183
Epoch 002 | loss=1.0497 | train_acc=0.5967 | val_loss=1.0299 | val_acc=0.6187
Epoch 003 | loss=1.0216 | train_acc=0.6057 | val_loss=1.0201 | val_acc=0.6187
Epoch 004 | loss=1.0070 | train_acc=0.6060 | val_loss=1.0163 | val_acc=0.6187
Epoch 005 | loss=0.9996 | train_acc=0.6068 | val_loss=1.0137 | val_acc=0.6187
Epoch 006 | loss=0.9968 | train_acc=0.6075 | val_loss=1.0102 | val_acc=0.6187
Epoch 007 | loss=0.9859 | train_acc=0.6073 | val_loss=1.0072 | val_acc=0.6187
Epoch 008 | loss=0.9793 | train_acc=0.6075 | val_loss=1.0043 | val_acc=0.6187
Epoch 009 | loss=0.9748 | train_acc=0.6075 | val_loss=1.0012 | val_acc=0.6187
Epoch 010 | loss=0.9723 | train_acc=0.6074 | val_loss=0.9990 | val_acc=0.6187
Epoch 011 | loss=0.9688 | train_acc=0.6075 | val_loss=0.9982 | val_acc=0.6187
Epoch 012 | loss=0.9634 | train_acc=0.6045 | val_loss=0.9971 | val_acc=0.6187
Epoch 013 | loss=0.9698 | train_acc=0.6081 | val_loss=0.9978 | val_acc=0.6187
Epoch 014 | loss=0.9586 | train_acc=0.6080 | val_loss=0.9953 | val_acc=0.6187
Epoch 015 | loss=0.9561 | train_acc=0.6074 | val_loss=0.9965 | val_acc=0.6187
Epoch 016 | loss=0.9590 | train_acc=0.6077 | val_loss=1.0022 | val_acc=0.6187
Epoch 017 | loss=0.9522 | train_acc=0.6076 | val_loss=1.0017 | val_acc=0.6187
Epoch 018 | loss=0.9520 | train_acc=0.6076 | val_loss=1.0022 | val_acc=0.6187
Epoch 019 | loss=0.9477 | train_acc=0.6081 | val_loss=1.0007 | val_acc=0.6187
Epoch 020 | loss=0.9487 | train_acc=0.6078 | val_loss=1.0022 | val_acc=0.6187
Epoch 021 | loss=0.9539 | train_acc=0.6076 | val_loss=1.0052 | val_acc=0.6187
Epoch 022 | loss=0.9486 | train_acc=0.6074 | val_loss=1.0024 | val_acc=0.6187
Epoch 023 | loss=0.9599 | train_acc=0.6051 | val_loss=1.0063 | val_acc=0.6187
Epoch 024 | loss=0.9471 | train_acc=0.6082 | val_loss=1.0066 | val_acc=0.6187
Epoch 025 | loss=0.9438 | train_acc=0.6082 | val_loss=1.0074 | val_acc=0.6187
Epoch 026 | loss=0.9415 | train_acc=0.6079 | val_loss=1.0099 | val_acc=0.6187
Epoch 027 | loss=0.9417 | train_acc=0.6077 | val_loss=1.0109 | val_acc=0.6187
Epoch 028 | loss=0.9406 | train_acc=0.6090 | val_loss=1.0158 | val_acc=0.6187
Epoch 029 | loss=0.9391 | train_acc=0.6077 | val_loss=1.0156 | val_acc=0.6187
Epoch 030 | loss=0.9517 | train_acc=0.6070 | val_loss=1.0168 | val_acc=0.6187
Epoch 031 | loss=0.9414 | train_acc=0.6080 | val_loss=1.0165 | val_acc=0.6187
Epoch 032 | loss=0.9538 | train_acc=0.6067 | val_loss=1.0193 | val_acc=0.6187
Epoch 033 | loss=0.9452 | train_acc=0.6068 | val_loss=1.0139 | val_acc=0.6187
Epoch 034 | loss=0.9434 | train_acc=0.6081 | val_loss=1.0136 | val_acc=0.6187
Epoch 035 | loss=0.9402 | train_acc=0.6071 | val_loss=1.0123 | val_acc=0.6187
Epoch 036 | loss=0.9415 | train_acc=0.6082 | val_loss=1.0136 | val_acc=0.6187
Epoch 037 | loss=0.9306 | train_acc=0.6065 | val_loss=1.0157 | val_acc=0.6187
Epoch 038 | loss=0.9373 | train_acc=0.6084 | val_loss=1.0173 | val_acc=0.6187
Epoch 039 | loss=0.9249 | train_acc=0.6076 | val_loss=1.0171 | val_acc=0.6031
Epoch 040 | loss=0.9547 | train_acc=0.6215 | val_loss=6.3313 | val_acc=0.2177
Epoch 041 | loss=1.0266 | train_acc=0.6017 | val_loss=1.0127 | val_acc=0.6179
Epoch 042 | loss=0.9451 | train_acc=0.6081 | val_loss=1.0140 | val_acc=0.6187
Epoch 043 | loss=0.9413 | train_acc=0.6086 | val_loss=1.0165 | val_acc=0.6187
Epoch 044 | loss=0.9372 | train_acc=0.6073 | val_loss=1.0151 | val_acc=0.6187
Epoch 045 | loss=0.9405 | train_acc=0.6080 | val_loss=1.0192 | val_acc=0.6187
Epoch 046 | loss=0.9405 | train_acc=0.6085 | val_loss=1.0001 | val_acc=0.6187
Epoch 047 | loss=0.9468 | train_acc=0.6082 | val_loss=0.9995 | val_acc=0.6187
Epoch 048 | loss=0.9336 | train_acc=0.6067 | val_loss=1.0153 | val_acc=0.6187
Epoch 049 | loss=0.9639 | train_acc=0.6082 | val_loss=1.0190 | val_acc=0.6187
Final Test Loss: 0.9235 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=3.0872 | train_acc=0.6036 | val_loss=1.0698 | val_acc=0.6046
Epoch 001 | loss=1.0791 | train_acc=0.5939 | val_loss=1.0284 | val_acc=0.6187
Epoch 002 | loss=0.9961 | train_acc=0.6071 | val_loss=1.0137 | val_acc=0.6187
Epoch 003 | loss=0.9746 | train_acc=0.6081 | val_loss=1.0100 | val_acc=0.6187
Epoch 004 | loss=0.9648 | train_acc=0.6080 | val_loss=1.0108 | val_acc=0.6187
Epoch 005 | loss=0.9603 | train_acc=0.6080 | val_loss=1.0129 | val_acc=0.6187
Epoch 006 | loss=0.9577 | train_acc=0.6077 | val_loss=1.0151 | val_acc=0.6187
Epoch 007 | loss=0.9556 | train_acc=0.6085 | val_loss=1.0168 | val_acc=0.6187
Epoch 008 | loss=0.9553 | train_acc=0.6084 | val_loss=1.0180 | val_acc=0.6187
Epoch 009 | loss=0.9553 | train_acc=0.6083 | val_loss=1.0189 | val_acc=0.6187
Epoch 010 | loss=0.9552 | train_acc=0.6082 | val_loss=1.0196 | val_acc=0.6187
Epoch 011 | loss=0.9546 | train_acc=0.6080 | val_loss=1.0199 | val_acc=0.6187
Epoch 012 | loss=0.9538 | train_acc=0.6085 | val_loss=1.0203 | val_acc=0.6187
Epoch 013 | loss=0.9541 | train_acc=0.6079 | val_loss=1.0207 | val_acc=0.6187
Epoch 014 | loss=0.9549 | train_acc=0.6089 | val_loss=1.0209 | val_acc=0.6187
Epoch 015 | loss=0.9550 | train_acc=0.6082 | val_loss=1.0208 | val_acc=0.6187
Epoch 016 | loss=0.9552 | train_acc=0.6081 | val_loss=1.0211 | val_acc=0.6187
Epoch 017 | loss=0.9551 | train_acc=0.6085 | val_loss=1.0212 | val_acc=0.6187
Epoch 018 | loss=0.9545 | train_acc=0.6083 | val_loss=1.0212 | val_acc=0.6187
Epoch 019 | loss=0.9553 | train_acc=0.6081 | val_loss=1.0212 | val_acc=0.6187
Epoch 020 | loss=0.9548 | train_acc=0.6082 | val_loss=1.0212 | val_acc=0.6187
Epoch 021 | loss=0.9546 | train_acc=0.6088 | val_loss=1.0212 | val_acc=0.6187
Epoch 022 | loss=0.9547 | train_acc=0.6084 | val_loss=1.0212 | val_acc=0.6187
Epoch 023 | loss=0.9384 | train_acc=0.6380 | val_loss=7.9631 | val_acc=0.2084
Epoch 024 | loss=1.1681 | train_acc=0.6024 | val_loss=1.0223 | val_acc=0.6187
Epoch 025 | loss=0.9602 | train_acc=0.6079 | val_loss=1.0225 | val_acc=0.6187
Epoch 026 | loss=0.9579 | train_acc=0.6079 | val_loss=1.0230 | val_acc=0.6187
Epoch 027 | loss=0.9564 | train_acc=0.6086 | val_loss=1.0233 | val_acc=0.6187
Epoch 028 | loss=0.9566 | train_acc=0.6082 | val_loss=1.0233 | val_acc=0.6187
Epoch 029 | loss=0.9566 | train_acc=0.6081 | val_loss=1.0234 | val_acc=0.6187
Epoch 030 | loss=0.9563 | train_acc=0.6080 | val_loss=1.0235 | val_acc=0.6187
Epoch 031 | loss=0.9559 | train_acc=0.6085 | val_loss=1.0266 | val_acc=0.6187
Epoch 032 | loss=0.9579 | train_acc=0.6086 | val_loss=1.0236 | val_acc=0.6187
Epoch 033 | loss=0.9565 | train_acc=0.6082 | val_loss=1.0231 | val_acc=0.6187
Epoch 034 | loss=0.9561 | train_acc=0.6086 | val_loss=1.0233 | val_acc=0.6187
Epoch 035 | loss=0.9559 | train_acc=0.6085 | val_loss=1.0234 | val_acc=0.6187
Epoch 036 | loss=0.9577 | train_acc=0.6080 | val_loss=1.0234 | val_acc=0.6187
Epoch 037 | loss=0.9562 | train_acc=0.6085 | val_loss=1.0234 | val_acc=0.6187
Epoch 038 | loss=0.9562 | train_acc=0.6081 | val_loss=1.0231 | val_acc=0.6187
Epoch 039 | loss=0.9534 | train_acc=0.6215 | val_loss=4.2808 | val_acc=0.2680
Epoch 040 | loss=1.0901 | train_acc=0.6053 | val_loss=1.0231 | val_acc=0.6187
Epoch 041 | loss=0.9577 | train_acc=0.6080 | val_loss=1.0231 | val_acc=0.6187
Epoch 042 | loss=0.9558 | train_acc=0.6088 | val_loss=1.0234 | val_acc=0.6187
Epoch 043 | loss=0.9552 | train_acc=0.6083 | val_loss=1.0234 | val_acc=0.6187
Epoch 044 | loss=0.9557 | train_acc=0.6083 | val_loss=1.0235 | val_acc=0.6187
Epoch 045 | loss=0.9597 | train_acc=0.6084 | val_loss=1.0230 | val_acc=0.6187
Epoch 046 | loss=0.9555 | train_acc=0.6078 | val_loss=1.0232 | val_acc=0.6187
Epoch 047 | loss=0.9553 | train_acc=0.6083 | val_loss=1.0234 | val_acc=0.6187
Epoch 048 | loss=0.9567 | train_acc=0.6081 | val_loss=1.0235 | val_acc=0.6187
Epoch 049 | loss=0.9553 | train_acc=0.6077 | val_loss=1.0234 | val_acc=0.6187
Final Test Loss: 0.9204 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=12.9182 | train_acc=0.5811 | val_loss=41.0863 | val_acc=0.1529
Epoch 001 | loss=7.0781 | train_acc=0.4776 | val_loss=1.0872 | val_acc=0.6168
Epoch 002 | loss=1.3378 | train_acc=0.5305 | val_loss=1.0688 | val_acc=0.6187
Epoch 003 | loss=1.1564 | train_acc=0.5683 | val_loss=1.0537 | val_acc=0.6187
Epoch 004 | loss=1.1063 | train_acc=0.5831 | val_loss=1.0418 | val_acc=0.6187
Epoch 005 | loss=1.0712 | train_acc=0.5912 | val_loss=1.0324 | val_acc=0.6187
Epoch 006 | loss=1.0508 | train_acc=0.5958 | val_loss=1.0252 | val_acc=0.6187
Epoch 007 | loss=1.0302 | train_acc=0.5989 | val_loss=1.0198 | val_acc=0.6187
Epoch 008 | loss=1.0145 | train_acc=0.6012 | val_loss=1.0157 | val_acc=0.6187
Epoch 009 | loss=1.0028 | train_acc=0.6020 | val_loss=1.0127 | val_acc=0.6187
Epoch 010 | loss=0.9907 | train_acc=0.6039 | val_loss=1.0105 | val_acc=0.6187
Epoch 011 | loss=0.9861 | train_acc=0.6048 | val_loss=1.0090 | val_acc=0.6187
Epoch 012 | loss=0.9787 | train_acc=0.6053 | val_loss=1.0080 | val_acc=0.6187
Epoch 013 | loss=0.9714 | train_acc=0.6060 | val_loss=1.0074 | val_acc=0.6187
Epoch 014 | loss=0.9678 | train_acc=0.6055 | val_loss=1.0070 | val_acc=0.6187
Epoch 015 | loss=0.9658 | train_acc=0.6060 | val_loss=1.0069 | val_acc=0.6187
Epoch 016 | loss=0.9620 | train_acc=0.6060 | val_loss=1.0070 | val_acc=0.6187
Epoch 017 | loss=0.9571 | train_acc=0.6063 | val_loss=1.0073 | val_acc=0.6187
Epoch 018 | loss=0.9563 | train_acc=0.6066 | val_loss=1.0076 | val_acc=0.6187
Epoch 019 | loss=0.9536 | train_acc=0.6071 | val_loss=1.0081 | val_acc=0.6187
Epoch 020 | loss=0.9518 | train_acc=0.6070 | val_loss=1.0086 | val_acc=0.6187
Epoch 021 | loss=0.9501 | train_acc=0.6070 | val_loss=1.0092 | val_acc=0.6187
Epoch 022 | loss=0.9479 | train_acc=0.6073 | val_loss=1.0098 | val_acc=0.6187
Epoch 023 | loss=0.9471 | train_acc=0.6072 | val_loss=1.0104 | val_acc=0.6187
Epoch 024 | loss=0.9451 | train_acc=0.6078 | val_loss=1.0110 | val_acc=0.6187
Epoch 025 | loss=0.9448 | train_acc=0.6071 | val_loss=1.0116 | val_acc=0.6187
Epoch 026 | loss=0.9423 | train_acc=0.6077 | val_loss=1.0123 | val_acc=0.6187
Epoch 027 | loss=0.9426 | train_acc=0.6076 | val_loss=1.0129 | val_acc=0.6187
Epoch 028 | loss=0.9423 | train_acc=0.6072 | val_loss=1.0135 | val_acc=0.6187
Epoch 029 | loss=0.9404 | train_acc=0.6076 | val_loss=1.0141 | val_acc=0.6187
Epoch 030 | loss=0.9401 | train_acc=0.6079 | val_loss=1.0147 | val_acc=0.6187
Epoch 031 | loss=0.9405 | train_acc=0.6074 | val_loss=1.0152 | val_acc=0.6187
Epoch 032 | loss=0.9382 | train_acc=0.6081 | val_loss=1.0158 | val_acc=0.6187
Epoch 033 | loss=0.9391 | train_acc=0.6078 | val_loss=1.0164 | val_acc=0.6187
Epoch 034 | loss=0.9382 | train_acc=0.6078 | val_loss=1.0168 | val_acc=0.6187
Epoch 035 | loss=0.9383 | train_acc=0.6074 | val_loss=1.0173 | val_acc=0.6187
Epoch 036 | loss=0.9375 | train_acc=0.6080 | val_loss=1.0178 | val_acc=0.6187
Epoch 037 | loss=0.9373 | train_acc=0.6078 | val_loss=1.0182 | val_acc=0.6187
Epoch 038 | loss=0.9367 | train_acc=0.6076 | val_loss=1.0186 | val_acc=0.6187
Epoch 039 | loss=0.9367 | train_acc=0.6085 | val_loss=1.0190 | val_acc=0.6187
Epoch 040 | loss=0.9361 | train_acc=0.6079 | val_loss=1.0194 | val_acc=0.6187
Epoch 041 | loss=0.9344 | train_acc=0.6080 | val_loss=1.0198 | val_acc=0.6187
Epoch 042 | loss=0.9354 | train_acc=0.6080 | val_loss=1.0201 | val_acc=0.6187
Epoch 043 | loss=0.9352 | train_acc=0.6082 | val_loss=1.0205 | val_acc=0.6187
Epoch 044 | loss=0.9360 | train_acc=0.6080 | val_loss=1.0208 | val_acc=0.6187
Epoch 045 | loss=0.9349 | train_acc=0.6084 | val_loss=1.0211 | val_acc=0.6187
Epoch 046 | loss=0.9347 | train_acc=0.6082 | val_loss=1.0213 | val_acc=0.6187
Epoch 047 | loss=0.9346 | train_acc=0.6080 | val_loss=1.0216 | val_acc=0.6187
Epoch 048 | loss=0.9341 | train_acc=0.6081 | val_loss=1.0218 | val_acc=0.6187
Epoch 049 | loss=0.9350 | train_acc=0.6078 | val_loss=1.0219 | val_acc=0.6187
Final Test Loss: 0.9147 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=3.5325 | train_acc=0.6003 | val_loss=3.7608 | val_acc=0.2036
Epoch 001 | loss=1.6490 | train_acc=0.5157 | val_loss=1.0441 | val_acc=0.6187
Epoch 002 | loss=1.0254 | train_acc=0.5972 | val_loss=1.0276 | val_acc=0.6187
Epoch 003 | loss=0.9977 | train_acc=0.6031 | val_loss=1.0188 | val_acc=0.6187
Epoch 004 | loss=0.9799 | train_acc=0.6049 | val_loss=1.0150 | val_acc=0.6187
Epoch 005 | loss=0.9711 | train_acc=0.6062 | val_loss=1.0142 | val_acc=0.6187
Epoch 006 | loss=0.9644 | train_acc=0.6062 | val_loss=1.0148 | val_acc=0.6187
Epoch 007 | loss=0.9600 | train_acc=0.6071 | val_loss=1.0162 | val_acc=0.6187
Epoch 008 | loss=0.9565 | train_acc=0.6071 | val_loss=1.0177 | val_acc=0.6187
Epoch 009 | loss=0.9545 | train_acc=0.6072 | val_loss=1.0195 | val_acc=0.6187
Epoch 010 | loss=0.9530 | train_acc=0.6071 | val_loss=1.0209 | val_acc=0.6187
Epoch 011 | loss=0.9509 | train_acc=0.6076 | val_loss=1.0221 | val_acc=0.6187
Epoch 012 | loss=0.9501 | train_acc=0.6077 | val_loss=1.0231 | val_acc=0.6187
Epoch 013 | loss=0.9494 | train_acc=0.6071 | val_loss=1.0240 | val_acc=0.6187
Epoch 014 | loss=0.9485 | train_acc=0.6074 | val_loss=1.0246 | val_acc=0.6187
Epoch 015 | loss=0.9483 | train_acc=0.6077 | val_loss=1.0251 | val_acc=0.6187
Epoch 016 | loss=0.9479 | train_acc=0.6079 | val_loss=1.0254 | val_acc=0.6187
Epoch 017 | loss=0.9473 | train_acc=0.6084 | val_loss=1.0257 | val_acc=0.6187
Epoch 018 | loss=0.9471 | train_acc=0.6080 | val_loss=1.0259 | val_acc=0.6187
Epoch 019 | loss=0.9478 | train_acc=0.6081 | val_loss=1.0259 | val_acc=0.6187
Epoch 020 | loss=0.9475 | train_acc=0.6077 | val_loss=1.0260 | val_acc=0.6187
Epoch 021 | loss=0.9465 | train_acc=0.6083 | val_loss=1.0261 | val_acc=0.6187
Epoch 022 | loss=0.9467 | train_acc=0.6082 | val_loss=1.0260 | val_acc=0.6187
Epoch 023 | loss=0.9472 | train_acc=0.6078 | val_loss=1.0260 | val_acc=0.6187
Epoch 024 | loss=0.9470 | train_acc=0.6081 | val_loss=1.0260 | val_acc=0.6187
Epoch 025 | loss=0.9465 | train_acc=0.6083 | val_loss=1.0259 | val_acc=0.6187
Epoch 026 | loss=0.9475 | train_acc=0.6077 | val_loss=1.0256 | val_acc=0.6187
Epoch 027 | loss=0.9471 | train_acc=0.6085 | val_loss=1.0255 | val_acc=0.6187
Epoch 028 | loss=0.9466 | train_acc=0.6077 | val_loss=1.0253 | val_acc=0.6187
Epoch 029 | loss=0.9469 | train_acc=0.6080 | val_loss=1.0251 | val_acc=0.6187
Epoch 030 | loss=0.9474 | train_acc=0.6081 | val_loss=1.0249 | val_acc=0.6187
Epoch 031 | loss=0.9461 | train_acc=0.6076 | val_loss=1.0247 | val_acc=0.6187
Epoch 032 | loss=0.9463 | train_acc=0.6080 | val_loss=1.0245 | val_acc=0.6187
Epoch 033 | loss=0.9464 | train_acc=0.6078 | val_loss=1.0241 | val_acc=0.6187
Epoch 034 | loss=0.9459 | train_acc=0.6082 | val_loss=1.0238 | val_acc=0.6187
Epoch 035 | loss=0.9459 | train_acc=0.6085 | val_loss=1.0234 | val_acc=0.6187
Epoch 036 | loss=0.9465 | train_acc=0.6082 | val_loss=1.0229 | val_acc=0.6187
Epoch 037 | loss=0.9455 | train_acc=0.6086 | val_loss=1.0225 | val_acc=0.6187
Epoch 038 | loss=0.9466 | train_acc=0.6083 | val_loss=1.0221 | val_acc=0.6187
Epoch 039 | loss=0.9455 | train_acc=0.6081 | val_loss=1.0218 | val_acc=0.6187
Epoch 040 | loss=0.9456 | train_acc=0.6087 | val_loss=1.0216 | val_acc=0.6187
Epoch 041 | loss=0.9447 | train_acc=0.6087 | val_loss=1.0212 | val_acc=0.6187
Epoch 042 | loss=0.9455 | train_acc=0.6078 | val_loss=1.0208 | val_acc=0.6187
Epoch 043 | loss=0.9447 | train_acc=0.6089 | val_loss=1.0208 | val_acc=0.6187
Epoch 044 | loss=0.9440 | train_acc=0.6086 | val_loss=1.0204 | val_acc=0.6187
Epoch 045 | loss=0.9448 | train_acc=0.6081 | val_loss=1.0202 | val_acc=0.6187
Epoch 046 | loss=0.9446 | train_acc=0.6082 | val_loss=1.0202 | val_acc=0.6187
Epoch 047 | loss=0.9442 | train_acc=0.6081 | val_loss=1.0208 | val_acc=0.6187
Epoch 048 | loss=0.9456 | train_acc=0.6066 | val_loss=1.0187 | val_acc=0.6187
Epoch 049 | loss=0.9454 | train_acc=0.6089 | val_loss=1.0185 | val_acc=0.6187
Final Test Loss: 0.9158 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=10.3318 | train_acc=0.6236 | val_loss=49.9943 | val_acc=0.1803
Epoch 001 | loss=10.4386 | train_acc=0.5956 | val_loss=13.5049 | val_acc=0.1803
Epoch 002 | loss=3.9715 | train_acc=0.5634 | val_loss=1.5230 | val_acc=0.2233
Epoch 003 | loss=1.5045 | train_acc=0.4983 | val_loss=1.1452 | val_acc=0.5313
Epoch 004 | loss=1.1516 | train_acc=0.5482 | val_loss=1.0615 | val_acc=0.5950
Epoch 005 | loss=1.0807 | train_acc=0.5846 | val_loss=1.0439 | val_acc=0.6187
Epoch 006 | loss=1.0560 | train_acc=0.5931 | val_loss=1.0348 | val_acc=0.6187
Epoch 007 | loss=1.0388 | train_acc=0.5990 | val_loss=1.0272 | val_acc=0.6187
Epoch 008 | loss=1.0195 | train_acc=0.6012 | val_loss=1.0213 | val_acc=0.6187
Epoch 009 | loss=1.0039 | train_acc=0.6031 | val_loss=1.0171 | val_acc=0.6187
Epoch 010 | loss=0.9990 | train_acc=0.6030 | val_loss=1.0136 | val_acc=0.6187
Epoch 011 | loss=0.9873 | train_acc=0.6049 | val_loss=1.0108 | val_acc=0.6187
Epoch 012 | loss=0.9792 | train_acc=0.6053 | val_loss=1.0088 | val_acc=0.6187
Epoch 013 | loss=0.9724 | train_acc=0.6054 | val_loss=1.0072 | val_acc=0.6187
Epoch 014 | loss=0.9666 | train_acc=0.6061 | val_loss=1.0061 | val_acc=0.6187
Epoch 015 | loss=0.9625 | train_acc=0.6066 | val_loss=1.0055 | val_acc=0.6187
Epoch 016 | loss=0.9595 | train_acc=0.6069 | val_loss=1.0052 | val_acc=0.6187
Epoch 017 | loss=0.9560 | train_acc=0.6073 | val_loss=1.0051 | val_acc=0.6187
Epoch 018 | loss=0.9552 | train_acc=0.6067 | val_loss=1.0051 | val_acc=0.6187
Epoch 019 | loss=0.9512 | train_acc=0.6065 | val_loss=1.0053 | val_acc=0.6187
Epoch 020 | loss=0.9498 | train_acc=0.6076 | val_loss=1.0056 | val_acc=0.6187
Epoch 021 | loss=0.9474 | train_acc=0.6070 | val_loss=1.0059 | val_acc=0.6187
Epoch 022 | loss=0.9446 | train_acc=0.6075 | val_loss=1.0063 | val_acc=0.6187
Epoch 023 | loss=0.9436 | train_acc=0.6075 | val_loss=1.0067 | val_acc=0.6187
Epoch 024 | loss=0.9417 | train_acc=0.6078 | val_loss=1.0073 | val_acc=0.6187
Epoch 025 | loss=0.9414 | train_acc=0.6076 | val_loss=1.0077 | val_acc=0.6187
Epoch 026 | loss=0.9399 | train_acc=0.6075 | val_loss=1.0082 | val_acc=0.6187
Epoch 027 | loss=0.9378 | train_acc=0.6074 | val_loss=1.0087 | val_acc=0.6187
Epoch 028 | loss=0.9365 | train_acc=0.6075 | val_loss=1.0089 | val_acc=0.6187
Epoch 029 | loss=0.9359 | train_acc=0.6080 | val_loss=1.0094 | val_acc=0.6187
Epoch 030 | loss=0.9336 | train_acc=0.6077 | val_loss=1.0092 | val_acc=0.6187
Epoch 031 | loss=0.9335 | train_acc=0.6075 | val_loss=1.0098 | val_acc=0.6187
Epoch 032 | loss=0.9320 | train_acc=0.6082 | val_loss=1.0100 | val_acc=0.6187
Epoch 033 | loss=0.9321 | train_acc=0.6077 | val_loss=1.0101 | val_acc=0.6187
Epoch 034 | loss=0.9303 | train_acc=0.6077 | val_loss=1.0102 | val_acc=0.6187
Epoch 035 | loss=0.9296 | train_acc=0.6082 | val_loss=1.0099 | val_acc=0.6187
Epoch 036 | loss=0.9279 | train_acc=0.6083 | val_loss=1.0094 | val_acc=0.6187
Epoch 037 | loss=0.9290 | train_acc=0.6074 | val_loss=1.0092 | val_acc=0.6187
Epoch 038 | loss=0.9286 | train_acc=0.6079 | val_loss=1.0071 | val_acc=0.6187
Epoch 039 | loss=0.9273 | train_acc=0.6082 | val_loss=1.0072 | val_acc=0.6187
Epoch 040 | loss=0.9267 | train_acc=0.6079 | val_loss=1.0070 | val_acc=0.6187
Epoch 041 | loss=0.9262 | train_acc=0.6086 | val_loss=1.0083 | val_acc=0.6187
Epoch 042 | loss=0.9264 | train_acc=0.6081 | val_loss=1.0087 | val_acc=0.6187
Epoch 043 | loss=0.9254 | train_acc=0.6083 | val_loss=1.0044 | val_acc=0.6187
Epoch 044 | loss=0.9253 | train_acc=0.6086 | val_loss=1.0072 | val_acc=0.6187
Epoch 045 | loss=0.9240 | train_acc=0.6077 | val_loss=1.0071 | val_acc=0.6187
Epoch 046 | loss=0.9242 | train_acc=0.6085 | val_loss=1.0072 | val_acc=0.6187
Epoch 047 | loss=0.9262 | train_acc=0.6082 | val_loss=1.0077 | val_acc=0.6187
Epoch 048 | loss=0.9256 | train_acc=0.6081 | val_loss=1.0084 | val_acc=0.6187
Epoch 049 | loss=0.9252 | train_acc=0.6080 | val_loss=1.0088 | val_acc=0.6187
Final Test Loss: 0.9065 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=3.6115 | train_acc=0.6591 | val_loss=7.4500 | val_acc=0.1918
Epoch 001 | loss=2.3874 | train_acc=0.5308 | val_loss=1.0871 | val_acc=0.3280
Epoch 002 | loss=1.3016 | train_acc=0.5684 | val_loss=1.0514 | val_acc=0.6187
Epoch 003 | loss=1.1027 | train_acc=0.5805 | val_loss=1.0436 | val_acc=0.6187
Epoch 004 | loss=1.0374 | train_acc=0.5915 | val_loss=1.0348 | val_acc=0.6187
Epoch 005 | loss=1.0144 | train_acc=0.6017 | val_loss=1.0275 | val_acc=0.6187
Epoch 006 | loss=1.0014 | train_acc=0.6051 | val_loss=1.0222 | val_acc=0.6187
Epoch 007 | loss=0.9920 | train_acc=0.6072 | val_loss=1.0178 | val_acc=0.6187
Epoch 008 | loss=0.9846 | train_acc=0.6074 | val_loss=1.0140 | val_acc=0.6187
Epoch 009 | loss=0.9786 | train_acc=0.6076 | val_loss=1.0119 | val_acc=0.6187
Epoch 010 | loss=0.9752 | train_acc=0.6074 | val_loss=1.0097 | val_acc=0.6187
Epoch 011 | loss=0.9695 | train_acc=0.6076 | val_loss=1.0088 | val_acc=0.6187
Epoch 012 | loss=0.9645 | train_acc=0.6079 | val_loss=1.0066 | val_acc=0.6187
Epoch 013 | loss=0.9616 | train_acc=0.6078 | val_loss=1.0057 | val_acc=0.6187
Epoch 014 | loss=0.9582 | train_acc=0.6083 | val_loss=1.0048 | val_acc=0.6187
Epoch 015 | loss=0.9560 | train_acc=0.6080 | val_loss=1.0043 | val_acc=0.6187
Epoch 016 | loss=0.9541 | train_acc=0.6079 | val_loss=1.0035 | val_acc=0.6187
Epoch 017 | loss=0.9519 | train_acc=0.6079 | val_loss=1.0033 | val_acc=0.6187
Epoch 018 | loss=0.9482 | train_acc=0.6084 | val_loss=1.0037 | val_acc=0.6187
Epoch 019 | loss=0.9471 | train_acc=0.6081 | val_loss=1.0033 | val_acc=0.6187
Epoch 020 | loss=0.9462 | train_acc=0.6080 | val_loss=1.0015 | val_acc=0.6187
Epoch 021 | loss=0.9445 | train_acc=0.6081 | val_loss=0.9999 | val_acc=0.6187
Epoch 022 | loss=0.9437 | train_acc=0.6078 | val_loss=0.9993 | val_acc=0.6187
Epoch 023 | loss=0.9412 | train_acc=0.6084 | val_loss=0.9992 | val_acc=0.6187
Epoch 024 | loss=0.9396 | train_acc=0.6083 | val_loss=0.9962 | val_acc=0.6187
Epoch 025 | loss=0.9390 | train_acc=0.6082 | val_loss=0.9944 | val_acc=0.6187
Epoch 026 | loss=0.9363 | train_acc=0.6084 | val_loss=0.9964 | val_acc=0.6187
Epoch 027 | loss=0.9369 | train_acc=0.6082 | val_loss=1.0006 | val_acc=0.6187
Epoch 028 | loss=0.9355 | train_acc=0.6075 | val_loss=0.9984 | val_acc=0.6187
Epoch 029 | loss=0.9369 | train_acc=0.6077 | val_loss=0.9960 | val_acc=0.6187
Epoch 030 | loss=0.9351 | train_acc=0.6085 | val_loss=0.9986 | val_acc=0.6187
Epoch 031 | loss=0.9346 | train_acc=0.6079 | val_loss=0.9967 | val_acc=0.6187
Epoch 032 | loss=0.9315 | train_acc=0.6083 | val_loss=0.9945 | val_acc=0.6187
Epoch 033 | loss=0.9295 | train_acc=0.6081 | val_loss=0.9959 | val_acc=0.6187
Epoch 034 | loss=0.9309 | train_acc=0.6082 | val_loss=1.0050 | val_acc=0.6187
Epoch 035 | loss=0.9324 | train_acc=0.6083 | val_loss=1.0088 | val_acc=0.6187
Epoch 036 | loss=0.9320 | train_acc=0.6082 | val_loss=1.0050 | val_acc=0.6187
Epoch 037 | loss=0.9294 | train_acc=0.6078 | val_loss=1.0076 | val_acc=0.6187
Epoch 038 | loss=0.9293 | train_acc=0.6088 | val_loss=1.0102 | val_acc=0.6187
Epoch 039 | loss=0.9300 | train_acc=0.6080 | val_loss=1.0100 | val_acc=0.6187
Epoch 040 | loss=0.9315 | train_acc=0.6084 | val_loss=1.0090 | val_acc=0.6187
Epoch 041 | loss=0.9315 | train_acc=0.6080 | val_loss=1.0115 | val_acc=0.6187
Epoch 042 | loss=0.9321 | train_acc=0.6079 | val_loss=1.0111 | val_acc=0.6187
Epoch 043 | loss=1.1253 | train_acc=0.6154 | val_loss=3.1288 | val_acc=0.2662
Epoch 044 | loss=1.2437 | train_acc=0.6651 | val_loss=12.0375 | val_acc=0.2007
Epoch 045 | loss=1.6669 | train_acc=0.6087 | val_loss=3.3288 | val_acc=0.3373
Epoch 046 | loss=1.3178 | train_acc=0.6289 | val_loss=4.6521 | val_acc=0.2862
Epoch 047 | loss=1.1868 | train_acc=0.6103 | val_loss=1.7626 | val_acc=0.4261
Epoch 048 | loss=1.1879 | train_acc=0.5959 | val_loss=0.9805 | val_acc=0.6179
Epoch 049 | loss=0.9450 | train_acc=0.6070 | val_loss=1.0124 | val_acc=0.6187
Final Test Loss: 0.9197 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=12.8623 | train_acc=0.6426 | val_loss=62.6815 | val_acc=0.1529
Epoch 001 | loss=14.0649 | train_acc=0.6122 | val_loss=59.2768 | val_acc=0.1803
Epoch 002 | loss=6.4569 | train_acc=0.5664 | val_loss=2.0732 | val_acc=0.1814
Epoch 003 | loss=1.8232 | train_acc=0.5128 | val_loss=1.0757 | val_acc=0.6183
Epoch 004 | loss=1.1463 | train_acc=0.5432 | val_loss=1.0622 | val_acc=0.6187
Epoch 005 | loss=1.0979 | train_acc=0.5793 | val_loss=1.0508 | val_acc=0.6187
Epoch 006 | loss=1.0670 | train_acc=0.5909 | val_loss=1.0414 | val_acc=0.6187
Epoch 007 | loss=1.0444 | train_acc=0.5969 | val_loss=1.0337 | val_acc=0.6187
Epoch 008 | loss=1.0264 | train_acc=0.5997 | val_loss=1.0272 | val_acc=0.6187
Epoch 009 | loss=1.0134 | train_acc=0.6019 | val_loss=1.0219 | val_acc=0.6187
Epoch 010 | loss=1.0090 | train_acc=0.6033 | val_loss=1.0179 | val_acc=0.6187
Epoch 011 | loss=0.9955 | train_acc=0.6045 | val_loss=1.0146 | val_acc=0.6187
Epoch 012 | loss=0.9877 | train_acc=0.6043 | val_loss=1.0120 | val_acc=0.6187
Epoch 013 | loss=0.9800 | train_acc=0.6054 | val_loss=1.0101 | val_acc=0.6187
Epoch 014 | loss=0.9748 | train_acc=0.6057 | val_loss=1.0086 | val_acc=0.6187
Epoch 015 | loss=0.9719 | train_acc=0.6062 | val_loss=1.0076 | val_acc=0.6187
Epoch 016 | loss=0.9670 | train_acc=0.6064 | val_loss=1.0069 | val_acc=0.6187
Epoch 017 | loss=0.9636 | train_acc=0.6068 | val_loss=1.0065 | val_acc=0.6187
Epoch 018 | loss=0.9583 | train_acc=0.6070 | val_loss=1.0061 | val_acc=0.6187
Epoch 019 | loss=0.9564 | train_acc=0.6069 | val_loss=1.0061 | val_acc=0.6187
Epoch 020 | loss=0.9533 | train_acc=0.6074 | val_loss=1.0061 | val_acc=0.6187
Epoch 021 | loss=0.9555 | train_acc=0.6071 | val_loss=1.0068 | val_acc=0.6187
Epoch 022 | loss=0.9518 | train_acc=0.6075 | val_loss=1.0071 | val_acc=0.6187
Epoch 023 | loss=0.9485 | train_acc=0.6073 | val_loss=1.0074 | val_acc=0.6187
Epoch 024 | loss=0.9481 | train_acc=0.6071 | val_loss=1.0077 | val_acc=0.6187
Epoch 025 | loss=0.9459 | train_acc=0.6079 | val_loss=1.0081 | val_acc=0.6187
Epoch 026 | loss=0.9435 | train_acc=0.6077 | val_loss=1.0083 | val_acc=0.6187
Epoch 027 | loss=0.9424 | train_acc=0.6074 | val_loss=1.0086 | val_acc=0.6187
Epoch 028 | loss=0.9417 | train_acc=0.6077 | val_loss=1.0091 | val_acc=0.6187
Epoch 029 | loss=0.9399 | train_acc=0.6085 | val_loss=1.0096 | val_acc=0.6187
Epoch 030 | loss=0.9401 | train_acc=0.6081 | val_loss=1.0101 | val_acc=0.6187
Epoch 031 | loss=0.9392 | train_acc=0.6076 | val_loss=1.0105 | val_acc=0.6187
Epoch 032 | loss=0.9383 | train_acc=0.6078 | val_loss=1.0109 | val_acc=0.6187
Epoch 033 | loss=0.9363 | train_acc=0.6079 | val_loss=1.0112 | val_acc=0.6187
Epoch 034 | loss=0.9353 | train_acc=0.6079 | val_loss=1.0115 | val_acc=0.6187
Epoch 035 | loss=0.9369 | train_acc=0.6082 | val_loss=1.0117 | val_acc=0.6187
Epoch 036 | loss=0.9365 | train_acc=0.6078 | val_loss=1.0122 | val_acc=0.6187
Epoch 037 | loss=0.9355 | train_acc=0.6075 | val_loss=1.0125 | val_acc=0.6187
Epoch 038 | loss=0.9342 | train_acc=0.6079 | val_loss=1.0124 | val_acc=0.6187
Epoch 039 | loss=0.9347 | train_acc=0.6080 | val_loss=1.0122 | val_acc=0.6187
Epoch 040 | loss=0.9332 | train_acc=0.6078 | val_loss=1.0116 | val_acc=0.6187
Epoch 041 | loss=0.9313 | train_acc=0.6083 | val_loss=1.0103 | val_acc=0.6187
Epoch 042 | loss=0.9300 | train_acc=0.6082 | val_loss=1.0093 | val_acc=0.6187
Epoch 043 | loss=0.9307 | train_acc=0.6085 | val_loss=1.0090 | val_acc=0.6187
Epoch 044 | loss=0.9287 | train_acc=0.6076 | val_loss=1.0086 | val_acc=0.6187
Epoch 045 | loss=0.9299 | train_acc=0.6082 | val_loss=1.0086 | val_acc=0.6187
Epoch 046 | loss=0.9300 | train_acc=0.6084 | val_loss=1.0083 | val_acc=0.6187
Epoch 047 | loss=0.9277 | train_acc=0.6083 | val_loss=1.0033 | val_acc=0.6187
Epoch 048 | loss=0.9283 | train_acc=0.6077 | val_loss=0.9990 | val_acc=0.6187
Epoch 049 | loss=0.9278 | train_acc=0.6080 | val_loss=1.0025 | val_acc=0.6187
Final Test Loss: 0.9000 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=6.1926 | train_acc=0.6437 | val_loss=8.9466 | val_acc=0.1888
Epoch 001 | loss=2.8280 | train_acc=0.5241 | val_loss=1.1910 | val_acc=0.2514
Epoch 002 | loss=1.2681 | train_acc=0.5677 | val_loss=1.0597 | val_acc=0.6187
Epoch 003 | loss=1.2036 | train_acc=0.5906 | val_loss=1.0443 | val_acc=0.6187
Epoch 004 | loss=1.4854 | train_acc=0.5961 | val_loss=1.0628 | val_acc=0.4950
Epoch 005 | loss=1.1307 | train_acc=0.5928 | val_loss=1.0416 | val_acc=0.6061
Epoch 006 | loss=1.1330 | train_acc=0.5955 | val_loss=1.0284 | val_acc=0.6164
Epoch 007 | loss=1.0360 | train_acc=0.5955 | val_loss=1.0241 | val_acc=0.6187
Epoch 008 | loss=0.9996 | train_acc=0.6028 | val_loss=1.0203 | val_acc=0.6187
Epoch 009 | loss=0.9885 | train_acc=0.6059 | val_loss=1.0169 | val_acc=0.6187
Epoch 010 | loss=0.9813 | train_acc=0.6069 | val_loss=1.0146 | val_acc=0.6187
Epoch 011 | loss=0.9757 | train_acc=0.6075 | val_loss=1.0128 | val_acc=0.6187
Epoch 012 | loss=0.9706 | train_acc=0.6071 | val_loss=1.0117 | val_acc=0.6187
Epoch 013 | loss=0.9665 | train_acc=0.6073 | val_loss=1.0108 | val_acc=0.6187
Epoch 014 | loss=0.9629 | train_acc=0.6079 | val_loss=1.0103 | val_acc=0.6187
Epoch 015 | loss=0.9594 | train_acc=0.6079 | val_loss=1.0096 | val_acc=0.6187
Epoch 016 | loss=0.9572 | train_acc=0.6079 | val_loss=1.0091 | val_acc=0.6187
Epoch 017 | loss=0.9544 | train_acc=0.6079 | val_loss=1.0085 | val_acc=0.6187
Epoch 018 | loss=0.9523 | train_acc=0.6083 | val_loss=1.0084 | val_acc=0.6187
Epoch 019 | loss=0.9493 | train_acc=0.6086 | val_loss=1.0090 | val_acc=0.6187
Epoch 020 | loss=0.9487 | train_acc=0.6085 | val_loss=1.0088 | val_acc=0.6187
Epoch 021 | loss=0.9469 | train_acc=0.6083 | val_loss=1.0087 | val_acc=0.6187
Epoch 022 | loss=0.9451 | train_acc=0.6086 | val_loss=1.0089 | val_acc=0.6187
Epoch 023 | loss=0.9438 | train_acc=0.6086 | val_loss=1.0091 | val_acc=0.6187
Epoch 024 | loss=0.9438 | train_acc=0.6080 | val_loss=1.0092 | val_acc=0.6187
Epoch 025 | loss=0.9411 | train_acc=0.6083 | val_loss=1.0094 | val_acc=0.6187
Epoch 026 | loss=0.9411 | train_acc=0.6079 | val_loss=1.0094 | val_acc=0.6187
Epoch 027 | loss=0.9398 | train_acc=0.6085 | val_loss=1.0099 | val_acc=0.6187
Epoch 028 | loss=0.9392 | train_acc=0.6083 | val_loss=1.0103 | val_acc=0.6187
Epoch 029 | loss=0.9390 | train_acc=0.6081 | val_loss=1.0103 | val_acc=0.6187
Epoch 030 | loss=0.9383 | train_acc=0.6088 | val_loss=1.0103 | val_acc=0.6187
Epoch 031 | loss=0.9376 | train_acc=0.6086 | val_loss=1.0107 | val_acc=0.6187
Epoch 032 | loss=0.9371 | train_acc=0.6084 | val_loss=1.0110 | val_acc=0.6187
Epoch 033 | loss=0.9366 | train_acc=0.6084 | val_loss=1.0114 | val_acc=0.6187
Epoch 034 | loss=0.9361 | train_acc=0.6086 | val_loss=1.0119 | val_acc=0.6187
Epoch 035 | loss=0.9362 | train_acc=0.6078 | val_loss=1.0119 | val_acc=0.6187
Epoch 036 | loss=0.9354 | train_acc=0.6078 | val_loss=1.0116 | val_acc=0.6187
Epoch 037 | loss=0.9364 | train_acc=0.6083 | val_loss=1.0127 | val_acc=0.6187
Epoch 038 | loss=0.9350 | train_acc=0.6085 | val_loss=1.0133 | val_acc=0.6187
Epoch 039 | loss=0.9351 | train_acc=0.6076 | val_loss=1.0120 | val_acc=0.6187
Epoch 040 | loss=0.9352 | train_acc=0.6088 | val_loss=1.0120 | val_acc=0.6187
Epoch 041 | loss=0.9334 | train_acc=0.6082 | val_loss=1.0118 | val_acc=0.6187
Epoch 042 | loss=0.9349 | train_acc=0.6085 | val_loss=1.0121 | val_acc=0.6187
Epoch 043 | loss=0.9337 | train_acc=0.6088 | val_loss=1.0122 | val_acc=0.6187
Epoch 044 | loss=0.9345 | train_acc=0.6083 | val_loss=1.0108 | val_acc=0.6187
Epoch 045 | loss=0.9333 | train_acc=0.6077 | val_loss=1.0100 | val_acc=0.6187
Epoch 046 | loss=0.9359 | train_acc=0.6081 | val_loss=1.0161 | val_acc=0.6187
Epoch 047 | loss=0.9346 | train_acc=0.6083 | val_loss=1.0178 | val_acc=0.6187
Epoch 048 | loss=1.4465 | train_acc=0.6450 | val_loss=9.8801 | val_acc=0.2447
Epoch 049 | loss=1.9749 | train_acc=0.6315 | val_loss=11.0371 | val_acc=0.2788
Final Test Loss: 11.2210 | Test Accuracy: 0.2714
