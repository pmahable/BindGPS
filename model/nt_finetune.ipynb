{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e139a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56a472c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "import torcheval\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pandas as pd\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9090c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin1_id</th>\n",
       "      <th>chr</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>clamp</th>\n",
       "      <th>gaf</th>\n",
       "      <th>h3k27ac</th>\n",
       "      <th>h3k27me3</th>\n",
       "      <th>h3k36me3</th>\n",
       "      <th>h3k4me1</th>\n",
       "      <th>h3k4me2</th>\n",
       "      <th>h3k4me3</th>\n",
       "      <th>h3k9me3</th>\n",
       "      <th>h4k16ac</th>\n",
       "      <th>psq</th>\n",
       "      <th>seq</th>\n",
       "      <th>gene_labels</th>\n",
       "      <th>mre_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>chr2L</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.011287</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.045745</td>\n",
       "      <td>CGACAATGCACGACAGAGGAAGCAGAACAGATATTTAGATTGCCTC...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>chr2L</td>\n",
       "      <td>1000</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.011801</td>\n",
       "      <td>0.029785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.008160</td>\n",
       "      <td>CGTATGCGAGAGTAGTGCCAACATATTGTGCTCTTTGATTTTTTGG...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>chr2L</td>\n",
       "      <td>2000</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.011536</td>\n",
       "      <td>0.029358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.077650</td>\n",
       "      <td>ATTCAAGTTGCCGCTAATCAGAAATAAATTCATTGCAACGTTAAAT...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>chr2L</td>\n",
       "      <td>3000</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.011079</td>\n",
       "      <td>0.030652</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.028765</td>\n",
       "      <td>AACATATTGTGCTAATGAGTGCCTCTCGTTCTCTGTCTTATATTAC...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>chr2L</td>\n",
       "      <td>4000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.011053</td>\n",
       "      <td>0.032013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.014130</td>\n",
       "      <td>GCAGCGGAGATATTTAGATTGCCTATTAAATATGATCGCGTATGCG...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bin1_id    chr  start   end     clamp       gaf  h3k27ac  h3k27me3  \\\n",
       "0        0  chr2L      0  1000  0.011287  0.026900      0.0       0.0   \n",
       "1        1  chr2L   1000  2000  0.011801  0.029785      0.0       0.0   \n",
       "2        2  chr2L   2000  3000  0.011536  0.029358      0.0       0.0   \n",
       "3        3  chr2L   3000  4000  0.011079  0.030652      0.0       0.0   \n",
       "4        4  chr2L   4000  5000  0.011053  0.032013      0.0       0.0   \n",
       "\n",
       "   h3k36me3  h3k4me1  h3k4me2  h3k4me3  h3k9me3  h4k16ac        psq  \\\n",
       "0    0.0000      0.0      0.0      0.0      0.0      0.0  30.045745   \n",
       "1    0.0000      0.0      0.0      0.0      0.0      0.0  30.008160   \n",
       "2    0.0000      0.0      0.0      0.0      0.0      0.0  30.077650   \n",
       "3    0.0000      0.0      0.0      0.0      0.0      0.0  40.028765   \n",
       "4   20.0032      0.0      0.0      0.0      0.0      0.0  40.014130   \n",
       "\n",
       "                                                 seq  gene_labels  mre_labels  \n",
       "0  CGACAATGCACGACAGAGGAAGCAGAACAGATATTTAGATTGCCTC...            0           1  \n",
       "1  CGTATGCGAGAGTAGTGCCAACATATTGTGCTCTTTGATTTTTTGG...            0           1  \n",
       "2  ATTCAAGTTGCCGCTAATCAGAAATAAATTCATTGCAACGTTAAAT...            0           1  \n",
       "3  AACATATTGTGCTAATGAGTGCCTCTCGTTCTCTGTCTTATATTAC...            0           1  \n",
       "4  GCAGCGGAGATATTTAGATTGCCTATTAAATATGATCGCGTATGCG...            0           1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data_transformation/chipseq/updated_histone_mods_and_GA_factors.feature_matrix.1kb_res.txt\", sep=\"\\t\")\n",
    "df['seq'] = df['seq'].map(lambda x: x.upper())\n",
    "df['gene_labels'] = df['gene_labels'].astype(int)\n",
    "df['mre_labels'] = df['mre_labels'].astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c734955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, TrainingArguments, Trainer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score, roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "device, _, _ = get_backend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa59ea41",
   "metadata": {},
   "source": [
    "## Gene Task Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "734c4e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading InstaDeepAI/nucleotide-transformer-v2-50m-multi-species requires to execute some code in that repo, you can inspect the content of the repository at https://hf.co/InstaDeepAI/nucleotide-transformer-v2-50m-multi-species. You can dismiss this prompt by passing `trust_remote_code=True`.\n",
      "Do you accept? [y/N] y\n",
      "Loading InstaDeepAI/nucleotide-transformer-v2-50m-multi-species requires to execute some code in that repo, you can inspect the content of the repository at https://hf.co/InstaDeepAI/nucleotide-transformer-v2-50m-multi-species. You can dismiss this prompt by passing `trust_remote_code=True`.\n",
      "Do you accept? [y/N] y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at InstaDeepAI/nucleotide-transformer-v2-50m-multi-species were not used when initializing EsmForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing EsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at InstaDeepAI/nucleotide-transformer-v2-50m-multi-species and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels_gene = 3\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"InstaDeepAI/nucleotide-transformer-v2-50m-multi-species\", num_labels=num_labels_gene)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "639a60bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 25,454,595 || all params: 79,253,191 || trainable%: 32.118069542461704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): EsmForSequenceClassification(\n",
       "      (esm): EsmModel(\n",
       "        (embeddings): EsmEmbeddings(\n",
       "          (word_embeddings): Embedding(4107, 512, padding_idx=1)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (position_embeddings): Embedding(2050, 512, padding_idx=1)\n",
       "        )\n",
       "        (encoder): EsmEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0): EsmLayer(\n",
       "              (attention): EsmAttention(\n",
       "                (self): EsmSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=1, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=1, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=1, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=1, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (rotary_embeddings): RotaryEmbedding()\n",
       "                )\n",
       "                (output): EsmSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (intermediate): ModulesToSaveWrapper(\n",
       "                (original_module): EsmIntermediate(\n",
       "                  (dense): Linear(in_features=512, out_features=4096, bias=False)\n",
       "                  (activation_fn): SiLU()\n",
       "                )\n",
       "                (modules_to_save): ModuleDict(\n",
       "                  (default): EsmIntermediate(\n",
       "                    (dense): Linear(in_features=512, out_features=4096, bias=False)\n",
       "                    (activation_fn): SiLU()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): EsmOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "            (1): EsmLayer(\n",
       "              (attention): EsmAttention(\n",
       "                (self): EsmSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=1, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=1, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=1, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=1, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (rotary_embeddings): RotaryEmbedding()\n",
       "                )\n",
       "                (output): EsmSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (intermediate): ModulesToSaveWrapper(\n",
       "                (original_module): EsmIntermediate(\n",
       "                  (dense): Linear(in_features=512, out_features=4096, bias=False)\n",
       "                  (activation_fn): SiLU()\n",
       "                )\n",
       "                (modules_to_save): ModuleDict(\n",
       "                  (default): EsmIntermediate(\n",
       "                    (dense): Linear(in_features=512, out_features=4096, bias=False)\n",
       "                    (activation_fn): SiLU()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): EsmOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "            (2): EsmLayer(\n",
       "              (attention): EsmAttention(\n",
       "                (self): EsmSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=1, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=1, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=1, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=1, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (rotary_embeddings): RotaryEmbedding()\n",
       "                )\n",
       "                (output): EsmSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (intermediate): ModulesToSaveWrapper(\n",
       "                (original_module): EsmIntermediate(\n",
       "                  (dense): Linear(in_features=512, out_features=4096, bias=False)\n",
       "                  (activation_fn): SiLU()\n",
       "                )\n",
       "                (modules_to_save): ModuleDict(\n",
       "                  (default): EsmIntermediate(\n",
       "                    (dense): Linear(in_features=512, out_features=4096, bias=False)\n",
       "                    (activation_fn): SiLU()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): EsmOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "            (3): EsmLayer(\n",
       "              (attention): EsmAttention(\n",
       "                (self): EsmSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=1, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=1, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=1, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=1, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (rotary_embeddings): RotaryEmbedding()\n",
       "                )\n",
       "                (output): EsmSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (intermediate): ModulesToSaveWrapper(\n",
       "                (original_module): EsmIntermediate(\n",
       "                  (dense): Linear(in_features=512, out_features=4096, bias=False)\n",
       "                  (activation_fn): SiLU()\n",
       "                )\n",
       "                (modules_to_save): ModuleDict(\n",
       "                  (default): EsmIntermediate(\n",
       "                    (dense): Linear(in_features=512, out_features=4096, bias=False)\n",
       "                    (activation_fn): SiLU()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): EsmOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "            (4): EsmLayer(\n",
       "              (attention): EsmAttention(\n",
       "                (self): EsmSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=1, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=1, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=1, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=1, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (rotary_embeddings): RotaryEmbedding()\n",
       "                )\n",
       "                (output): EsmSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (intermediate): ModulesToSaveWrapper(\n",
       "                (original_module): EsmIntermediate(\n",
       "                  (dense): Linear(in_features=512, out_features=4096, bias=False)\n",
       "                  (activation_fn): SiLU()\n",
       "                )\n",
       "                (modules_to_save): ModuleDict(\n",
       "                  (default): EsmIntermediate(\n",
       "                    (dense): Linear(in_features=512, out_features=4096, bias=False)\n",
       "                    (activation_fn): SiLU()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): EsmOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "            (5): EsmLayer(\n",
       "              (attention): EsmAttention(\n",
       "                (self): EsmSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=1, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=1, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=1, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=1, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (rotary_embeddings): RotaryEmbedding()\n",
       "                )\n",
       "                (output): EsmSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (intermediate): ModulesToSaveWrapper(\n",
       "                (original_module): EsmIntermediate(\n",
       "                  (dense): Linear(in_features=512, out_features=4096, bias=False)\n",
       "                  (activation_fn): SiLU()\n",
       "                )\n",
       "                (modules_to_save): ModuleDict(\n",
       "                  (default): EsmIntermediate(\n",
       "                    (dense): Linear(in_features=512, out_features=4096, bias=False)\n",
       "                    (activation_fn): SiLU()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): EsmOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "            (6): EsmLayer(\n",
       "              (attention): EsmAttention(\n",
       "                (self): EsmSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=1, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=1, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=1, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=1, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (rotary_embeddings): RotaryEmbedding()\n",
       "                )\n",
       "                (output): EsmSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (intermediate): ModulesToSaveWrapper(\n",
       "                (original_module): EsmIntermediate(\n",
       "                  (dense): Linear(in_features=512, out_features=4096, bias=False)\n",
       "                  (activation_fn): SiLU()\n",
       "                )\n",
       "                (modules_to_save): ModuleDict(\n",
       "                  (default): EsmIntermediate(\n",
       "                    (dense): Linear(in_features=512, out_features=4096, bias=False)\n",
       "                    (activation_fn): SiLU()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): EsmOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "            (7): EsmLayer(\n",
       "              (attention): EsmAttention(\n",
       "                (self): EsmSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=1, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=1, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=1, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=1, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (rotary_embeddings): RotaryEmbedding()\n",
       "                )\n",
       "                (output): EsmSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (intermediate): ModulesToSaveWrapper(\n",
       "                (original_module): EsmIntermediate(\n",
       "                  (dense): Linear(in_features=512, out_features=4096, bias=False)\n",
       "                  (activation_fn): SiLU()\n",
       "                )\n",
       "                (modules_to_save): ModuleDict(\n",
       "                  (default): EsmIntermediate(\n",
       "                    (dense): Linear(in_features=512, out_features=4096, bias=False)\n",
       "                    (activation_fn): SiLU()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): EsmOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "            (8): EsmLayer(\n",
       "              (attention): EsmAttention(\n",
       "                (self): EsmSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=1, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=1, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=1, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=1, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (rotary_embeddings): RotaryEmbedding()\n",
       "                )\n",
       "                (output): EsmSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (intermediate): ModulesToSaveWrapper(\n",
       "                (original_module): EsmIntermediate(\n",
       "                  (dense): Linear(in_features=512, out_features=4096, bias=False)\n",
       "                  (activation_fn): SiLU()\n",
       "                )\n",
       "                (modules_to_save): ModuleDict(\n",
       "                  (default): EsmIntermediate(\n",
       "                    (dense): Linear(in_features=512, out_features=4096, bias=False)\n",
       "                    (activation_fn): SiLU()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): EsmOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "            (9): EsmLayer(\n",
       "              (attention): EsmAttention(\n",
       "                (self): EsmSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=1, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=1, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=1, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=1, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (rotary_embeddings): RotaryEmbedding()\n",
       "                )\n",
       "                (output): EsmSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (intermediate): ModulesToSaveWrapper(\n",
       "                (original_module): EsmIntermediate(\n",
       "                  (dense): Linear(in_features=512, out_features=4096, bias=False)\n",
       "                  (activation_fn): SiLU()\n",
       "                )\n",
       "                (modules_to_save): ModuleDict(\n",
       "                  (default): EsmIntermediate(\n",
       "                    (dense): Linear(in_features=512, out_features=4096, bias=False)\n",
       "                    (activation_fn): SiLU()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): EsmOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "            (10): EsmLayer(\n",
       "              (attention): EsmAttention(\n",
       "                (self): EsmSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=1, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=1, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=1, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=1, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (rotary_embeddings): RotaryEmbedding()\n",
       "                )\n",
       "                (output): EsmSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (intermediate): ModulesToSaveWrapper(\n",
       "                (original_module): EsmIntermediate(\n",
       "                  (dense): Linear(in_features=512, out_features=4096, bias=False)\n",
       "                  (activation_fn): SiLU()\n",
       "                )\n",
       "                (modules_to_save): ModuleDict(\n",
       "                  (default): EsmIntermediate(\n",
       "                    (dense): Linear(in_features=512, out_features=4096, bias=False)\n",
       "                    (activation_fn): SiLU()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): EsmOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "            (11): EsmLayer(\n",
       "              (attention): EsmAttention(\n",
       "                (self): EsmSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=1, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=1, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=1, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=1, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (rotary_embeddings): RotaryEmbedding()\n",
       "                )\n",
       "                (output): EsmSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (intermediate): ModulesToSaveWrapper(\n",
       "                (original_module): EsmIntermediate(\n",
       "                  (dense): Linear(in_features=512, out_features=4096, bias=False)\n",
       "                  (activation_fn): SiLU()\n",
       "                )\n",
       "                (modules_to_save): ModuleDict(\n",
       "                  (default): EsmIntermediate(\n",
       "                    (dense): Linear(in_features=512, out_features=4096, bias=False)\n",
       "                    (activation_fn): SiLU()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (output): EsmOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=False)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (emb_layer_norm_after): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (contact_head): EsmContactPredictionHead(\n",
       "          (regression): Linear(in_features=192, out_features=1, bias=True)\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): EsmClassificationHead(\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (out_proj): Linear(in_features=512, out_features=3, bias=True)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): EsmClassificationHead(\n",
       "            (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (out_proj): Linear(in_features=512, out_features=3, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, inference_mode=False, r=1, lora_alpha= 32, lora_dropout=0.1, target_modules= [\"query\", \"value\"],\n",
    "    modules_to_save=[\"intermediate\"] # modules that are not frozen and updated during the training\n",
    ")\n",
    "\n",
    "from peft import get_peft_model\n",
    "\n",
    "lora_classifier = get_peft_model(model, peft_config) # transform our classifier into a peft model\n",
    "lora_classifier.print_trainable_parameters()\n",
    "lora_classifier.to(device) # Put the model on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc35e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "X, y = df['seq'], df['gene_labels']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "\n",
    "# validation = 0.25 * 0.8 = 0.2; train size = 0.6 overall\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=1, stratify=y_train)\n",
    "\n",
    "# Gene Label Dataset dataset\n",
    "ds_train = Dataset.from_dict({\"data\": X_train,'labels':y_train})\n",
    "ds_val = Dataset.from_dict({\"data\": X_val,'labels':y_val})\n",
    "ds_test = Dataset.from_dict({\"data\": X_test,'labels':y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "885ef8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6018.6875, 859.875, 1719.6875)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train) / 16, len(X_val) / 16, len(X_test) / 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8bd7a1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"InstaDeepAI/nucleotide-transformer-v2-50m-multi-species\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    outputs = tokenizer(examples[\"data\"])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4d77e330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2524701167c64e6ca753441970f7b910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/96299 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63fd1027a7b04b9688189bf81bc9ff61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13758 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484d8014e0434565a49d99f3708c9bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27515 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets_train_promoter = ds_train.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"data\"],\n",
    ")\n",
    "tokenized_datasets_validation_promoter = ds_val.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"data\"],\n",
    ")\n",
    "tokenized_datasets_test_promoter = ds_test.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"data\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "548419a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "model_name='nucleotide-transformer'\n",
    "args_promoter = TrainingArguments(\n",
    "    f\"models/{model_name}-50m\",\n",
    "    overwrite_output_dir=True,\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    learning_rate=5e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps= 4,\n",
    "    per_device_eval_batch_size= 64,\n",
    "    num_train_epochs= 2,\n",
    "    logging_steps=1000,\n",
    "    save_steps=1000,\n",
    "    load_best_model_at_end=True,  # Keep the best model according to the evaluation\n",
    "    metric_for_best_model=\"f1_score\",\n",
    "    label_names=[\"labels\"],\n",
    "    dataloader_drop_last=True,\n",
    "    max_steps= 5000,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=-1)\n",
    "    raw_out = torch.Tensor(eval_pred.predictions)\n",
    "    logits = F.softmax(raw_out, dim=-1).numpy()\n",
    "    n_classes = logits.shape[-1] if logits.ndim > 1 else 2\n",
    "    \n",
    "    labels = eval_pred.label_ids\n",
    "    ohe_labels = np.eye(n_classes, dtype=int)[labels]\n",
    "    \n",
    "    try:\n",
    "        r={\n",
    "            'f1_score': f1_score(labels, predictions, average=\"macro\"),\n",
    "            'accuracy': np.mean(labels == predictions),\n",
    "            'avg_auroc': roc_auc_score(labels, logits, multi_class='ovr', average='macro'),\n",
    "            'avg_auprc': average_precision_score(ohe_labels, logits)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(\"Error on the following:\")\n",
    "        print(f\"logits: {logits}, {type(logits)}\")\n",
    "        print(f\"predictions: {predictions}, {type(predictions)}\")\n",
    "        print(f\"labels: {labels}, {type(labels)}\")\n",
    "        raise ValueError(\"error produced from above vectors.\")\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a00f4605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:zg7t6bxr) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9ec2808d51429292e1214e7e29a9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▅▁███</td></tr><tr><td>eval/avg_auprc</td><td>▁▇▇██</td></tr><tr><td>eval/avg_auroc</td><td>▁▇▇██</td></tr><tr><td>eval/f1_score</td><td>▃█▂▁▃</td></tr><tr><td>eval/loss</td><td>█▃▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁█▇▅▅</td></tr><tr><td>eval/samples_per_second</td><td>█▁▂▄▄</td></tr><tr><td>eval/steps_per_second</td><td>█▁▂▃▄</td></tr><tr><td>train/epoch</td><td>▁▁▃▃▅▅▆▆███</td></tr><tr><td>train/global_step</td><td>▁▁▃▃▅▅▆▆███</td></tr><tr><td>train/learning_rate</td><td>█▆▅▃▁</td></tr><tr><td>train/loss</td><td>▁█▇▇▆</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.60295</td></tr><tr><td>eval/avg_auprc</td><td>0.43914</td></tr><tr><td>eval/avg_auroc</td><td>0.65147</td></tr><tr><td>eval/f1_score</td><td>0.33478</td></tr><tr><td>eval/loss</td><td>0.86242</td></tr><tr><td>eval/runtime</td><td>88.1912</td></tr><tr><td>eval/samples_per_second</td><td>156.002</td></tr><tr><td>eval/steps_per_second</td><td>2.438</td></tr><tr><td>train/epoch</td><td>0.42</td></tr><tr><td>train/global_step</td><td>5000</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.8508</td></tr><tr><td>train/total_flos</td><td>3558540704030160.0</td></tr><tr><td>train/train_loss</td><td>0.84057</td></tr><tr><td>train/train_runtime</td><td>835.7126</td></tr><tr><td>train/train_samples_per_second</td><td>47.863</td></tr><tr><td>train/train_steps_per_second</td><td>5.983</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">50m</strong> at: <a href='https://wandb.ai/pranavmahabs/bindgps_seq_nt-finetune/runs/zg7t6bxr' target=\"_blank\">https://wandb.ai/pranavmahabs/bindgps_seq_nt-finetune/runs/zg7t6bxr</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250829_193926-zg7t6bxr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:zg7t6bxr). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835f10dea19d419d8df266aba6418f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666843860099713, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.21.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/oscar/data/larschan/shared_data/bind_gps/seq_modeling/wandb/run-20250829_201439-ci5w6hmh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pranavmahabs/bindgps_seq_nt-finetune/runs/ci5w6hmh' target=\"_blank\">50m</a></strong> to <a href='https://wandb.ai/pranavmahabs/bindgps_seq_nt-finetune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pranavmahabs/bindgps_seq_nt-finetune' target=\"_blank\">https://wandb.ai/pranavmahabs/bindgps_seq_nt-finetune</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pranavmahabs/bindgps_seq_nt-finetune/runs/ci5w6hmh' target=\"_blank\">https://wandb.ai/pranavmahabs/bindgps_seq_nt-finetune/runs/ci5w6hmh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/pranavmahabs/bindgps_seq_nt-finetune/runs/ci5w6hmh?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f64cd97e670>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"bindgps_seq_nt-finetune\", name=\"50m\", reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a9ccf83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model.to(device),\n",
    "    args_promoter,\n",
    "    train_dataset= tokenized_datasets_train_promoter,\n",
    "    eval_dataset= tokenized_datasets_validation_promoter,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c121016d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/oscar/home/pmahable/my_env/lib64/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1001' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1001/5000 09:52 < 39:30, 1.69 it/s, Epoch 0.66/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='184' max='214' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [184/214 01:18 < 00:12, 2.34 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "103e7f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098e9b9fc13749dda52cd4230054fd0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>0.08</td></tr><tr><td>train/global_step</td><td>1000</td></tr><tr><td>train/learning_rate</td><td>0.0004</td></tr><tr><td>train/loss</td><td>0.9192</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">50m</strong> at: <a href='https://wandb.ai/pranavmahabs/bindgps_seq_nt-finetune/runs/yiyn1qsv' target=\"_blank\">https://wandb.ai/pranavmahabs/bindgps_seq_nt-finetune/runs/yiyn1qsv</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250829_185007-yiyn1qsv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7ccca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_evaluation_f1_score =[[a['step'],a['eval_f1_score']] for a in trainer.state.log_history if 'eval_f1_score' in a.keys()]\n",
    "eval_f1_score = [c[1] for c in curve_evaluation_f1_score]\n",
    "steps = [c[0] for c in curve_evaluation_f1_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98948855",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(steps, eval_f1_score, 'b', label='Validation F1 score')\n",
    "plt.title('Validation F1 score for promoter prediction')\n",
    "plt.xlabel('Number of training steps performed')\n",
    "plt.ylabel('Validation F1 score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac194efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"F1 score on the test dataset: {trainer.predict(tokenized_datasets_test_promoter).metrics['test_f1_score']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
