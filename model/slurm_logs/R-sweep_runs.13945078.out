## SLURM PROLOG ###############################################################
##    Job ID : 13945078
##  Job Name : sweep_runs
##  Nodelist : gpu2010
##      CPUs : 1
##  Mem/Node : 32000 MB
## Directory : /oscar/data/larschan/shared_data/BindGPS/model
##   Job Started : Mon Nov 10 01:24:36 EST 2025
###############################################################################
Running model parameter sweep with GAT (NO SVM)
Create sweep with ID: 03nm2u20
Sweep URL: https://wandb.ai/bind-gps/gps-gat-model-no-svm-parameter-test2/sweeps/03nm2u20
Created sweep: 03nm2u20
Starting 24 runs...
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=2.0309 | train_acc=0.6540 | val_loss=5.6448 | val_acc=0.1881
Epoch 001 | loss=1.2879 | train_acc=0.5380 | val_loss=1.0720 | val_acc=0.6183
Epoch 002 | loss=1.1525 | train_acc=0.5794 | val_loss=1.0475 | val_acc=0.6187
Epoch 003 | loss=1.0744 | train_acc=0.5919 | val_loss=1.0287 | val_acc=0.6187
Epoch 004 | loss=1.0352 | train_acc=0.5955 | val_loss=1.0189 | val_acc=0.6187
Epoch 005 | loss=1.0234 | train_acc=0.5977 | val_loss=1.0149 | val_acc=0.6187
Epoch 006 | loss=0.9842 | train_acc=0.6008 | val_loss=1.0121 | val_acc=0.6187
Epoch 007 | loss=0.9710 | train_acc=0.6018 | val_loss=1.0137 | val_acc=0.6187
Epoch 008 | loss=0.9635 | train_acc=0.6027 | val_loss=1.0163 | val_acc=0.6187
Epoch 009 | loss=0.9591 | train_acc=0.6035 | val_loss=1.0185 | val_acc=0.6187
Epoch 010 | loss=0.9589 | train_acc=0.6020 | val_loss=1.0200 | val_acc=0.6187
Epoch 011 | loss=0.9563 | train_acc=0.6022 | val_loss=1.0210 | val_acc=0.6187
Epoch 012 | loss=0.9566 | train_acc=0.6013 | val_loss=1.0215 | val_acc=0.6187
Epoch 013 | loss=0.9557 | train_acc=0.6016 | val_loss=1.0216 | val_acc=0.6187
Epoch 014 | loss=0.9545 | train_acc=0.6027 | val_loss=1.0215 | val_acc=0.6187
Epoch 015 | loss=0.9540 | train_acc=0.6022 | val_loss=1.0214 | val_acc=0.6187
Epoch 016 | loss=0.9536 | train_acc=0.6020 | val_loss=1.0211 | val_acc=0.6187
Epoch 017 | loss=0.9527 | train_acc=0.6027 | val_loss=1.0206 | val_acc=0.6187
Epoch 018 | loss=0.9527 | train_acc=0.6026 | val_loss=1.0202 | val_acc=0.6187
Epoch 019 | loss=0.9523 | train_acc=0.6017 | val_loss=1.0197 | val_acc=0.6187
Epoch 020 | loss=0.9516 | train_acc=0.6038 | val_loss=1.0198 | val_acc=0.6187
Epoch 021 | loss=0.9698 | train_acc=0.6014 | val_loss=1.0170 | val_acc=0.6187
Epoch 022 | loss=0.9411 | train_acc=0.6203 | val_loss=3.1038 | val_acc=0.2555
Epoch 023 | loss=1.0347 | train_acc=0.5946 | val_loss=1.6739 | val_acc=0.3428
Epoch 024 | loss=1.0409 | train_acc=0.5916 | val_loss=1.0066 | val_acc=0.6187
Epoch 025 | loss=1.0161 | train_acc=0.5992 | val_loss=0.9968 | val_acc=0.6179
Epoch 026 | loss=0.9745 | train_acc=0.6010 | val_loss=0.9994 | val_acc=0.6187
Epoch 027 | loss=1.0991 | train_acc=0.6014 | val_loss=0.9554 | val_acc=0.6101
Epoch 028 | loss=0.9794 | train_acc=0.5949 | val_loss=0.9944 | val_acc=0.5672
Epoch 029 | loss=0.9857 | train_acc=0.5993 | val_loss=0.9974 | val_acc=0.6187
Epoch 030 | loss=0.9710 | train_acc=0.6014 | val_loss=1.0024 | val_acc=0.6076
Epoch 031 | loss=0.9603 | train_acc=0.6013 | val_loss=1.0000 | val_acc=0.6187
Epoch 032 | loss=0.9483 | train_acc=0.6022 | val_loss=0.9849 | val_acc=0.6187
Epoch 033 | loss=0.9393 | train_acc=0.6028 | val_loss=1.0002 | val_acc=0.6187
Epoch 034 | loss=0.9427 | train_acc=0.6012 | val_loss=0.9976 | val_acc=0.6187
Epoch 035 | loss=0.9369 | train_acc=0.6031 | val_loss=0.9889 | val_acc=0.6187
Epoch 036 | loss=0.9361 | train_acc=0.6021 | val_loss=0.9909 | val_acc=0.6187
Epoch 037 | loss=0.9333 | train_acc=0.6027 | val_loss=0.9821 | val_acc=0.6187
Epoch 038 | loss=0.9305 | train_acc=0.6035 | val_loss=0.9761 | val_acc=0.6187
Epoch 039 | loss=0.9313 | train_acc=0.6020 | val_loss=0.9676 | val_acc=0.6187
Epoch 040 | loss=0.9299 | train_acc=0.6022 | val_loss=0.9648 | val_acc=0.6187
Epoch 041 | loss=0.9325 | train_acc=0.6022 | val_loss=0.9758 | val_acc=0.6187
Epoch 042 | loss=0.9333 | train_acc=0.6031 | val_loss=0.9742 | val_acc=0.6187
Epoch 043 | loss=0.9318 | train_acc=0.6026 | val_loss=0.9907 | val_acc=0.6187
Epoch 044 | loss=0.9346 | train_acc=0.6026 | val_loss=0.9958 | val_acc=0.6187
Epoch 045 | loss=0.9518 | train_acc=0.6282 | val_loss=1.6272 | val_acc=0.4506
Epoch 046 | loss=0.9541 | train_acc=0.6586 | val_loss=2.0494 | val_acc=0.4058
Epoch 047 | loss=0.9889 | train_acc=0.5945 | val_loss=1.4556 | val_acc=0.4295
Epoch 048 | loss=1.0018 | train_acc=0.5949 | val_loss=1.0076 | val_acc=0.6187
Epoch 049 | loss=0.9639 | train_acc=0.5986 | val_loss=1.0122 | val_acc=0.6179
Final Test Loss: 0.9564 | Test Accuracy: 0.6318
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=2.3183 | train_acc=0.6604 | val_loss=7.7559 | val_acc=0.1866
Epoch 001 | loss=1.6262 | train_acc=0.6031 | val_loss=3.7367 | val_acc=0.2010
Epoch 002 | loss=1.2028 | train_acc=0.5536 | val_loss=1.0650 | val_acc=0.6179
Epoch 003 | loss=1.1094 | train_acc=0.5921 | val_loss=1.0529 | val_acc=0.6175
Epoch 004 | loss=1.1288 | train_acc=0.5915 | val_loss=1.0499 | val_acc=0.6194
Epoch 005 | loss=1.1011 | train_acc=0.5626 | val_loss=1.0406 | val_acc=0.6187
Epoch 006 | loss=1.1328 | train_acc=0.6011 | val_loss=1.1579 | val_acc=0.4476
Epoch 007 | loss=1.0961 | train_acc=0.5810 | val_loss=1.0294 | val_acc=0.6153
Epoch 008 | loss=1.0690 | train_acc=0.5841 | val_loss=1.0249 | val_acc=0.6187
Epoch 009 | loss=1.0080 | train_acc=0.6004 | val_loss=1.0163 | val_acc=0.6187
Epoch 010 | loss=0.9855 | train_acc=0.6078 | val_loss=1.0125 | val_acc=0.6187
Epoch 011 | loss=0.9720 | train_acc=0.6082 | val_loss=1.0126 | val_acc=0.6187
Epoch 012 | loss=0.9643 | train_acc=0.6077 | val_loss=1.0144 | val_acc=0.6187
Epoch 013 | loss=0.9597 | train_acc=0.6080 | val_loss=1.0165 | val_acc=0.6187
Epoch 014 | loss=0.9557 | train_acc=0.6084 | val_loss=1.0185 | val_acc=0.6187
Epoch 015 | loss=0.9539 | train_acc=0.6080 | val_loss=1.0200 | val_acc=0.6187
Epoch 016 | loss=0.9570 | train_acc=0.6081 | val_loss=1.0201 | val_acc=0.6187
Epoch 017 | loss=0.9527 | train_acc=0.6085 | val_loss=1.0205 | val_acc=0.6187
Epoch 018 | loss=0.9514 | train_acc=0.6080 | val_loss=1.0202 | val_acc=0.6187
Epoch 019 | loss=0.9488 | train_acc=0.6084 | val_loss=1.0214 | val_acc=0.6187
Epoch 020 | loss=0.9497 | train_acc=0.6081 | val_loss=1.0199 | val_acc=0.6187
Epoch 021 | loss=0.9465 | train_acc=0.6081 | val_loss=1.0222 | val_acc=0.6187
Epoch 022 | loss=0.9477 | train_acc=0.6084 | val_loss=1.0210 | val_acc=0.6187
Epoch 023 | loss=0.9415 | train_acc=0.6088 | val_loss=0.9873 | val_acc=0.6187
Epoch 024 | loss=1.0775 | train_acc=0.6483 | val_loss=1.9078 | val_acc=0.4110
Epoch 025 | loss=0.9532 | train_acc=0.6431 | val_loss=2.2495 | val_acc=0.3310
Epoch 026 | loss=1.0577 | train_acc=0.6012 | val_loss=1.2214 | val_acc=0.4661
Epoch 027 | loss=0.9997 | train_acc=0.5926 | val_loss=0.9732 | val_acc=0.5916
Epoch 028 | loss=1.0521 | train_acc=0.6082 | val_loss=1.0491 | val_acc=0.4654
Epoch 029 | loss=1.0645 | train_acc=0.6008 | val_loss=0.9994 | val_acc=0.6187
Epoch 030 | loss=1.0025 | train_acc=0.6056 | val_loss=1.0058 | val_acc=0.6172
Epoch 031 | loss=0.9812 | train_acc=0.6055 | val_loss=0.9953 | val_acc=0.6187
Epoch 032 | loss=0.9521 | train_acc=0.6074 | val_loss=0.9906 | val_acc=0.6187
Epoch 033 | loss=0.9425 | train_acc=0.6084 | val_loss=0.9696 | val_acc=0.6187
Epoch 034 | loss=0.9278 | train_acc=0.6084 | val_loss=0.9744 | val_acc=0.6187
Epoch 035 | loss=0.9244 | train_acc=0.6084 | val_loss=0.9573 | val_acc=0.6187
Epoch 036 | loss=0.9170 | train_acc=0.6084 | val_loss=0.9629 | val_acc=0.6187
Epoch 037 | loss=0.9167 | train_acc=0.6081 | val_loss=0.9536 | val_acc=0.6187
Epoch 038 | loss=0.9140 | train_acc=0.6083 | val_loss=0.9485 | val_acc=0.6187
Epoch 039 | loss=0.9118 | train_acc=0.6081 | val_loss=0.9596 | val_acc=0.6187
Epoch 040 | loss=0.9120 | train_acc=0.6084 | val_loss=0.9393 | val_acc=0.6187
Epoch 041 | loss=0.9066 | train_acc=0.6085 | val_loss=0.9334 | val_acc=0.6187
Epoch 042 | loss=0.9037 | train_acc=0.6081 | val_loss=0.9271 | val_acc=0.6187
Epoch 043 | loss=0.9062 | train_acc=0.6083 | val_loss=0.9207 | val_acc=0.6187
Epoch 044 | loss=0.9037 | train_acc=0.6085 | val_loss=0.9239 | val_acc=0.6187
Epoch 045 | loss=0.9036 | train_acc=0.6088 | val_loss=0.9098 | val_acc=0.6187
Epoch 046 | loss=0.9020 | train_acc=0.6078 | val_loss=0.9321 | val_acc=0.6187
Epoch 047 | loss=0.9173 | train_acc=0.6081 | val_loss=0.9731 | val_acc=0.6187
Epoch 048 | loss=1.0163 | train_acc=0.6075 | val_loss=0.9984 | val_acc=0.6187
Epoch 049 | loss=0.9940 | train_acc=0.6270 | val_loss=1.0168 | val_acc=0.6312
Final Test Loss: 1.6635 | Test Accuracy: 0.5274
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.9474 | train_acc=0.6627 | val_loss=4.2772 | val_acc=0.1866
Epoch 001 | loss=1.2558 | train_acc=0.5713 | val_loss=1.2026 | val_acc=0.2203
Epoch 002 | loss=1.1373 | train_acc=0.5681 | val_loss=1.0541 | val_acc=0.6187
Epoch 003 | loss=1.0402 | train_acc=0.5966 | val_loss=1.0480 | val_acc=0.6183
Epoch 004 | loss=1.0979 | train_acc=0.5927 | val_loss=1.0723 | val_acc=0.4602
Epoch 005 | loss=1.0593 | train_acc=0.5969 | val_loss=1.0265 | val_acc=0.6187
Epoch 006 | loss=1.0026 | train_acc=0.6064 | val_loss=1.0156 | val_acc=0.6187
Epoch 007 | loss=0.9801 | train_acc=0.6090 | val_loss=1.0117 | val_acc=0.6187
Epoch 008 | loss=0.9644 | train_acc=0.6095 | val_loss=1.0118 | val_acc=0.6187
Epoch 009 | loss=0.9596 | train_acc=0.6095 | val_loss=1.0135 | val_acc=0.6187
Epoch 010 | loss=0.9516 | train_acc=0.6093 | val_loss=1.0151 | val_acc=0.6187
Epoch 011 | loss=0.9489 | train_acc=0.6091 | val_loss=1.0166 | val_acc=0.6187
Epoch 012 | loss=0.9453 | train_acc=0.6093 | val_loss=1.0178 | val_acc=0.6187
Epoch 013 | loss=0.9420 | train_acc=0.6096 | val_loss=1.0194 | val_acc=0.6187
Epoch 014 | loss=0.9405 | train_acc=0.6094 | val_loss=1.0189 | val_acc=0.6187
Epoch 015 | loss=0.9397 | train_acc=0.6093 | val_loss=1.0203 | val_acc=0.6187
Epoch 016 | loss=0.9370 | train_acc=0.6098 | val_loss=1.0201 | val_acc=0.6187
Epoch 017 | loss=0.9378 | train_acc=0.6094 | val_loss=1.0210 | val_acc=0.6187
Epoch 018 | loss=0.9323 | train_acc=0.6094 | val_loss=1.0213 | val_acc=0.6175
Epoch 019 | loss=0.9378 | train_acc=0.6089 | val_loss=1.0205 | val_acc=0.6187
Epoch 020 | loss=0.9438 | train_acc=0.6266 | val_loss=1.8676 | val_acc=0.3495
Epoch 021 | loss=0.9280 | train_acc=0.6573 | val_loss=1.2215 | val_acc=0.4780
Epoch 022 | loss=0.9058 | train_acc=0.6598 | val_loss=1.2298 | val_acc=0.4613
Epoch 023 | loss=0.9607 | train_acc=0.5965 | val_loss=1.0144 | val_acc=0.6105
Epoch 024 | loss=0.9956 | train_acc=0.6072 | val_loss=0.9975 | val_acc=0.6187
Epoch 025 | loss=0.9917 | train_acc=0.6237 | val_loss=1.1884 | val_acc=0.4813
Epoch 026 | loss=1.0294 | train_acc=0.5944 | val_loss=0.9643 | val_acc=0.6179
Epoch 027 | loss=0.9566 | train_acc=0.6049 | val_loss=1.0035 | val_acc=0.6187
Epoch 028 | loss=0.9505 | train_acc=0.6084 | val_loss=0.9438 | val_acc=0.6187
Epoch 029 | loss=0.8958 | train_acc=0.6086 | val_loss=0.9250 | val_acc=0.6187
Epoch 030 | loss=0.8857 | train_acc=0.6100 | val_loss=0.9239 | val_acc=0.5824
Epoch 031 | loss=0.9258 | train_acc=0.6124 | val_loss=0.8708 | val_acc=0.6194
Epoch 032 | loss=0.8778 | train_acc=0.6044 | val_loss=0.9586 | val_acc=0.6187
Epoch 033 | loss=0.8918 | train_acc=0.6088 | val_loss=0.9128 | val_acc=0.6187
Epoch 034 | loss=0.8689 | train_acc=0.6096 | val_loss=0.9300 | val_acc=0.6187
Epoch 035 | loss=0.8743 | train_acc=0.6091 | val_loss=0.9030 | val_acc=0.6187
Epoch 036 | loss=0.8608 | train_acc=0.6094 | val_loss=0.9129 | val_acc=0.6187
Epoch 037 | loss=0.8682 | train_acc=0.6091 | val_loss=0.8897 | val_acc=0.6187
Epoch 038 | loss=0.8579 | train_acc=0.6095 | val_loss=0.9182 | val_acc=0.6187
Epoch 039 | loss=0.8659 | train_acc=0.6094 | val_loss=0.9102 | val_acc=0.6187
Epoch 040 | loss=0.8866 | train_acc=0.6091 | val_loss=0.8617 | val_acc=0.6187
Epoch 041 | loss=0.8491 | train_acc=0.6086 | val_loss=0.9212 | val_acc=0.6187
Epoch 042 | loss=0.8746 | train_acc=0.6089 | val_loss=0.8585 | val_acc=0.6187
Epoch 043 | loss=0.8485 | train_acc=0.6097 | val_loss=0.8636 | val_acc=0.6187
Epoch 044 | loss=0.8549 | train_acc=0.6099 | val_loss=0.8672 | val_acc=0.6187
Epoch 045 | loss=0.8606 | train_acc=0.6086 | val_loss=0.8487 | val_acc=0.6187
Epoch 046 | loss=0.8430 | train_acc=0.6095 | val_loss=0.8613 | val_acc=0.6187
Epoch 047 | loss=0.8620 | train_acc=0.6093 | val_loss=0.8348 | val_acc=0.6183
Epoch 048 | loss=0.8476 | train_acc=0.6084 | val_loss=0.8689 | val_acc=0.6187
Epoch 049 | loss=0.8545 | train_acc=0.6090 | val_loss=0.9300 | val_acc=0.6187
Final Test Loss: 0.8741 | Test Accuracy: 0.6322
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=2.4987 | train_acc=0.6573 | val_loss=3.6389 | val_acc=0.1877
Epoch 001 | loss=1.3338 | train_acc=0.6093 | val_loss=1.8856 | val_acc=0.1973
Epoch 002 | loss=1.1482 | train_acc=0.5817 | val_loss=1.0977 | val_acc=0.4161
Epoch 003 | loss=1.0837 | train_acc=0.5863 | val_loss=1.0519 | val_acc=0.6194
Epoch 004 | loss=1.0614 | train_acc=0.5830 | val_loss=1.0377 | val_acc=0.6187
Epoch 005 | loss=1.0346 | train_acc=0.6027 | val_loss=1.0223 | val_acc=0.6187
Epoch 006 | loss=1.0027 | train_acc=0.6065 | val_loss=1.0127 | val_acc=0.6187
Epoch 007 | loss=0.9758 | train_acc=0.6091 | val_loss=1.0054 | val_acc=0.6187
Epoch 008 | loss=0.9728 | train_acc=0.6075 | val_loss=1.0029 | val_acc=0.6187
Epoch 009 | loss=0.9481 | train_acc=0.6091 | val_loss=0.9770 | val_acc=0.6187
Epoch 010 | loss=0.9253 | train_acc=0.6084 | val_loss=0.9708 | val_acc=0.6164
Epoch 011 | loss=0.9653 | train_acc=0.6032 | val_loss=0.9578 | val_acc=0.6127
Epoch 012 | loss=0.9881 | train_acc=0.6050 | val_loss=0.9485 | val_acc=0.6046
Epoch 013 | loss=0.9155 | train_acc=0.6004 | val_loss=0.9719 | val_acc=0.6009
Epoch 014 | loss=0.9171 | train_acc=0.6013 | val_loss=0.9232 | val_acc=0.6187
Epoch 015 | loss=0.8632 | train_acc=0.6086 | val_loss=0.9318 | val_acc=0.6179
Epoch 016 | loss=0.8621 | train_acc=0.6090 | val_loss=0.9353 | val_acc=0.6183
Epoch 017 | loss=0.8540 | train_acc=0.6094 | val_loss=0.9508 | val_acc=0.6157
Epoch 018 | loss=0.8640 | train_acc=0.6092 | val_loss=0.9208 | val_acc=0.6187
Epoch 019 | loss=0.8435 | train_acc=0.6095 | val_loss=0.9396 | val_acc=0.6146
Epoch 020 | loss=0.8610 | train_acc=0.6092 | val_loss=0.9015 | val_acc=0.6187
Epoch 021 | loss=0.8370 | train_acc=0.6097 | val_loss=0.9090 | val_acc=0.6153
Epoch 022 | loss=0.8337 | train_acc=0.6096 | val_loss=0.8981 | val_acc=0.6187
Epoch 023 | loss=0.8246 | train_acc=0.6095 | val_loss=0.8999 | val_acc=0.6168
Epoch 024 | loss=0.8268 | train_acc=0.6097 | val_loss=0.9262 | val_acc=0.6187
Epoch 025 | loss=0.8330 | train_acc=0.6092 | val_loss=0.8949 | val_acc=0.6187
Epoch 026 | loss=0.8126 | train_acc=0.6091 | val_loss=0.8896 | val_acc=0.6190
Epoch 027 | loss=0.8035 | train_acc=0.6091 | val_loss=0.8946 | val_acc=0.6187
Epoch 028 | loss=0.8026 | train_acc=0.6091 | val_loss=0.8777 | val_acc=0.6187
Epoch 029 | loss=0.7983 | train_acc=0.6094 | val_loss=0.8759 | val_acc=0.6187
Epoch 030 | loss=0.7873 | train_acc=0.6090 | val_loss=0.9253 | val_acc=0.6187
Epoch 031 | loss=0.7964 | train_acc=0.6092 | val_loss=0.9416 | val_acc=0.6187
Epoch 032 | loss=0.7819 | train_acc=0.6089 | val_loss=0.8507 | val_acc=0.6187
Epoch 033 | loss=0.7699 | train_acc=0.6090 | val_loss=0.8274 | val_acc=0.6187
Epoch 034 | loss=0.7595 | train_acc=0.6095 | val_loss=0.9080 | val_acc=0.6187
Epoch 035 | loss=0.7654 | train_acc=0.6088 | val_loss=0.8130 | val_acc=0.6187
Epoch 036 | loss=0.7518 | train_acc=0.6097 | val_loss=0.8759 | val_acc=0.6187
Epoch 037 | loss=0.7573 | train_acc=0.6089 | val_loss=0.8012 | val_acc=0.6187
Epoch 038 | loss=0.7479 | train_acc=0.6098 | val_loss=0.8361 | val_acc=0.6187
Epoch 039 | loss=0.7459 | train_acc=0.6096 | val_loss=0.8334 | val_acc=0.6187
Epoch 040 | loss=0.7533 | train_acc=0.6097 | val_loss=0.8635 | val_acc=0.6187
Epoch 041 | loss=0.7548 | train_acc=0.6092 | val_loss=0.8252 | val_acc=0.6187
Epoch 042 | loss=0.7488 | train_acc=0.6101 | val_loss=0.7988 | val_acc=0.6187
Epoch 043 | loss=0.7463 | train_acc=0.6227 | val_loss=0.8052 | val_acc=0.6964
Epoch 044 | loss=0.7336 | train_acc=0.6825 | val_loss=0.8742 | val_acc=0.5350
Epoch 045 | loss=0.7365 | train_acc=0.6830 | val_loss=0.8214 | val_acc=0.6783
Epoch 046 | loss=0.7397 | train_acc=0.6816 | val_loss=0.8052 | val_acc=0.6853
Epoch 047 | loss=0.7387 | train_acc=0.6787 | val_loss=0.7865 | val_acc=0.7105
Epoch 048 | loss=0.7252 | train_acc=0.6841 | val_loss=1.1835 | val_acc=0.5391
Epoch 049 | loss=0.8066 | train_acc=0.7166 | val_loss=0.9846 | val_acc=0.3862
Final Test Loss: 0.9985 | Test Accuracy: 0.3405
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=3.3997 | train_acc=0.6473 | val_loss=7.2569 | val_acc=0.1910
Epoch 001 | loss=2.0078 | train_acc=0.5325 | val_loss=1.3185 | val_acc=0.2173
Epoch 002 | loss=1.2269 | train_acc=0.5499 | val_loss=1.0549 | val_acc=0.6187
Epoch 003 | loss=1.1935 | train_acc=0.5811 | val_loss=1.0449 | val_acc=0.6187
Epoch 004 | loss=1.0996 | train_acc=0.5830 | val_loss=1.0377 | val_acc=0.6187
Epoch 005 | loss=1.0598 | train_acc=0.5890 | val_loss=1.0316 | val_acc=0.6187
Epoch 006 | loss=1.0160 | train_acc=0.5954 | val_loss=1.0256 | val_acc=0.6187
Epoch 007 | loss=1.0002 | train_acc=0.5989 | val_loss=1.0204 | val_acc=0.6187
Epoch 008 | loss=0.9905 | train_acc=0.6011 | val_loss=1.0167 | val_acc=0.6187
Epoch 009 | loss=0.9831 | train_acc=0.6018 | val_loss=1.0139 | val_acc=0.6187
Epoch 010 | loss=0.9772 | train_acc=0.6021 | val_loss=1.0121 | val_acc=0.6187
Epoch 011 | loss=0.9722 | train_acc=0.6000 | val_loss=1.0105 | val_acc=0.6187
Epoch 012 | loss=0.9679 | train_acc=0.6015 | val_loss=1.0093 | val_acc=0.6187
Epoch 013 | loss=0.9638 | train_acc=0.6014 | val_loss=1.0082 | val_acc=0.6187
Epoch 014 | loss=0.9601 | train_acc=0.6016 | val_loss=1.0070 | val_acc=0.6187
Epoch 015 | loss=0.9579 | train_acc=0.6024 | val_loss=1.0070 | val_acc=0.6187
Epoch 016 | loss=0.9541 | train_acc=0.6011 | val_loss=1.0063 | val_acc=0.6187
Epoch 017 | loss=0.9503 | train_acc=0.6021 | val_loss=1.0056 | val_acc=0.6187
Epoch 018 | loss=0.9498 | train_acc=0.6029 | val_loss=1.0060 | val_acc=0.6187
Epoch 019 | loss=0.9478 | train_acc=0.6017 | val_loss=1.0048 | val_acc=0.6187
Epoch 020 | loss=0.9463 | train_acc=0.6021 | val_loss=1.0057 | val_acc=0.6187
Epoch 021 | loss=0.9440 | train_acc=0.6022 | val_loss=1.0062 | val_acc=0.6187
Epoch 022 | loss=0.9432 | train_acc=0.6022 | val_loss=1.0065 | val_acc=0.6187
Epoch 023 | loss=0.9414 | train_acc=0.6012 | val_loss=1.0052 | val_acc=0.6187
Epoch 024 | loss=0.9414 | train_acc=0.6020 | val_loss=1.0061 | val_acc=0.6187
Epoch 025 | loss=0.9382 | train_acc=0.6021 | val_loss=1.0051 | val_acc=0.6187
Epoch 026 | loss=0.9394 | train_acc=0.6027 | val_loss=1.0058 | val_acc=0.6187
Epoch 027 | loss=0.9373 | train_acc=0.6019 | val_loss=1.0058 | val_acc=0.6187
Epoch 028 | loss=0.9366 | train_acc=0.6026 | val_loss=1.0066 | val_acc=0.6187
Epoch 029 | loss=0.9372 | train_acc=0.6026 | val_loss=1.0058 | val_acc=0.6187
Epoch 030 | loss=0.9387 | train_acc=0.6028 | val_loss=1.0097 | val_acc=0.6187
Epoch 031 | loss=0.9366 | train_acc=0.6029 | val_loss=1.0104 | val_acc=0.6187
Epoch 032 | loss=0.9361 | train_acc=0.6018 | val_loss=1.0095 | val_acc=0.6187
Epoch 033 | loss=0.9361 | train_acc=0.6037 | val_loss=1.0104 | val_acc=0.6187
Epoch 034 | loss=0.9349 | train_acc=0.6020 | val_loss=1.0099 | val_acc=0.6187
Epoch 035 | loss=0.9364 | train_acc=0.6031 | val_loss=1.0110 | val_acc=0.6187
Epoch 036 | loss=0.9333 | train_acc=0.6036 | val_loss=1.0101 | val_acc=0.6187
Epoch 037 | loss=0.9326 | train_acc=0.6025 | val_loss=1.0106 | val_acc=0.6187
Epoch 038 | loss=0.9335 | train_acc=0.6017 | val_loss=1.0098 | val_acc=0.6187
Epoch 039 | loss=1.0176 | train_acc=0.6025 | val_loss=1.0132 | val_acc=0.6187
Epoch 040 | loss=0.9443 | train_acc=0.6019 | val_loss=1.0142 | val_acc=0.6187
Epoch 041 | loss=0.9617 | train_acc=0.6011 | val_loss=1.0143 | val_acc=0.6187
Epoch 042 | loss=1.0689 | train_acc=0.5998 | val_loss=1.0155 | val_acc=0.6187
Epoch 043 | loss=0.9390 | train_acc=0.6031 | val_loss=1.0161 | val_acc=0.6187
Epoch 044 | loss=0.9361 | train_acc=0.6035 | val_loss=1.0165 | val_acc=0.6187
Epoch 045 | loss=0.9366 | train_acc=0.6027 | val_loss=1.0167 | val_acc=0.6187
Epoch 046 | loss=0.9347 | train_acc=0.6032 | val_loss=1.0174 | val_acc=0.6187
Epoch 047 | loss=0.9347 | train_acc=0.6032 | val_loss=1.0183 | val_acc=0.6187
Epoch 048 | loss=0.9349 | train_acc=0.6029 | val_loss=1.0189 | val_acc=0.6187
Epoch 049 | loss=0.9351 | train_acc=0.6022 | val_loss=1.0194 | val_acc=0.6187
Final Test Loss: 0.9159 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=3.9901 | train_acc=0.6586 | val_loss=5.9918 | val_acc=0.1922
Epoch 001 | loss=2.5781 | train_acc=0.5322 | val_loss=1.0777 | val_acc=0.4380
Epoch 002 | loss=1.1367 | train_acc=0.5616 | val_loss=1.0503 | val_acc=0.6187
Epoch 003 | loss=1.0680 | train_acc=0.5885 | val_loss=1.0400 | val_acc=0.6187
Epoch 004 | loss=1.0420 | train_acc=0.5985 | val_loss=1.0321 | val_acc=0.6187
Epoch 005 | loss=1.0154 | train_acc=0.6031 | val_loss=1.0256 | val_acc=0.6187
Epoch 006 | loss=0.9991 | train_acc=0.6049 | val_loss=1.0205 | val_acc=0.6187
Epoch 007 | loss=0.9898 | train_acc=0.6068 | val_loss=1.0167 | val_acc=0.6187
Epoch 008 | loss=0.9829 | train_acc=0.6073 | val_loss=1.0141 | val_acc=0.6187
Epoch 009 | loss=0.9756 | train_acc=0.6074 | val_loss=1.0118 | val_acc=0.6187
Epoch 010 | loss=0.9709 | train_acc=0.6071 | val_loss=1.0103 | val_acc=0.6187
Epoch 011 | loss=0.9662 | train_acc=0.6076 | val_loss=1.0095 | val_acc=0.6187
Epoch 012 | loss=0.9624 | train_acc=0.6078 | val_loss=1.0089 | val_acc=0.6187
Epoch 013 | loss=0.9598 | train_acc=0.6078 | val_loss=1.0085 | val_acc=0.6187
Epoch 014 | loss=0.9567 | train_acc=0.6081 | val_loss=1.0084 | val_acc=0.6187
Epoch 015 | loss=0.9538 | train_acc=0.6079 | val_loss=1.0083 | val_acc=0.6187
Epoch 016 | loss=0.9521 | train_acc=0.6078 | val_loss=1.0083 | val_acc=0.6187
Epoch 017 | loss=0.9499 | train_acc=0.6079 | val_loss=1.0085 | val_acc=0.6187
Epoch 018 | loss=0.9475 | train_acc=0.6084 | val_loss=1.0087 | val_acc=0.6187
Epoch 019 | loss=0.9458 | train_acc=0.6080 | val_loss=1.0090 | val_acc=0.6187
Epoch 020 | loss=0.9451 | train_acc=0.6080 | val_loss=1.0094 | val_acc=0.6187
Epoch 021 | loss=0.9434 | train_acc=0.6082 | val_loss=1.0098 | val_acc=0.6187
Epoch 022 | loss=0.9433 | train_acc=0.6077 | val_loss=1.0102 | val_acc=0.6187
Epoch 023 | loss=0.9415 | train_acc=0.6084 | val_loss=1.0107 | val_acc=0.6187
Epoch 024 | loss=0.9416 | train_acc=0.6083 | val_loss=1.0111 | val_acc=0.6187
Epoch 025 | loss=0.9401 | train_acc=0.6082 | val_loss=1.0116 | val_acc=0.6187
Epoch 026 | loss=0.9397 | train_acc=0.6083 | val_loss=1.0122 | val_acc=0.6187
Epoch 027 | loss=0.9387 | train_acc=0.6082 | val_loss=1.0126 | val_acc=0.6187
Epoch 028 | loss=0.9392 | train_acc=0.6074 | val_loss=1.0130 | val_acc=0.6187
Epoch 029 | loss=0.9380 | train_acc=0.6076 | val_loss=1.0134 | val_acc=0.6187
Epoch 030 | loss=0.9374 | train_acc=0.6085 | val_loss=1.0138 | val_acc=0.6187
Epoch 031 | loss=0.9374 | train_acc=0.6079 | val_loss=1.0141 | val_acc=0.6187
Epoch 032 | loss=0.9369 | train_acc=0.6082 | val_loss=1.0145 | val_acc=0.6187
Epoch 033 | loss=0.9360 | train_acc=0.6080 | val_loss=1.0149 | val_acc=0.6187
Epoch 034 | loss=0.9364 | train_acc=0.6082 | val_loss=1.0150 | val_acc=0.6187
Epoch 035 | loss=0.9356 | train_acc=0.6083 | val_loss=1.0155 | val_acc=0.6187
Epoch 036 | loss=0.9357 | train_acc=0.6082 | val_loss=1.0160 | val_acc=0.6187
Epoch 037 | loss=0.9355 | train_acc=0.6078 | val_loss=1.0163 | val_acc=0.6187
Epoch 038 | loss=0.9354 | train_acc=0.6088 | val_loss=1.0166 | val_acc=0.6187
Epoch 039 | loss=0.9346 | train_acc=0.6081 | val_loss=1.0169 | val_acc=0.6187
Epoch 040 | loss=0.9352 | train_acc=0.6084 | val_loss=1.0171 | val_acc=0.6187
Epoch 041 | loss=0.9337 | train_acc=0.6080 | val_loss=1.0174 | val_acc=0.6187
Epoch 042 | loss=0.9352 | train_acc=0.6079 | val_loss=1.0177 | val_acc=0.6187
Epoch 043 | loss=0.9339 | train_acc=0.6082 | val_loss=1.0188 | val_acc=0.6187
Epoch 044 | loss=0.9340 | train_acc=0.6081 | val_loss=1.0193 | val_acc=0.6187
Epoch 045 | loss=0.9340 | train_acc=0.6087 | val_loss=1.0196 | val_acc=0.6187
Epoch 046 | loss=0.9336 | train_acc=0.6083 | val_loss=1.0203 | val_acc=0.6187
Epoch 047 | loss=0.9351 | train_acc=0.6080 | val_loss=1.0198 | val_acc=0.6187
Epoch 048 | loss=0.9338 | train_acc=0.6076 | val_loss=1.0200 | val_acc=0.6187
Epoch 049 | loss=0.9335 | train_acc=0.6083 | val_loss=1.0206 | val_acc=0.6187
Final Test Loss: 0.9148 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=4.1625 | train_acc=0.6556 | val_loss=6.6784 | val_acc=0.1907
Epoch 001 | loss=2.0923 | train_acc=0.5492 | val_loss=1.4703 | val_acc=0.2147
Epoch 002 | loss=1.2332 | train_acc=0.5391 | val_loss=1.0628 | val_acc=0.6187
Epoch 003 | loss=1.0769 | train_acc=0.5794 | val_loss=1.0489 | val_acc=0.6187
Epoch 004 | loss=1.0549 | train_acc=0.5956 | val_loss=1.0386 | val_acc=0.6187
Epoch 005 | loss=1.0227 | train_acc=0.5989 | val_loss=1.0323 | val_acc=0.6187
Epoch 006 | loss=1.0061 | train_acc=0.6052 | val_loss=1.0262 | val_acc=0.6187
Epoch 007 | loss=0.9979 | train_acc=0.6072 | val_loss=1.0214 | val_acc=0.6187
Epoch 008 | loss=0.9864 | train_acc=0.6081 | val_loss=1.0177 | val_acc=0.6187
Epoch 009 | loss=0.9812 | train_acc=0.6085 | val_loss=1.0142 | val_acc=0.6187
Epoch 010 | loss=0.9751 | train_acc=0.6088 | val_loss=1.0112 | val_acc=0.6187
Epoch 011 | loss=0.9697 | train_acc=0.6090 | val_loss=1.0084 | val_acc=0.6187
Epoch 012 | loss=0.9643 | train_acc=0.6090 | val_loss=1.0057 | val_acc=0.6187
Epoch 013 | loss=0.9608 | train_acc=0.6093 | val_loss=1.0042 | val_acc=0.6187
Epoch 014 | loss=0.9559 | train_acc=0.6095 | val_loss=1.0027 | val_acc=0.6187
Epoch 015 | loss=0.9523 | train_acc=0.6092 | val_loss=1.0003 | val_acc=0.6187
Epoch 016 | loss=0.9467 | train_acc=0.6090 | val_loss=0.9988 | val_acc=0.6187
Epoch 017 | loss=0.9441 | train_acc=0.6097 | val_loss=0.9971 | val_acc=0.6187
Epoch 018 | loss=0.9379 | train_acc=0.6097 | val_loss=0.9958 | val_acc=0.6187
Epoch 019 | loss=0.9363 | train_acc=0.6098 | val_loss=0.9935 | val_acc=0.6187
Epoch 020 | loss=0.9329 | train_acc=0.6097 | val_loss=0.9936 | val_acc=0.6187
Epoch 021 | loss=0.9298 | train_acc=0.6096 | val_loss=0.9900 | val_acc=0.6187
Epoch 022 | loss=0.9236 | train_acc=0.6098 | val_loss=0.9925 | val_acc=0.6187
Epoch 023 | loss=0.9287 | train_acc=0.6096 | val_loss=0.9913 | val_acc=0.6187
Epoch 024 | loss=0.9218 | train_acc=0.6097 | val_loss=0.9949 | val_acc=0.6187
Epoch 025 | loss=0.9251 | train_acc=0.6094 | val_loss=0.9893 | val_acc=0.6187
Epoch 026 | loss=0.9148 | train_acc=0.6098 | val_loss=0.9922 | val_acc=0.6187
Epoch 027 | loss=0.9214 | train_acc=0.6097 | val_loss=0.9881 | val_acc=0.6187
Epoch 028 | loss=0.9128 | train_acc=0.6100 | val_loss=0.9883 | val_acc=0.6187
Epoch 029 | loss=0.9172 | train_acc=0.6095 | val_loss=0.9884 | val_acc=0.6187
Epoch 030 | loss=0.9102 | train_acc=0.6098 | val_loss=0.9902 | val_acc=0.6187
Epoch 031 | loss=0.9103 | train_acc=0.6091 | val_loss=0.9929 | val_acc=0.6187
Epoch 032 | loss=0.9118 | train_acc=0.6094 | val_loss=0.9995 | val_acc=0.6187
Epoch 033 | loss=0.9031 | train_acc=0.6094 | val_loss=1.0006 | val_acc=0.6187
Epoch 034 | loss=0.9124 | train_acc=0.6096 | val_loss=0.9967 | val_acc=0.6187
Epoch 035 | loss=0.9045 | train_acc=0.6096 | val_loss=0.9914 | val_acc=0.6187
Epoch 036 | loss=0.9022 | train_acc=0.6098 | val_loss=1.0032 | val_acc=0.6187
Epoch 037 | loss=0.9120 | train_acc=0.6094 | val_loss=0.9992 | val_acc=0.6187
Epoch 038 | loss=0.9123 | train_acc=0.6096 | val_loss=0.9953 | val_acc=0.6187
Epoch 039 | loss=0.9088 | train_acc=0.6092 | val_loss=0.9961 | val_acc=0.6187
Epoch 040 | loss=0.9134 | train_acc=0.6094 | val_loss=0.9882 | val_acc=0.6187
Epoch 041 | loss=0.9020 | train_acc=0.6101 | val_loss=0.9893 | val_acc=0.6187
Epoch 042 | loss=0.9135 | train_acc=0.6097 | val_loss=0.9841 | val_acc=0.6187
Epoch 043 | loss=0.9037 | train_acc=0.6097 | val_loss=0.9915 | val_acc=0.6187
Epoch 044 | loss=1.1357 | train_acc=0.6092 | val_loss=0.9820 | val_acc=0.6120
Epoch 045 | loss=1.0971 | train_acc=0.6070 | val_loss=0.9712 | val_acc=0.6179
Epoch 046 | loss=1.1406 | train_acc=0.6077 | val_loss=1.0047 | val_acc=0.6187
Epoch 047 | loss=0.9485 | train_acc=0.6085 | val_loss=1.0062 | val_acc=0.6187
Epoch 048 | loss=0.9446 | train_acc=0.6091 | val_loss=1.0063 | val_acc=0.6187
Epoch 049 | loss=0.9563 | train_acc=0.6044 | val_loss=1.0020 | val_acc=0.6187
Final Test Loss: 0.9133 | Test Accuracy: 0.6322
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=3.7263 | train_acc=0.6573 | val_loss=2.4236 | val_acc=0.1955
Epoch 001 | loss=1.6196 | train_acc=0.5742 | val_loss=1.1303 | val_acc=0.3691
Epoch 002 | loss=1.1963 | train_acc=0.5339 | val_loss=1.0565 | val_acc=0.6187
Epoch 003 | loss=1.0564 | train_acc=0.5772 | val_loss=1.0439 | val_acc=0.6187
Epoch 004 | loss=1.0272 | train_acc=0.6018 | val_loss=1.0352 | val_acc=0.6187
Epoch 005 | loss=1.0156 | train_acc=0.6049 | val_loss=1.0289 | val_acc=0.6187
Epoch 006 | loss=1.0032 | train_acc=0.6067 | val_loss=1.0215 | val_acc=0.6187
Epoch 007 | loss=0.9988 | train_acc=0.6066 | val_loss=1.0194 | val_acc=0.6187
Epoch 008 | loss=0.9876 | train_acc=0.6076 | val_loss=1.0159 | val_acc=0.6187
Epoch 009 | loss=0.9776 | train_acc=0.6084 | val_loss=1.0133 | val_acc=0.6187
Epoch 010 | loss=0.9696 | train_acc=0.6089 | val_loss=1.0104 | val_acc=0.6187
Epoch 011 | loss=0.9636 | train_acc=0.6088 | val_loss=1.0082 | val_acc=0.6187
Epoch 012 | loss=0.9579 | train_acc=0.6092 | val_loss=1.0063 | val_acc=0.6187
Epoch 013 | loss=0.9435 | train_acc=0.6089 | val_loss=1.0028 | val_acc=0.6187
Epoch 014 | loss=0.9475 | train_acc=0.6093 | val_loss=0.9967 | val_acc=0.6187
Epoch 015 | loss=0.9349 | train_acc=0.6097 | val_loss=0.9918 | val_acc=0.6187
Epoch 016 | loss=0.9389 | train_acc=0.6096 | val_loss=0.9824 | val_acc=0.6187
Epoch 017 | loss=0.9251 | train_acc=0.6095 | val_loss=0.9806 | val_acc=0.6187
Epoch 018 | loss=0.9271 | train_acc=0.6097 | val_loss=0.9704 | val_acc=0.6187
Epoch 019 | loss=0.9094 | train_acc=0.6098 | val_loss=0.9802 | val_acc=0.6187
Epoch 020 | loss=0.9220 | train_acc=0.6099 | val_loss=0.9637 | val_acc=0.6187
Epoch 021 | loss=0.8925 | train_acc=0.6094 | val_loss=0.9761 | val_acc=0.6183
Epoch 022 | loss=0.9053 | train_acc=0.6096 | val_loss=0.9559 | val_acc=0.6187
Epoch 023 | loss=0.8860 | train_acc=0.6095 | val_loss=0.9651 | val_acc=0.6183
Epoch 024 | loss=0.8968 | train_acc=0.6098 | val_loss=0.9548 | val_acc=0.6187
Epoch 025 | loss=0.8919 | train_acc=0.6096 | val_loss=0.9715 | val_acc=0.6183
Epoch 026 | loss=0.9080 | train_acc=0.6095 | val_loss=0.9527 | val_acc=0.6187
Epoch 027 | loss=0.8856 | train_acc=0.6094 | val_loss=0.9813 | val_acc=0.6187
Epoch 028 | loss=0.9111 | train_acc=0.6097 | val_loss=0.9739 | val_acc=0.6183
Epoch 029 | loss=0.9884 | train_acc=0.6096 | val_loss=0.9731 | val_acc=0.6187
Epoch 030 | loss=1.2506 | train_acc=0.6398 | val_loss=5.8346 | val_acc=0.2803
Epoch 031 | loss=1.1797 | train_acc=0.6723 | val_loss=6.6317 | val_acc=0.2880
Epoch 032 | loss=1.4632 | train_acc=0.6453 | val_loss=3.6842 | val_acc=0.2606
Epoch 033 | loss=1.3115 | train_acc=0.6015 | val_loss=1.0085 | val_acc=0.6187
Epoch 034 | loss=1.5781 | train_acc=0.6103 | val_loss=2.7656 | val_acc=0.3462
Epoch 035 | loss=1.0699 | train_acc=0.6043 | val_loss=1.0089 | val_acc=0.6187
Epoch 036 | loss=0.9584 | train_acc=0.6092 | val_loss=1.0084 | val_acc=0.6187
Epoch 037 | loss=0.9514 | train_acc=0.6099 | val_loss=1.0084 | val_acc=0.6187
Epoch 038 | loss=0.9490 | train_acc=0.6098 | val_loss=1.0087 | val_acc=0.6187
Epoch 039 | loss=0.9478 | train_acc=0.6098 | val_loss=1.0091 | val_acc=0.6187
Epoch 040 | loss=0.9457 | train_acc=0.6099 | val_loss=1.0096 | val_acc=0.6187
Epoch 041 | loss=0.9442 | train_acc=0.6098 | val_loss=1.0102 | val_acc=0.6187
Epoch 042 | loss=0.9428 | train_acc=0.6099 | val_loss=1.0107 | val_acc=0.6187
Epoch 043 | loss=0.9416 | train_acc=0.6100 | val_loss=1.0113 | val_acc=0.6187
Epoch 044 | loss=0.9408 | train_acc=0.6100 | val_loss=1.0119 | val_acc=0.6187
Epoch 045 | loss=0.9692 | train_acc=0.6099 | val_loss=1.0129 | val_acc=0.6187
Epoch 046 | loss=0.9483 | train_acc=0.6100 | val_loss=1.0134 | val_acc=0.6187
Epoch 047 | loss=0.9407 | train_acc=0.6095 | val_loss=1.0140 | val_acc=0.6187
Epoch 048 | loss=0.9391 | train_acc=0.6099 | val_loss=1.0146 | val_acc=0.6187
Epoch 049 | loss=0.9377 | train_acc=0.6099 | val_loss=1.0149 | val_acc=0.6187
Final Test Loss: 0.9205 | Test Accuracy: 0.6322
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=5.1248 | train_acc=0.6092 | val_loss=7.4629 | val_acc=0.1529
Epoch 001 | loss=3.3307 | train_acc=0.4835 | val_loss=1.2036 | val_acc=0.1870
Epoch 002 | loss=1.2124 | train_acc=0.4420 | val_loss=1.0734 | val_acc=0.6187
Epoch 003 | loss=1.0832 | train_acc=0.5635 | val_loss=1.0644 | val_acc=0.6187
Epoch 004 | loss=1.0605 | train_acc=0.5861 | val_loss=1.0524 | val_acc=0.6187
Epoch 005 | loss=1.0429 | train_acc=0.5926 | val_loss=1.0427 | val_acc=0.6187
Epoch 006 | loss=1.0260 | train_acc=0.5965 | val_loss=1.0348 | val_acc=0.6187
Epoch 007 | loss=1.0140 | train_acc=0.5981 | val_loss=1.0283 | val_acc=0.6187
Epoch 008 | loss=1.0032 | train_acc=0.6006 | val_loss=1.0233 | val_acc=0.6187
Epoch 009 | loss=0.9932 | train_acc=0.6004 | val_loss=1.0193 | val_acc=0.6187
Epoch 010 | loss=0.9887 | train_acc=0.6008 | val_loss=1.0163 | val_acc=0.6187
Epoch 011 | loss=0.9810 | train_acc=0.6002 | val_loss=1.0140 | val_acc=0.6187
Epoch 012 | loss=0.9737 | train_acc=0.6019 | val_loss=1.0121 | val_acc=0.6187
Epoch 013 | loss=0.9699 | train_acc=0.6027 | val_loss=1.0108 | val_acc=0.6187
Epoch 014 | loss=0.9648 | train_acc=0.6018 | val_loss=1.0099 | val_acc=0.6187
Epoch 015 | loss=0.9622 | train_acc=0.6018 | val_loss=1.0094 | val_acc=0.6187
Epoch 016 | loss=0.9587 | train_acc=0.6010 | val_loss=1.0090 | val_acc=0.6187
Epoch 017 | loss=0.9551 | train_acc=0.6010 | val_loss=1.0089 | val_acc=0.6187
Epoch 018 | loss=0.9533 | train_acc=0.6016 | val_loss=1.0090 | val_acc=0.6187
Epoch 019 | loss=0.9514 | train_acc=0.6017 | val_loss=1.0092 | val_acc=0.6187
Epoch 020 | loss=0.9500 | train_acc=0.6018 | val_loss=1.0094 | val_acc=0.6187
Epoch 021 | loss=0.9470 | train_acc=0.6017 | val_loss=1.0097 | val_acc=0.6187
Epoch 022 | loss=0.9462 | train_acc=0.6022 | val_loss=1.0101 | val_acc=0.6187
Epoch 023 | loss=0.9446 | train_acc=0.6010 | val_loss=1.0105 | val_acc=0.6187
Epoch 024 | loss=0.9430 | train_acc=0.6040 | val_loss=1.0109 | val_acc=0.6187
Epoch 025 | loss=0.9426 | train_acc=0.6028 | val_loss=1.0114 | val_acc=0.6187
Epoch 026 | loss=0.9420 | train_acc=0.6025 | val_loss=1.0119 | val_acc=0.6187
Epoch 027 | loss=0.9423 | train_acc=0.6016 | val_loss=1.0126 | val_acc=0.6187
Epoch 028 | loss=0.9405 | train_acc=0.6025 | val_loss=1.0132 | val_acc=0.6187
Epoch 029 | loss=0.9402 | train_acc=0.6025 | val_loss=1.0138 | val_acc=0.6187
Epoch 030 | loss=0.9389 | train_acc=0.6029 | val_loss=1.0144 | val_acc=0.6187
Epoch 031 | loss=0.9380 | train_acc=0.6019 | val_loss=1.0150 | val_acc=0.6187
Epoch 032 | loss=0.9378 | train_acc=0.6019 | val_loss=1.0155 | val_acc=0.6187
Epoch 033 | loss=0.9381 | train_acc=0.6027 | val_loss=1.0160 | val_acc=0.6187
Epoch 034 | loss=0.9374 | train_acc=0.6019 | val_loss=1.0166 | val_acc=0.6187
Epoch 035 | loss=0.9373 | train_acc=0.6013 | val_loss=1.0172 | val_acc=0.6187
Epoch 036 | loss=0.9368 | train_acc=0.6028 | val_loss=1.0178 | val_acc=0.6187
Epoch 037 | loss=0.9374 | train_acc=0.6013 | val_loss=1.0183 | val_acc=0.6187
Epoch 038 | loss=0.9374 | train_acc=0.6024 | val_loss=1.0188 | val_acc=0.6187
Epoch 039 | loss=0.9354 | train_acc=0.6029 | val_loss=1.0192 | val_acc=0.6187
Epoch 040 | loss=0.9364 | train_acc=0.6022 | val_loss=1.0196 | val_acc=0.6187
Epoch 041 | loss=0.9349 | train_acc=0.6029 | val_loss=1.0200 | val_acc=0.6187
Epoch 042 | loss=0.9357 | train_acc=0.6030 | val_loss=1.0204 | val_acc=0.6187
Epoch 043 | loss=0.9357 | train_acc=0.6021 | val_loss=1.0208 | val_acc=0.6187
Epoch 044 | loss=0.9345 | train_acc=0.6032 | val_loss=1.0211 | val_acc=0.6187
Epoch 045 | loss=0.9352 | train_acc=0.6019 | val_loss=1.0214 | val_acc=0.6187
Epoch 046 | loss=0.9345 | train_acc=0.6031 | val_loss=1.0217 | val_acc=0.6187
Epoch 047 | loss=0.9352 | train_acc=0.6024 | val_loss=1.0220 | val_acc=0.6187
Epoch 048 | loss=0.9352 | train_acc=0.6022 | val_loss=1.0223 | val_acc=0.6187
Epoch 049 | loss=0.9352 | train_acc=0.6021 | val_loss=1.0226 | val_acc=0.6187
Final Test Loss: 0.9150 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=6.2659 | train_acc=0.6205 | val_loss=8.2151 | val_acc=0.1529
Epoch 001 | loss=4.3501 | train_acc=0.5005 | val_loss=2.7963 | val_acc=0.1529
Epoch 002 | loss=1.4695 | train_acc=0.5018 | val_loss=1.0866 | val_acc=0.6190
Epoch 003 | loss=1.1592 | train_acc=0.5440 | val_loss=1.0750 | val_acc=0.5605
Epoch 004 | loss=1.1028 | train_acc=0.5706 | val_loss=1.0571 | val_acc=0.6187
Epoch 005 | loss=1.0595 | train_acc=0.5880 | val_loss=1.0475 | val_acc=0.6187
Epoch 006 | loss=1.0365 | train_acc=0.5978 | val_loss=1.0392 | val_acc=0.6187
Epoch 007 | loss=1.0216 | train_acc=0.6024 | val_loss=1.0320 | val_acc=0.6187
Epoch 008 | loss=1.0083 | train_acc=0.6034 | val_loss=1.0262 | val_acc=0.6187
Epoch 009 | loss=0.9993 | train_acc=0.6056 | val_loss=1.0217 | val_acc=0.6187
Epoch 010 | loss=0.9909 | train_acc=0.6058 | val_loss=1.0182 | val_acc=0.6187
Epoch 011 | loss=0.9829 | train_acc=0.6067 | val_loss=1.0155 | val_acc=0.6187
Epoch 012 | loss=0.9773 | train_acc=0.6065 | val_loss=1.0135 | val_acc=0.6187
Epoch 013 | loss=0.9729 | train_acc=0.6067 | val_loss=1.0120 | val_acc=0.6187
Epoch 014 | loss=0.9676 | train_acc=0.6073 | val_loss=1.0109 | val_acc=0.6187
Epoch 015 | loss=0.9647 | train_acc=0.6075 | val_loss=1.0102 | val_acc=0.6187
Epoch 016 | loss=0.9602 | train_acc=0.6078 | val_loss=1.0097 | val_acc=0.6187
Epoch 017 | loss=0.9575 | train_acc=0.6076 | val_loss=1.0095 | val_acc=0.6187
Epoch 018 | loss=0.9556 | train_acc=0.6074 | val_loss=1.0094 | val_acc=0.6187
Epoch 019 | loss=0.9529 | train_acc=0.6073 | val_loss=1.0094 | val_acc=0.6187
Epoch 020 | loss=0.9510 | train_acc=0.6074 | val_loss=1.0096 | val_acc=0.6187
Epoch 021 | loss=0.9489 | train_acc=0.6077 | val_loss=1.0098 | val_acc=0.6187
Epoch 022 | loss=0.9471 | train_acc=0.6076 | val_loss=1.0101 | val_acc=0.6187
Epoch 023 | loss=0.9457 | train_acc=0.6078 | val_loss=1.0104 | val_acc=0.6187
Epoch 024 | loss=0.9441 | train_acc=0.6077 | val_loss=1.0108 | val_acc=0.6187
Epoch 025 | loss=0.9432 | train_acc=0.6076 | val_loss=1.0111 | val_acc=0.6187
Epoch 026 | loss=0.9419 | train_acc=0.6079 | val_loss=1.0115 | val_acc=0.6187
Epoch 027 | loss=0.9420 | train_acc=0.6078 | val_loss=1.0120 | val_acc=0.6187
Epoch 028 | loss=0.9392 | train_acc=0.6083 | val_loss=1.0125 | val_acc=0.6187
Epoch 029 | loss=0.9397 | train_acc=0.6079 | val_loss=1.0129 | val_acc=0.6187
Epoch 030 | loss=0.9401 | train_acc=0.6081 | val_loss=1.0133 | val_acc=0.6187
Epoch 031 | loss=0.9388 | train_acc=0.6078 | val_loss=1.0137 | val_acc=0.6187
Epoch 032 | loss=0.9582 | train_acc=0.6079 | val_loss=1.0143 | val_acc=0.6187
Epoch 033 | loss=0.9399 | train_acc=0.6080 | val_loss=1.0148 | val_acc=0.6187
Epoch 034 | loss=0.9376 | train_acc=0.6083 | val_loss=1.0154 | val_acc=0.6187
Epoch 035 | loss=0.9379 | train_acc=0.6081 | val_loss=1.0159 | val_acc=0.6187
Epoch 036 | loss=0.9368 | train_acc=0.6085 | val_loss=1.0163 | val_acc=0.6187
Epoch 037 | loss=0.9368 | train_acc=0.6082 | val_loss=1.0168 | val_acc=0.6187
Epoch 038 | loss=0.9366 | train_acc=0.6075 | val_loss=1.0171 | val_acc=0.6187
Epoch 039 | loss=0.9366 | train_acc=0.6080 | val_loss=1.0175 | val_acc=0.6187
Epoch 040 | loss=0.9364 | train_acc=0.6085 | val_loss=1.0179 | val_acc=0.6187
Epoch 041 | loss=0.9352 | train_acc=0.6080 | val_loss=1.0182 | val_acc=0.6187
Epoch 042 | loss=0.9359 | train_acc=0.6080 | val_loss=1.0185 | val_acc=0.6187
Epoch 043 | loss=0.9356 | train_acc=0.6082 | val_loss=1.0189 | val_acc=0.6187
Epoch 044 | loss=0.9343 | train_acc=0.6084 | val_loss=1.0191 | val_acc=0.6187
Epoch 045 | loss=0.9351 | train_acc=0.6086 | val_loss=1.0194 | val_acc=0.6187
Epoch 046 | loss=0.9349 | train_acc=0.6084 | val_loss=1.0195 | val_acc=0.6187
Epoch 047 | loss=0.9359 | train_acc=0.6085 | val_loss=1.0197 | val_acc=0.6187
Epoch 048 | loss=0.9339 | train_acc=0.6085 | val_loss=1.0199 | val_acc=0.6187
Epoch 049 | loss=0.9499 | train_acc=0.6082 | val_loss=1.0200 | val_acc=0.6187
Final Test Loss: 0.9155 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=7.8714 | train_acc=0.6230 | val_loss=11.3538 | val_acc=0.1529
Epoch 001 | loss=4.4456 | train_acc=0.4735 | val_loss=1.1879 | val_acc=0.1677
Epoch 002 | loss=1.1843 | train_acc=0.4370 | val_loss=1.0810 | val_acc=0.6187
Epoch 003 | loss=1.0878 | train_acc=0.5623 | val_loss=1.0655 | val_acc=0.6187
Epoch 004 | loss=1.0614 | train_acc=0.5880 | val_loss=1.0530 | val_acc=0.6187
Epoch 005 | loss=1.0425 | train_acc=0.5967 | val_loss=1.0431 | val_acc=0.6187
Epoch 006 | loss=1.0273 | train_acc=0.6017 | val_loss=1.0350 | val_acc=0.6187
Epoch 007 | loss=1.0132 | train_acc=0.6041 | val_loss=1.0285 | val_acc=0.6187
Epoch 008 | loss=1.0038 | train_acc=0.6061 | val_loss=1.0233 | val_acc=0.6187
Epoch 009 | loss=0.9943 | train_acc=0.6070 | val_loss=1.0193 | val_acc=0.6187
Epoch 010 | loss=0.9873 | train_acc=0.6076 | val_loss=1.0161 | val_acc=0.6187
Epoch 011 | loss=0.9805 | train_acc=0.6079 | val_loss=1.0137 | val_acc=0.6187
Epoch 012 | loss=0.9750 | train_acc=0.6081 | val_loss=1.0118 | val_acc=0.6187
Epoch 013 | loss=0.9723 | train_acc=0.6086 | val_loss=1.0106 | val_acc=0.6187
Epoch 014 | loss=0.9664 | train_acc=0.6086 | val_loss=1.0096 | val_acc=0.6187
Epoch 015 | loss=0.9624 | train_acc=0.6089 | val_loss=1.0090 | val_acc=0.6187
Epoch 016 | loss=0.9596 | train_acc=0.6091 | val_loss=1.0087 | val_acc=0.6187
Epoch 017 | loss=0.9563 | train_acc=0.6091 | val_loss=1.0086 | val_acc=0.6187
Epoch 018 | loss=0.9541 | train_acc=0.6093 | val_loss=1.0086 | val_acc=0.6187
Epoch 019 | loss=0.9519 | train_acc=0.6090 | val_loss=1.0088 | val_acc=0.6187
Epoch 020 | loss=0.9494 | train_acc=0.6096 | val_loss=1.0091 | val_acc=0.6187
Epoch 021 | loss=0.9475 | train_acc=0.6095 | val_loss=1.0095 | val_acc=0.6187
Epoch 022 | loss=0.9467 | train_acc=0.6095 | val_loss=1.0099 | val_acc=0.6187
Epoch 023 | loss=0.9454 | train_acc=0.6093 | val_loss=1.0104 | val_acc=0.6187
Epoch 024 | loss=0.9439 | train_acc=0.6095 | val_loss=1.0109 | val_acc=0.6187
Epoch 025 | loss=0.9432 | train_acc=0.6094 | val_loss=1.0115 | val_acc=0.6187
Epoch 026 | loss=0.9422 | train_acc=0.6093 | val_loss=1.0120 | val_acc=0.6187
Epoch 027 | loss=0.9414 | train_acc=0.6094 | val_loss=1.0126 | val_acc=0.6187
Epoch 028 | loss=0.9408 | train_acc=0.6094 | val_loss=1.0132 | val_acc=0.6187
Epoch 029 | loss=0.9408 | train_acc=0.6095 | val_loss=1.0138 | val_acc=0.6187
Epoch 030 | loss=0.9389 | train_acc=0.6096 | val_loss=1.0143 | val_acc=0.6187
Epoch 031 | loss=0.9389 | train_acc=0.6095 | val_loss=1.0149 | val_acc=0.6187
Epoch 032 | loss=0.9382 | train_acc=0.6099 | val_loss=1.0155 | val_acc=0.6187
Epoch 033 | loss=0.9381 | train_acc=0.6097 | val_loss=1.0160 | val_acc=0.6187
Epoch 034 | loss=0.9377 | train_acc=0.6099 | val_loss=1.0165 | val_acc=0.6187
Epoch 035 | loss=0.9376 | train_acc=0.6093 | val_loss=1.0170 | val_acc=0.6187
Epoch 036 | loss=0.9371 | train_acc=0.6094 | val_loss=1.0174 | val_acc=0.6187
Epoch 037 | loss=0.9366 | train_acc=0.6103 | val_loss=1.0179 | val_acc=0.6187
Epoch 038 | loss=0.9371 | train_acc=0.6097 | val_loss=1.0183 | val_acc=0.6187
Epoch 039 | loss=0.9368 | train_acc=0.6097 | val_loss=1.0187 | val_acc=0.6187
Epoch 040 | loss=0.9364 | train_acc=0.6097 | val_loss=1.0191 | val_acc=0.6187
Epoch 041 | loss=0.9358 | train_acc=0.6098 | val_loss=1.0194 | val_acc=0.6187
Epoch 042 | loss=0.9362 | train_acc=0.6096 | val_loss=1.0198 | val_acc=0.6187
Epoch 043 | loss=0.9357 | train_acc=0.6097 | val_loss=1.0201 | val_acc=0.6187
Epoch 044 | loss=0.9356 | train_acc=0.6095 | val_loss=1.0204 | val_acc=0.6187
Epoch 045 | loss=0.9357 | train_acc=0.6097 | val_loss=1.0207 | val_acc=0.6187
Epoch 046 | loss=0.9352 | train_acc=0.6096 | val_loss=1.0211 | val_acc=0.6187
Epoch 047 | loss=0.9351 | train_acc=0.6098 | val_loss=1.0215 | val_acc=0.6187
Epoch 048 | loss=0.9349 | train_acc=0.6097 | val_loss=1.0217 | val_acc=0.6187
Epoch 049 | loss=0.9352 | train_acc=0.6099 | val_loss=1.0219 | val_acc=0.6187
Final Test Loss: 0.9153 | Test Accuracy: 0.6322
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=6.6333 | train_acc=0.6358 | val_loss=3.5370 | val_acc=0.1529
Epoch 001 | loss=2.9412 | train_acc=0.5478 | val_loss=1.5306 | val_acc=0.1581
Epoch 002 | loss=1.4807 | train_acc=0.5146 | val_loss=1.0897 | val_acc=0.1947
Epoch 003 | loss=1.2084 | train_acc=0.5029 | val_loss=1.0760 | val_acc=0.6187
Epoch 004 | loss=1.1037 | train_acc=0.5693 | val_loss=1.0615 | val_acc=0.6187
Epoch 005 | loss=1.0586 | train_acc=0.5904 | val_loss=1.0486 | val_acc=0.6187
Epoch 006 | loss=1.0365 | train_acc=0.6017 | val_loss=1.0388 | val_acc=0.6187
Epoch 007 | loss=1.0202 | train_acc=0.6053 | val_loss=1.0313 | val_acc=0.6187
Epoch 008 | loss=1.0106 | train_acc=0.6065 | val_loss=1.0254 | val_acc=0.6187
Epoch 009 | loss=1.0036 | train_acc=0.6066 | val_loss=1.0211 | val_acc=0.6187
Epoch 010 | loss=0.9922 | train_acc=0.6066 | val_loss=1.0175 | val_acc=0.6187
Epoch 011 | loss=0.9833 | train_acc=0.6082 | val_loss=1.0147 | val_acc=0.6187
Epoch 012 | loss=0.9779 | train_acc=0.6087 | val_loss=1.0127 | val_acc=0.6187
Epoch 013 | loss=0.9719 | train_acc=0.6087 | val_loss=1.0112 | val_acc=0.6187
Epoch 014 | loss=0.9683 | train_acc=0.6090 | val_loss=1.0102 | val_acc=0.6187
Epoch 015 | loss=0.9637 | train_acc=0.6091 | val_loss=1.0096 | val_acc=0.6187
Epoch 016 | loss=0.9597 | train_acc=0.6091 | val_loss=1.0092 | val_acc=0.6187
Epoch 017 | loss=0.9566 | train_acc=0.6094 | val_loss=1.0089 | val_acc=0.6187
Epoch 018 | loss=0.9535 | train_acc=0.6095 | val_loss=1.0089 | val_acc=0.6187
Epoch 019 | loss=0.9524 | train_acc=0.6095 | val_loss=1.0090 | val_acc=0.6187
Epoch 020 | loss=0.9490 | train_acc=0.6095 | val_loss=1.0092 | val_acc=0.6187
Epoch 021 | loss=0.9477 | train_acc=0.6093 | val_loss=1.0095 | val_acc=0.6187
Epoch 022 | loss=0.9460 | train_acc=0.6095 | val_loss=1.0099 | val_acc=0.6187
Epoch 023 | loss=0.9448 | train_acc=0.6095 | val_loss=1.0102 | val_acc=0.6187
Epoch 024 | loss=0.9442 | train_acc=0.6096 | val_loss=1.0106 | val_acc=0.6187
Epoch 025 | loss=0.9414 | train_acc=0.6097 | val_loss=1.0111 | val_acc=0.6187
Epoch 026 | loss=0.9422 | train_acc=0.6098 | val_loss=1.0114 | val_acc=0.6187
Epoch 027 | loss=0.9398 | train_acc=0.6095 | val_loss=1.0117 | val_acc=0.6187
Epoch 028 | loss=0.9395 | train_acc=0.6097 | val_loss=1.0120 | val_acc=0.6187
Epoch 029 | loss=0.9403 | train_acc=0.6095 | val_loss=1.0127 | val_acc=0.6187
Epoch 030 | loss=0.9377 | train_acc=0.6096 | val_loss=1.0132 | val_acc=0.6187
Epoch 031 | loss=0.9396 | train_acc=0.6097 | val_loss=1.0133 | val_acc=0.6187
Epoch 032 | loss=0.9372 | train_acc=0.6099 | val_loss=1.0139 | val_acc=0.6187
Epoch 033 | loss=0.9366 | train_acc=0.6097 | val_loss=1.0144 | val_acc=0.6187
Epoch 034 | loss=0.9357 | train_acc=0.6097 | val_loss=1.0147 | val_acc=0.6187
Epoch 035 | loss=0.9664 | train_acc=0.6097 | val_loss=1.0152 | val_acc=0.6187
Epoch 036 | loss=0.9363 | train_acc=0.6098 | val_loss=1.0157 | val_acc=0.6187
Epoch 037 | loss=0.9340 | train_acc=0.6096 | val_loss=1.0161 | val_acc=0.6187
Epoch 038 | loss=0.9392 | train_acc=0.6098 | val_loss=1.0160 | val_acc=0.6187
Epoch 039 | loss=0.9361 | train_acc=0.6093 | val_loss=1.0166 | val_acc=0.6187
Epoch 040 | loss=0.9343 | train_acc=0.6096 | val_loss=1.0171 | val_acc=0.6187
Epoch 041 | loss=0.9338 | train_acc=0.6099 | val_loss=1.0175 | val_acc=0.6187
Epoch 042 | loss=0.9351 | train_acc=0.6097 | val_loss=1.0171 | val_acc=0.6187
Epoch 043 | loss=0.9320 | train_acc=0.6096 | val_loss=1.0174 | val_acc=0.6187
Epoch 044 | loss=0.9319 | train_acc=0.6096 | val_loss=1.0174 | val_acc=0.6187
Epoch 045 | loss=0.9774 | train_acc=0.6096 | val_loss=1.0176 | val_acc=0.6187
Epoch 046 | loss=0.9360 | train_acc=0.6093 | val_loss=1.0179 | val_acc=0.6187
Epoch 047 | loss=0.9330 | train_acc=0.6097 | val_loss=1.0182 | val_acc=0.6187
Epoch 048 | loss=1.3143 | train_acc=0.6379 | val_loss=10.8791 | val_acc=0.2943
Epoch 049 | loss=1.3730 | train_acc=0.6501 | val_loss=10.1133 | val_acc=0.2969
Final Test Loss: 12.2043 | Test Accuracy: 0.3104
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.3030 | train_acc=0.6598 | val_loss=4.1609 | val_acc=0.1970
Epoch 001 | loss=1.4344 | train_acc=0.5059 | val_loss=1.0645 | val_acc=0.5272
Epoch 002 | loss=1.1520 | train_acc=0.5807 | val_loss=1.0510 | val_acc=0.6187
Epoch 003 | loss=1.0625 | train_acc=0.5874 | val_loss=1.0393 | val_acc=0.6187
Epoch 004 | loss=1.0953 | train_acc=0.5995 | val_loss=1.0310 | val_acc=0.6187
Epoch 005 | loss=1.0824 | train_acc=0.5988 | val_loss=1.0305 | val_acc=0.6187
Epoch 006 | loss=1.0140 | train_acc=0.6011 | val_loss=1.0190 | val_acc=0.6187
Epoch 007 | loss=0.9942 | train_acc=0.6027 | val_loss=1.0179 | val_acc=0.6187
Epoch 008 | loss=0.9843 | train_acc=0.6026 | val_loss=1.0149 | val_acc=0.6187
Epoch 009 | loss=0.9723 | train_acc=0.6024 | val_loss=1.0147 | val_acc=0.6187
Epoch 010 | loss=0.9657 | train_acc=0.6031 | val_loss=1.0163 | val_acc=0.6187
Epoch 011 | loss=0.9624 | train_acc=0.6025 | val_loss=1.0174 | val_acc=0.6187
Epoch 012 | loss=0.9602 | train_acc=0.6023 | val_loss=1.0192 | val_acc=0.6187
Epoch 013 | loss=0.9581 | train_acc=0.6026 | val_loss=1.0195 | val_acc=0.6187
Epoch 014 | loss=0.9580 | train_acc=0.6037 | val_loss=1.0192 | val_acc=0.6187
Epoch 015 | loss=0.9862 | train_acc=0.6014 | val_loss=1.0160 | val_acc=0.6187
Epoch 016 | loss=0.9997 | train_acc=0.6006 | val_loss=1.0087 | val_acc=0.6187
Epoch 017 | loss=1.0678 | train_acc=0.6009 | val_loss=1.0175 | val_acc=0.6187
Epoch 018 | loss=1.0473 | train_acc=0.6005 | val_loss=1.0125 | val_acc=0.6187
Epoch 019 | loss=1.0451 | train_acc=0.6006 | val_loss=1.0152 | val_acc=0.6146
Epoch 020 | loss=1.0592 | train_acc=0.5979 | val_loss=1.0148 | val_acc=0.6187
Epoch 021 | loss=1.0353 | train_acc=0.6010 | val_loss=1.0113 | val_acc=0.6187
Epoch 022 | loss=0.9836 | train_acc=0.6005 | val_loss=1.0063 | val_acc=0.6187
Epoch 023 | loss=0.9699 | train_acc=0.6029 | val_loss=1.0088 | val_acc=0.6187
Epoch 024 | loss=0.9606 | train_acc=0.6019 | val_loss=1.0112 | val_acc=0.6187
Epoch 025 | loss=0.9570 | train_acc=0.6022 | val_loss=1.0107 | val_acc=0.6187
Epoch 026 | loss=0.9544 | train_acc=0.6024 | val_loss=1.0092 | val_acc=0.6187
Epoch 027 | loss=0.9509 | train_acc=0.6020 | val_loss=1.0067 | val_acc=0.6187
Epoch 028 | loss=0.9922 | train_acc=0.6020 | val_loss=1.0115 | val_acc=0.6187
Epoch 029 | loss=0.9696 | train_acc=0.6031 | val_loss=1.0136 | val_acc=0.6183
Epoch 030 | loss=0.9598 | train_acc=0.6031 | val_loss=1.0155 | val_acc=0.6187
Epoch 031 | loss=0.9520 | train_acc=0.6037 | val_loss=1.0168 | val_acc=0.6187
Epoch 032 | loss=0.9492 | train_acc=0.6013 | val_loss=1.0173 | val_acc=0.6187
Epoch 033 | loss=0.9459 | train_acc=0.6021 | val_loss=1.0184 | val_acc=0.6187
Epoch 034 | loss=0.9442 | train_acc=0.6018 | val_loss=1.0180 | val_acc=0.6187
Epoch 035 | loss=0.9419 | train_acc=0.6027 | val_loss=1.0168 | val_acc=0.6187
Epoch 036 | loss=0.9409 | train_acc=0.6032 | val_loss=1.0161 | val_acc=0.6187
Epoch 037 | loss=0.9396 | train_acc=0.6034 | val_loss=1.0155 | val_acc=0.6187
Epoch 038 | loss=0.9396 | train_acc=0.6024 | val_loss=1.0148 | val_acc=0.6187
Epoch 039 | loss=0.9377 | train_acc=0.6037 | val_loss=1.0140 | val_acc=0.6187
Epoch 040 | loss=0.9369 | train_acc=0.6027 | val_loss=1.0128 | val_acc=0.6187
Epoch 041 | loss=0.9366 | train_acc=0.6029 | val_loss=1.0114 | val_acc=0.6187
Epoch 042 | loss=0.9360 | train_acc=0.6012 | val_loss=1.0093 | val_acc=0.6187
Epoch 043 | loss=0.9374 | train_acc=0.6019 | val_loss=1.0129 | val_acc=0.6187
Epoch 044 | loss=0.9345 | train_acc=0.6022 | val_loss=1.0076 | val_acc=0.6187
Epoch 045 | loss=0.9310 | train_acc=0.6029 | val_loss=0.9919 | val_acc=0.6187
Epoch 046 | loss=0.9384 | train_acc=0.6027 | val_loss=1.0169 | val_acc=0.6187
Epoch 047 | loss=0.9350 | train_acc=0.6024 | val_loss=1.0146 | val_acc=0.6187
Epoch 048 | loss=0.9321 | train_acc=0.6022 | val_loss=1.0042 | val_acc=0.6187
Epoch 049 | loss=0.9015 | train_acc=0.6254 | val_loss=1.2182 | val_acc=0.5228
Final Test Loss: 1.7335 | Test Accuracy: 0.4524
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.3665 | train_acc=0.6654 | val_loss=3.3478 | val_acc=0.1970
Epoch 001 | loss=1.4389 | train_acc=0.5213 | val_loss=1.0499 | val_acc=0.6190
Epoch 002 | loss=1.0982 | train_acc=0.5889 | val_loss=1.0445 | val_acc=0.6187
Epoch 003 | loss=1.1055 | train_acc=0.5976 | val_loss=1.0418 | val_acc=0.6150
Epoch 004 | loss=1.0604 | train_acc=0.5978 | val_loss=1.0336 | val_acc=0.6187
Epoch 005 | loss=1.0408 | train_acc=0.6057 | val_loss=1.0245 | val_acc=0.6187
Epoch 006 | loss=1.0046 | train_acc=0.6079 | val_loss=1.0184 | val_acc=0.6187
Epoch 007 | loss=0.9913 | train_acc=0.6078 | val_loss=1.0110 | val_acc=0.6187
Epoch 008 | loss=0.9809 | train_acc=0.6083 | val_loss=1.0093 | val_acc=0.6187
Epoch 009 | loss=0.9737 | train_acc=0.6077 | val_loss=1.0091 | val_acc=0.6187
Epoch 010 | loss=0.9653 | train_acc=0.6084 | val_loss=1.0057 | val_acc=0.6187
Epoch 011 | loss=0.9600 | train_acc=0.6080 | val_loss=1.0050 | val_acc=0.6187
Epoch 012 | loss=0.9559 | train_acc=0.6078 | val_loss=1.0078 | val_acc=0.6187
Epoch 013 | loss=0.9606 | train_acc=0.6081 | val_loss=1.0012 | val_acc=0.6187
Epoch 014 | loss=0.9726 | train_acc=0.6084 | val_loss=1.0037 | val_acc=0.6183
Epoch 015 | loss=0.9648 | train_acc=0.6079 | val_loss=0.9996 | val_acc=0.6187
Epoch 016 | loss=0.9747 | train_acc=0.6079 | val_loss=0.9871 | val_acc=0.6187
Epoch 017 | loss=0.9821 | train_acc=0.6068 | val_loss=0.9955 | val_acc=0.6187
Epoch 018 | loss=0.9787 | train_acc=0.6082 | val_loss=0.9992 | val_acc=0.6187
Epoch 019 | loss=0.9698 | train_acc=0.6084 | val_loss=1.0032 | val_acc=0.6187
Epoch 020 | loss=0.9525 | train_acc=0.6079 | val_loss=0.9880 | val_acc=0.6187
Epoch 021 | loss=0.9418 | train_acc=0.6089 | val_loss=0.9844 | val_acc=0.6187
Epoch 022 | loss=0.9444 | train_acc=0.6083 | val_loss=0.9862 | val_acc=0.6187
Epoch 023 | loss=0.9279 | train_acc=0.6085 | val_loss=0.9688 | val_acc=0.6187
Epoch 024 | loss=0.9230 | train_acc=0.6087 | val_loss=0.9555 | val_acc=0.6187
Epoch 025 | loss=0.9128 | train_acc=0.6084 | val_loss=0.9575 | val_acc=0.6187
Epoch 026 | loss=0.9133 | train_acc=0.6085 | val_loss=0.9349 | val_acc=0.6187
Epoch 027 | loss=0.9099 | train_acc=0.6086 | val_loss=0.9205 | val_acc=0.6187
Epoch 028 | loss=0.9016 | train_acc=0.6084 | val_loss=0.9487 | val_acc=0.6187
Epoch 029 | loss=0.9127 | train_acc=0.6082 | val_loss=0.9526 | val_acc=0.6187
Epoch 030 | loss=0.9079 | train_acc=0.6085 | val_loss=0.9390 | val_acc=0.6187
Epoch 031 | loss=0.9016 | train_acc=0.6085 | val_loss=0.9342 | val_acc=0.6187
Epoch 032 | loss=0.9024 | train_acc=0.6082 | val_loss=0.9108 | val_acc=0.6187
Epoch 033 | loss=0.8951 | train_acc=0.6088 | val_loss=0.8806 | val_acc=0.6150
Epoch 034 | loss=0.9395 | train_acc=0.6022 | val_loss=0.9192 | val_acc=0.6187
Epoch 035 | loss=0.9907 | train_acc=0.6073 | val_loss=1.0036 | val_acc=0.6187
Epoch 036 | loss=0.9754 | train_acc=0.6081 | val_loss=1.0062 | val_acc=0.6187
Epoch 037 | loss=1.2751 | train_acc=0.6328 | val_loss=1.3098 | val_acc=0.5846
Epoch 038 | loss=1.1280 | train_acc=0.6540 | val_loss=1.3604 | val_acc=0.5883
Epoch 039 | loss=1.1126 | train_acc=0.6195 | val_loss=1.0126 | val_acc=0.6087
Epoch 040 | loss=1.0312 | train_acc=0.5996 | val_loss=1.0037 | val_acc=0.6187
Epoch 041 | loss=0.9770 | train_acc=0.6077 | val_loss=1.0080 | val_acc=0.6187
Epoch 042 | loss=1.0232 | train_acc=0.6072 | val_loss=1.0032 | val_acc=0.6168
Epoch 043 | loss=0.9874 | train_acc=0.6040 | val_loss=1.0093 | val_acc=0.6187
Epoch 044 | loss=0.9882 | train_acc=0.6077 | val_loss=0.9883 | val_acc=0.6187
Epoch 045 | loss=0.9345 | train_acc=0.6070 | val_loss=0.9290 | val_acc=0.6187
Epoch 046 | loss=0.9157 | train_acc=0.6074 | val_loss=0.9386 | val_acc=0.6187
Epoch 047 | loss=0.9121 | train_acc=0.6084 | val_loss=0.9222 | val_acc=0.6187
Epoch 048 | loss=0.9482 | train_acc=0.6076 | val_loss=0.9209 | val_acc=0.6187
Epoch 049 | loss=0.9135 | train_acc=0.6075 | val_loss=0.9216 | val_acc=0.6187
Final Test Loss: 0.8370 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.1614 | train_acc=0.6643 | val_loss=2.1384 | val_acc=0.1966
Epoch 001 | loss=1.2810 | train_acc=0.5254 | val_loss=1.0413 | val_acc=0.6194
Epoch 002 | loss=1.0413 | train_acc=0.5913 | val_loss=1.0279 | val_acc=0.6183
Epoch 003 | loss=1.0258 | train_acc=0.5989 | val_loss=0.9972 | val_acc=0.6187
Epoch 004 | loss=0.9796 | train_acc=0.6090 | val_loss=0.9908 | val_acc=0.6187
Epoch 005 | loss=0.9818 | train_acc=0.6081 | val_loss=0.9231 | val_acc=0.6187
Epoch 006 | loss=0.9129 | train_acc=0.6061 | val_loss=0.9689 | val_acc=0.6187
Epoch 007 | loss=0.9426 | train_acc=0.6096 | val_loss=0.9028 | val_acc=0.6187
Epoch 008 | loss=0.8964 | train_acc=0.6089 | val_loss=0.9399 | val_acc=0.6187
Epoch 009 | loss=0.9098 | train_acc=0.6088 | val_loss=0.8843 | val_acc=0.6187
Epoch 010 | loss=0.8751 | train_acc=0.6087 | val_loss=0.9080 | val_acc=0.6187
Epoch 011 | loss=0.8799 | train_acc=0.6095 | val_loss=0.9298 | val_acc=0.6187
Epoch 012 | loss=0.8777 | train_acc=0.6093 | val_loss=0.8787 | val_acc=0.6187
Epoch 013 | loss=0.8606 | train_acc=0.6097 | val_loss=0.8753 | val_acc=0.6187
Epoch 014 | loss=0.8560 | train_acc=0.6097 | val_loss=0.8895 | val_acc=0.6187
Epoch 015 | loss=0.8592 | train_acc=0.6095 | val_loss=0.8704 | val_acc=0.6187
Epoch 016 | loss=0.8508 | train_acc=0.6097 | val_loss=0.9081 | val_acc=0.6187
Epoch 017 | loss=0.8549 | train_acc=0.6092 | val_loss=0.8772 | val_acc=0.6187
Epoch 018 | loss=0.8489 | train_acc=0.6092 | val_loss=0.8944 | val_acc=0.6187
Epoch 019 | loss=0.8508 | train_acc=0.6090 | val_loss=0.8884 | val_acc=0.6187
Epoch 020 | loss=0.8512 | train_acc=0.6098 | val_loss=0.8997 | val_acc=0.6187
Epoch 021 | loss=0.8493 | train_acc=0.6100 | val_loss=0.8931 | val_acc=0.6187
Epoch 022 | loss=0.8591 | train_acc=0.6099 | val_loss=0.9175 | val_acc=0.6187
Epoch 023 | loss=0.8416 | train_acc=0.6258 | val_loss=1.1181 | val_acc=0.3813
Epoch 024 | loss=0.9026 | train_acc=0.5995 | val_loss=0.9021 | val_acc=0.6187
Epoch 025 | loss=0.8761 | train_acc=0.6096 | val_loss=0.8694 | val_acc=0.6187
Epoch 026 | loss=0.8756 | train_acc=0.6217 | val_loss=0.8077 | val_acc=0.6601
Epoch 027 | loss=0.9413 | train_acc=0.5951 | val_loss=0.9071 | val_acc=0.6187
Epoch 028 | loss=0.9416 | train_acc=0.6088 | val_loss=0.9790 | val_acc=0.6187
Epoch 029 | loss=0.9152 | train_acc=0.6093 | val_loss=0.8715 | val_acc=0.6187
Epoch 030 | loss=0.8639 | train_acc=0.6095 | val_loss=0.8745 | val_acc=0.6187
Epoch 031 | loss=0.8612 | train_acc=0.6093 | val_loss=0.8648 | val_acc=0.6187
Epoch 032 | loss=0.8530 | train_acc=0.6093 | val_loss=0.8683 | val_acc=0.6187
Epoch 033 | loss=0.8547 | train_acc=0.6099 | val_loss=0.8624 | val_acc=0.6187
Epoch 034 | loss=0.8528 | train_acc=0.6097 | val_loss=0.8677 | val_acc=0.6187
Epoch 035 | loss=0.8529 | train_acc=0.6093 | val_loss=0.8652 | val_acc=0.6187
Epoch 036 | loss=0.8525 | train_acc=0.6095 | val_loss=0.8535 | val_acc=0.6187
Epoch 037 | loss=0.8599 | train_acc=0.6092 | val_loss=0.9144 | val_acc=0.6187
Epoch 038 | loss=0.8678 | train_acc=0.6097 | val_loss=0.9807 | val_acc=0.6187
Epoch 039 | loss=0.9055 | train_acc=0.6099 | val_loss=0.8512 | val_acc=0.6179
Epoch 040 | loss=0.9261 | train_acc=0.6090 | val_loss=0.9355 | val_acc=0.6275
Epoch 041 | loss=0.9092 | train_acc=0.6013 | val_loss=0.9204 | val_acc=0.6194
Epoch 042 | loss=0.8865 | train_acc=0.6085 | val_loss=0.8701 | val_acc=0.6187
Epoch 043 | loss=0.8599 | train_acc=0.6097 | val_loss=0.8671 | val_acc=0.6187
Epoch 044 | loss=0.8490 | train_acc=0.6093 | val_loss=0.8622 | val_acc=0.6187
Epoch 045 | loss=0.8498 | train_acc=0.6096 | val_loss=0.8613 | val_acc=0.6187
Epoch 046 | loss=0.8432 | train_acc=0.6093 | val_loss=0.8421 | val_acc=0.6187
Epoch 047 | loss=0.8442 | train_acc=0.6096 | val_loss=0.8445 | val_acc=0.6187
Epoch 048 | loss=0.8412 | train_acc=0.6095 | val_loss=0.8314 | val_acc=0.6187
Epoch 049 | loss=0.8424 | train_acc=0.6095 | val_loss=0.8388 | val_acc=0.6187
Final Test Loss: 0.7402 | Test Accuracy: 0.6322
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.2037 | train_acc=0.6808 | val_loss=2.4677 | val_acc=0.2007
Epoch 001 | loss=1.2445 | train_acc=0.5463 | val_loss=1.0586 | val_acc=0.6135
Epoch 002 | loss=1.0973 | train_acc=0.5952 | val_loss=1.0453 | val_acc=0.6187
Epoch 003 | loss=1.0839 | train_acc=0.6022 | val_loss=1.0298 | val_acc=0.6187
Epoch 004 | loss=1.0264 | train_acc=0.6044 | val_loss=1.0022 | val_acc=0.6187
Epoch 005 | loss=0.9850 | train_acc=0.6086 | val_loss=0.9936 | val_acc=0.6031
Epoch 006 | loss=0.9575 | train_acc=0.6079 | val_loss=0.9272 | val_acc=0.6187
Epoch 007 | loss=0.8682 | train_acc=0.6082 | val_loss=0.9542 | val_acc=0.6187
Epoch 008 | loss=0.8625 | train_acc=0.6090 | val_loss=0.8542 | val_acc=0.6187
Epoch 009 | loss=0.8017 | train_acc=0.6089 | val_loss=0.8465 | val_acc=0.6187
Epoch 010 | loss=0.7810 | train_acc=0.6094 | val_loss=0.8428 | val_acc=0.6187
Epoch 011 | loss=0.7691 | train_acc=0.6095 | val_loss=0.8206 | val_acc=0.6187
Epoch 012 | loss=0.7566 | train_acc=0.6095 | val_loss=0.8492 | val_acc=0.6187
Epoch 013 | loss=0.7529 | train_acc=0.6097 | val_loss=0.8308 | val_acc=0.6187
Epoch 014 | loss=0.7460 | train_acc=0.6098 | val_loss=0.8263 | val_acc=0.6187
Epoch 015 | loss=0.7440 | train_acc=0.6098 | val_loss=0.8173 | val_acc=0.6187
Epoch 016 | loss=0.7371 | train_acc=0.6098 | val_loss=0.8150 | val_acc=0.6187
Epoch 017 | loss=0.7347 | train_acc=0.6100 | val_loss=0.8264 | val_acc=0.6187
Epoch 018 | loss=0.7339 | train_acc=0.6099 | val_loss=0.8488 | val_acc=0.6187
Epoch 019 | loss=0.7300 | train_acc=0.6098 | val_loss=0.8209 | val_acc=0.6187
Epoch 020 | loss=0.7268 | train_acc=0.6099 | val_loss=0.9579 | val_acc=0.6187
Epoch 021 | loss=0.7273 | train_acc=0.6097 | val_loss=0.8668 | val_acc=0.6187
Epoch 022 | loss=0.7308 | train_acc=0.6098 | val_loss=0.8411 | val_acc=0.6187
Epoch 023 | loss=0.7300 | train_acc=0.6095 | val_loss=0.8258 | val_acc=0.6946
Epoch 024 | loss=0.7332 | train_acc=0.6770 | val_loss=0.9124 | val_acc=0.5006
Epoch 025 | loss=0.7332 | train_acc=0.6901 | val_loss=0.8038 | val_acc=0.6753
Epoch 026 | loss=0.7269 | train_acc=0.6944 | val_loss=0.8581 | val_acc=0.6305
Epoch 027 | loss=0.7391 | train_acc=0.7015 | val_loss=0.8936 | val_acc=0.5320
Epoch 028 | loss=0.7335 | train_acc=0.6851 | val_loss=0.8111 | val_acc=0.6601
Epoch 029 | loss=0.7656 | train_acc=0.6708 | val_loss=0.7889 | val_acc=0.7257
Epoch 030 | loss=0.7434 | train_acc=0.7090 | val_loss=0.8950 | val_acc=0.6431
Epoch 031 | loss=0.7534 | train_acc=0.6904 | val_loss=0.7881 | val_acc=0.7094
Epoch 032 | loss=0.7377 | train_acc=0.6982 | val_loss=0.8686 | val_acc=0.5702
Epoch 033 | loss=0.7438 | train_acc=0.6797 | val_loss=0.9136 | val_acc=0.5009
Epoch 034 | loss=0.7456 | train_acc=0.6804 | val_loss=0.8101 | val_acc=0.6553
Epoch 035 | loss=0.7377 | train_acc=0.6738 | val_loss=0.7588 | val_acc=0.7157
Epoch 036 | loss=0.7261 | train_acc=0.6851 | val_loss=0.7474 | val_acc=0.7153
Epoch 037 | loss=0.7136 | train_acc=0.6871 | val_loss=0.7527 | val_acc=0.7120
Epoch 038 | loss=0.7283 | train_acc=0.6963 | val_loss=0.9733 | val_acc=0.4528
Epoch 039 | loss=0.7641 | train_acc=0.6961 | val_loss=0.8325 | val_acc=0.6531
Epoch 040 | loss=0.7708 | train_acc=0.7140 | val_loss=0.9801 | val_acc=0.4280
Epoch 041 | loss=0.7545 | train_acc=0.6817 | val_loss=0.8357 | val_acc=0.6379
Epoch 042 | loss=0.7518 | train_acc=0.6779 | val_loss=0.7659 | val_acc=0.7101
Epoch 043 | loss=0.7318 | train_acc=0.6852 | val_loss=0.7523 | val_acc=0.7116
Epoch 044 | loss=0.7211 | train_acc=0.6831 | val_loss=0.7704 | val_acc=0.7179
Epoch 045 | loss=0.7432 | train_acc=0.6916 | val_loss=0.7486 | val_acc=0.7101
Epoch 046 | loss=0.7384 | train_acc=0.6957 | val_loss=0.7616 | val_acc=0.7212
Epoch 047 | loss=0.8010 | train_acc=0.6991 | val_loss=0.7773 | val_acc=0.6994
Epoch 048 | loss=0.7347 | train_acc=0.6988 | val_loss=0.7891 | val_acc=0.6727
Epoch 049 | loss=0.7405 | train_acc=0.6715 | val_loss=0.7567 | val_acc=0.7068
Final Test Loss: 0.7099 | Test Accuracy: 0.7215
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.1476 | train_acc=0.6474 | val_loss=2.4228 | val_acc=0.1940
Epoch 001 | loss=1.3045 | train_acc=0.4934 | val_loss=1.0608 | val_acc=0.6183
Epoch 002 | loss=1.0761 | train_acc=0.5803 | val_loss=1.0482 | val_acc=0.6187
Epoch 003 | loss=1.0661 | train_acc=0.5964 | val_loss=1.0327 | val_acc=0.6187
Epoch 004 | loss=1.0267 | train_acc=0.6014 | val_loss=1.0231 | val_acc=0.6187
Epoch 005 | loss=1.0118 | train_acc=0.6035 | val_loss=1.0139 | val_acc=0.6187
Epoch 006 | loss=0.9887 | train_acc=0.6027 | val_loss=1.0129 | val_acc=0.6187
Epoch 007 | loss=0.9753 | train_acc=0.6032 | val_loss=1.0153 | val_acc=0.6187
Epoch 008 | loss=0.9686 | train_acc=0.6025 | val_loss=1.0174 | val_acc=0.6187
Epoch 009 | loss=0.9652 | train_acc=0.6029 | val_loss=1.0188 | val_acc=0.6187
Epoch 010 | loss=0.9638 | train_acc=0.6017 | val_loss=1.0195 | val_acc=0.6187
Epoch 011 | loss=0.9610 | train_acc=0.6033 | val_loss=1.0197 | val_acc=0.6187
Epoch 012 | loss=0.9606 | train_acc=0.6030 | val_loss=1.0197 | val_acc=0.6187
Epoch 013 | loss=0.9584 | train_acc=0.6028 | val_loss=1.0196 | val_acc=0.6187
Epoch 014 | loss=0.9602 | train_acc=0.6027 | val_loss=1.0199 | val_acc=0.6187
Epoch 015 | loss=0.9590 | train_acc=0.6021 | val_loss=1.0197 | val_acc=0.6187
Epoch 016 | loss=0.9580 | train_acc=0.6015 | val_loss=1.0191 | val_acc=0.6187
Epoch 017 | loss=0.9563 | train_acc=0.6028 | val_loss=1.0181 | val_acc=0.6187
Epoch 018 | loss=0.9556 | train_acc=0.6026 | val_loss=1.0175 | val_acc=0.6187
Epoch 019 | loss=0.9572 | train_acc=0.6038 | val_loss=0.9900 | val_acc=0.6187
Epoch 020 | loss=1.0776 | train_acc=0.5991 | val_loss=1.0374 | val_acc=0.4965
Epoch 021 | loss=1.1957 | train_acc=0.5990 | val_loss=1.0480 | val_acc=0.4617
Epoch 022 | loss=1.1099 | train_acc=0.5930 | val_loss=1.0255 | val_acc=0.6187
Epoch 023 | loss=0.9995 | train_acc=0.6016 | val_loss=1.0199 | val_acc=0.6187
Epoch 024 | loss=0.9875 | train_acc=0.6023 | val_loss=1.0159 | val_acc=0.6187
Epoch 025 | loss=1.0467 | train_acc=0.6056 | val_loss=1.0224 | val_acc=0.6135
Epoch 026 | loss=1.1505 | train_acc=0.5953 | val_loss=1.0117 | val_acc=0.6187
Epoch 027 | loss=1.2141 | train_acc=0.6049 | val_loss=1.0062 | val_acc=0.6164
Epoch 028 | loss=0.9983 | train_acc=0.5964 | val_loss=1.0121 | val_acc=0.6187
Epoch 029 | loss=0.9759 | train_acc=0.6028 | val_loss=1.0112 | val_acc=0.6187
Epoch 030 | loss=0.9751 | train_acc=0.6027 | val_loss=1.0110 | val_acc=0.6187
Epoch 031 | loss=0.9605 | train_acc=0.6036 | val_loss=1.0125 | val_acc=0.6187
Epoch 032 | loss=0.9562 | train_acc=0.6030 | val_loss=1.0133 | val_acc=0.6187
Epoch 033 | loss=0.9534 | train_acc=0.6024 | val_loss=1.0141 | val_acc=0.6187
Epoch 034 | loss=0.9507 | train_acc=0.6022 | val_loss=1.0135 | val_acc=0.6187
Epoch 035 | loss=0.9485 | train_acc=0.6024 | val_loss=1.0133 | val_acc=0.6187
Epoch 036 | loss=0.9466 | train_acc=0.6032 | val_loss=1.0121 | val_acc=0.6187
Epoch 037 | loss=0.9475 | train_acc=0.6015 | val_loss=1.0140 | val_acc=0.6187
Epoch 038 | loss=0.9479 | train_acc=0.6033 | val_loss=1.0174 | val_acc=0.6187
Epoch 039 | loss=0.9470 | train_acc=0.6018 | val_loss=1.0159 | val_acc=0.6187
Epoch 040 | loss=0.9466 | train_acc=0.6027 | val_loss=1.0133 | val_acc=0.6187
Epoch 041 | loss=0.9452 | train_acc=0.6029 | val_loss=1.0095 | val_acc=0.6187
Epoch 042 | loss=0.9435 | train_acc=0.6020 | val_loss=1.0036 | val_acc=0.6187
Epoch 043 | loss=0.9417 | train_acc=0.6030 | val_loss=1.0007 | val_acc=0.6187
Epoch 044 | loss=0.9410 | train_acc=0.6019 | val_loss=0.9987 | val_acc=0.6187
Epoch 045 | loss=0.9389 | train_acc=0.6019 | val_loss=0.9906 | val_acc=0.6187
Epoch 046 | loss=0.9431 | train_acc=0.6013 | val_loss=0.9984 | val_acc=0.6187
Epoch 047 | loss=0.9374 | train_acc=0.6021 | val_loss=0.9982 | val_acc=0.6187
Epoch 048 | loss=0.9362 | train_acc=0.6021 | val_loss=0.9996 | val_acc=0.6187
Epoch 049 | loss=0.9335 | train_acc=0.6033 | val_loss=1.0026 | val_acc=0.6187
Final Test Loss: 0.9091 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.2459 | train_acc=0.6534 | val_loss=1.8504 | val_acc=0.1918
Epoch 001 | loss=1.3138 | train_acc=0.5002 | val_loss=1.0640 | val_acc=0.6190
Epoch 002 | loss=1.0647 | train_acc=0.5898 | val_loss=1.0478 | val_acc=0.6187
Epoch 003 | loss=1.0447 | train_acc=0.6035 | val_loss=1.0304 | val_acc=0.6187
Epoch 004 | loss=1.0240 | train_acc=0.6079 | val_loss=1.0199 | val_acc=0.6187
Epoch 005 | loss=1.0046 | train_acc=0.6082 | val_loss=1.0128 | val_acc=0.6187
Epoch 006 | loss=0.9872 | train_acc=0.6084 | val_loss=1.0140 | val_acc=0.6187
Epoch 007 | loss=0.9762 | train_acc=0.6083 | val_loss=1.0167 | val_acc=0.6187
Epoch 008 | loss=0.9691 | train_acc=0.6079 | val_loss=1.0192 | val_acc=0.6187
Epoch 009 | loss=0.9654 | train_acc=0.6087 | val_loss=1.0206 | val_acc=0.6187
Epoch 010 | loss=0.9641 | train_acc=0.6088 | val_loss=1.0215 | val_acc=0.6187
Epoch 011 | loss=0.9630 | train_acc=0.6088 | val_loss=1.0220 | val_acc=0.6187
Epoch 012 | loss=0.9618 | train_acc=0.6081 | val_loss=1.0223 | val_acc=0.6187
Epoch 013 | loss=0.9623 | train_acc=0.6085 | val_loss=1.0228 | val_acc=0.6187
Epoch 014 | loss=0.9611 | train_acc=0.6084 | val_loss=1.0230 | val_acc=0.6187
Epoch 015 | loss=0.9600 | train_acc=0.6085 | val_loss=1.0232 | val_acc=0.6187
Epoch 016 | loss=0.9603 | train_acc=0.6077 | val_loss=1.0235 | val_acc=0.6187
Epoch 017 | loss=0.9588 | train_acc=0.6076 | val_loss=1.0152 | val_acc=0.6187
Epoch 018 | loss=0.9627 | train_acc=0.6084 | val_loss=1.0239 | val_acc=0.6187
Epoch 019 | loss=0.9618 | train_acc=0.6085 | val_loss=1.0237 | val_acc=0.6187
Epoch 020 | loss=0.9594 | train_acc=0.6082 | val_loss=1.0236 | val_acc=0.6187
Epoch 021 | loss=0.9573 | train_acc=0.6080 | val_loss=1.0230 | val_acc=0.6187
Epoch 022 | loss=0.9579 | train_acc=0.6083 | val_loss=1.0219 | val_acc=0.6187
Epoch 023 | loss=0.9588 | train_acc=0.6082 | val_loss=1.0212 | val_acc=0.6187
Epoch 024 | loss=0.9585 | train_acc=0.6080 | val_loss=1.0240 | val_acc=0.6187
Epoch 025 | loss=0.9601 | train_acc=0.6084 | val_loss=1.0243 | val_acc=0.6187
Epoch 026 | loss=0.9555 | train_acc=0.6087 | val_loss=1.0217 | val_acc=0.6187
Epoch 027 | loss=0.9558 | train_acc=0.6085 | val_loss=1.0229 | val_acc=0.6187
Epoch 028 | loss=0.9565 | train_acc=0.6079 | val_loss=0.9948 | val_acc=0.6187
Epoch 029 | loss=1.0119 | train_acc=0.6083 | val_loss=1.1208 | val_acc=0.4158
Epoch 030 | loss=1.1019 | train_acc=0.6047 | val_loss=1.0878 | val_acc=0.3876
Epoch 031 | loss=1.1890 | train_acc=0.6052 | val_loss=1.0106 | val_acc=0.6187
Epoch 032 | loss=1.1432 | train_acc=0.6069 | val_loss=1.0327 | val_acc=0.5024
Epoch 033 | loss=1.1556 | train_acc=0.6088 | val_loss=1.1016 | val_acc=0.4398
Epoch 034 | loss=1.1109 | train_acc=0.6002 | val_loss=1.0100 | val_acc=0.6187
Epoch 035 | loss=1.1030 | train_acc=0.6086 | val_loss=1.0056 | val_acc=0.6187
Epoch 036 | loss=1.2590 | train_acc=0.6077 | val_loss=1.0147 | val_acc=0.6187
Epoch 037 | loss=1.2368 | train_acc=0.6072 | val_loss=1.0132 | val_acc=0.6187
Epoch 038 | loss=1.0054 | train_acc=0.6080 | val_loss=1.0116 | val_acc=0.6187
Epoch 039 | loss=0.9779 | train_acc=0.6080 | val_loss=1.0106 | val_acc=0.6187
Epoch 040 | loss=0.9638 | train_acc=0.6085 | val_loss=1.0114 | val_acc=0.6187
Epoch 041 | loss=0.9578 | train_acc=0.6083 | val_loss=1.0129 | val_acc=0.6187
Epoch 042 | loss=0.9542 | train_acc=0.6084 | val_loss=1.0146 | val_acc=0.6187
Epoch 043 | loss=0.9508 | train_acc=0.6084 | val_loss=1.0160 | val_acc=0.6187
Epoch 044 | loss=0.9500 | train_acc=0.6085 | val_loss=1.0169 | val_acc=0.6187
Epoch 045 | loss=0.9466 | train_acc=0.6080 | val_loss=1.0175 | val_acc=0.6187
Epoch 046 | loss=0.9469 | train_acc=0.6088 | val_loss=1.0176 | val_acc=0.6187
Epoch 047 | loss=0.9453 | train_acc=0.6077 | val_loss=1.0180 | val_acc=0.6187
Epoch 048 | loss=0.9434 | train_acc=0.6082 | val_loss=1.0165 | val_acc=0.6187
Epoch 049 | loss=0.9430 | train_acc=0.6086 | val_loss=1.0166 | val_acc=0.6187
Final Test Loss: 0.9128 | Test Accuracy: 0.6316
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.0756 | train_acc=0.6802 | val_loss=3.6571 | val_acc=0.1933
Epoch 001 | loss=1.3745 | train_acc=0.5024 | val_loss=1.0583 | val_acc=0.6187
Epoch 002 | loss=1.0700 | train_acc=0.5969 | val_loss=1.0367 | val_acc=0.6187
Epoch 003 | loss=1.0292 | train_acc=0.6075 | val_loss=1.0184 | val_acc=0.6187
Epoch 004 | loss=1.0097 | train_acc=0.6099 | val_loss=1.0073 | val_acc=0.6187
Epoch 005 | loss=0.9791 | train_acc=0.6092 | val_loss=1.0197 | val_acc=0.6187
Epoch 006 | loss=1.0066 | train_acc=0.6092 | val_loss=1.0054 | val_acc=0.6187
Epoch 007 | loss=0.9922 | train_acc=0.6095 | val_loss=1.0080 | val_acc=0.6187
Epoch 008 | loss=0.9779 | train_acc=0.6096 | val_loss=1.0097 | val_acc=0.6187
Epoch 009 | loss=0.9632 | train_acc=0.6093 | val_loss=1.0075 | val_acc=0.6187
Epoch 010 | loss=0.9576 | train_acc=0.6097 | val_loss=1.0004 | val_acc=0.6187
Epoch 011 | loss=0.9494 | train_acc=0.6094 | val_loss=0.9894 | val_acc=0.6187
Epoch 012 | loss=0.9298 | train_acc=0.6096 | val_loss=0.9851 | val_acc=0.6187
Epoch 013 | loss=1.0358 | train_acc=0.6076 | val_loss=0.9961 | val_acc=0.6187
Epoch 014 | loss=0.9904 | train_acc=0.6091 | val_loss=1.0111 | val_acc=0.6187
Epoch 015 | loss=0.9900 | train_acc=0.6093 | val_loss=1.0022 | val_acc=0.6187
Epoch 016 | loss=0.9554 | train_acc=0.6097 | val_loss=0.9978 | val_acc=0.6187
Epoch 017 | loss=0.9386 | train_acc=0.6093 | val_loss=0.9575 | val_acc=0.6187
Epoch 018 | loss=0.8976 | train_acc=0.6094 | val_loss=0.9669 | val_acc=0.6187
Epoch 019 | loss=0.8961 | train_acc=0.6094 | val_loss=0.9580 | val_acc=0.6187
Epoch 020 | loss=0.8895 | train_acc=0.6094 | val_loss=0.9509 | val_acc=0.6187
Epoch 021 | loss=0.8842 | train_acc=0.6090 | val_loss=0.9482 | val_acc=0.6187
Epoch 022 | loss=0.8985 | train_acc=0.6091 | val_loss=0.9331 | val_acc=0.6187
Epoch 023 | loss=0.8897 | train_acc=0.6081 | val_loss=0.9021 | val_acc=0.6187
Epoch 024 | loss=0.8817 | train_acc=0.6068 | val_loss=0.8848 | val_acc=0.6187
Epoch 025 | loss=0.8627 | train_acc=0.6082 | val_loss=0.8686 | val_acc=0.6187
Epoch 026 | loss=0.8414 | train_acc=0.6079 | val_loss=0.8625 | val_acc=0.6187
Epoch 027 | loss=0.8263 | train_acc=0.6098 | val_loss=0.8550 | val_acc=0.6187
Epoch 028 | loss=0.8199 | train_acc=0.6096 | val_loss=0.8375 | val_acc=0.6187
Epoch 029 | loss=0.8145 | train_acc=0.6090 | val_loss=0.8340 | val_acc=0.6187
Epoch 030 | loss=0.8137 | train_acc=0.6092 | val_loss=0.8302 | val_acc=0.6187
Epoch 031 | loss=0.8246 | train_acc=0.6088 | val_loss=0.8140 | val_acc=0.6187
Epoch 032 | loss=0.8488 | train_acc=0.6117 | val_loss=0.9285 | val_acc=0.4857
Epoch 033 | loss=0.8620 | train_acc=0.6148 | val_loss=0.9025 | val_acc=0.5143
Epoch 034 | loss=0.8631 | train_acc=0.6198 | val_loss=0.8708 | val_acc=0.5713
Epoch 035 | loss=0.8648 | train_acc=0.6421 | val_loss=0.8270 | val_acc=0.6794
Epoch 036 | loss=0.8350 | train_acc=0.6428 | val_loss=0.8603 | val_acc=0.5831
Epoch 037 | loss=0.8165 | train_acc=0.6572 | val_loss=0.8656 | val_acc=0.5424
Epoch 038 | loss=0.8324 | train_acc=0.6056 | val_loss=0.8180 | val_acc=0.6398
Epoch 039 | loss=0.8238 | train_acc=0.6700 | val_loss=0.8448 | val_acc=0.5798
Epoch 040 | loss=0.8717 | train_acc=0.6385 | val_loss=0.9023 | val_acc=0.4939
Epoch 041 | loss=0.9112 | train_acc=0.5797 | val_loss=0.7675 | val_acc=0.7253
Epoch 042 | loss=0.7893 | train_acc=0.7267 | val_loss=0.9191 | val_acc=0.4843
Epoch 043 | loss=0.8303 | train_acc=0.6474 | val_loss=0.7179 | val_acc=0.7253
Epoch 044 | loss=0.7934 | train_acc=0.6910 | val_loss=0.7030 | val_acc=0.7234
Epoch 045 | loss=0.8039 | train_acc=0.6749 | val_loss=0.7263 | val_acc=0.7205
Epoch 046 | loss=0.8331 | train_acc=0.6803 | val_loss=0.7193 | val_acc=0.7164
Epoch 047 | loss=0.8375 | train_acc=0.6111 | val_loss=0.7603 | val_acc=0.7175
Epoch 048 | loss=0.8269 | train_acc=0.6739 | val_loss=0.7530 | val_acc=0.7197
Epoch 049 | loss=0.8308 | train_acc=0.5879 | val_loss=1.0303 | val_acc=0.5861
Final Test Loss: 1.0099 | Test Accuracy: 0.5952
Unique train labels: tensor([0, 1, 2])
Training with num_classes: 3
Train Samples: 20232
Val Samples: 2601
Test Samples: 6071
cuda
Epoch 000 | loss=1.1540 | train_acc=0.6818 | val_loss=1.9992 | val_acc=0.1529
Epoch 001 | loss=1.3377 | train_acc=0.4992 | val_loss=1.0691 | val_acc=0.6187
Epoch 002 | loss=1.0826 | train_acc=0.5808 | val_loss=1.0599 | val_acc=0.6183
Epoch 003 | loss=1.0435 | train_acc=0.6000 | val_loss=1.0381 | val_acc=0.6187
Epoch 004 | loss=1.0225 | train_acc=0.6095 | val_loss=1.0191 | val_acc=0.6187
Epoch 005 | loss=1.0020 | train_acc=0.6099 | val_loss=1.0096 | val_acc=0.6187
Epoch 006 | loss=0.9909 | train_acc=0.6098 | val_loss=1.0007 | val_acc=0.6187
Epoch 007 | loss=0.9757 | train_acc=0.6099 | val_loss=0.9963 | val_acc=0.6187
Epoch 008 | loss=0.9583 | train_acc=0.6099 | val_loss=0.9780 | val_acc=0.6187
Epoch 009 | loss=0.9481 | train_acc=0.6097 | val_loss=0.9303 | val_acc=0.6187
Epoch 010 | loss=0.9202 | train_acc=0.6099 | val_loss=0.9266 | val_acc=0.6187
Epoch 011 | loss=0.9233 | train_acc=0.6097 | val_loss=0.8984 | val_acc=0.6187
Epoch 012 | loss=0.8915 | train_acc=0.6099 | val_loss=0.9109 | val_acc=0.6187
Epoch 013 | loss=0.9021 | train_acc=0.6099 | val_loss=0.8727 | val_acc=0.6187
Epoch 014 | loss=0.8630 | train_acc=0.6100 | val_loss=0.8743 | val_acc=0.6187
Epoch 015 | loss=0.8630 | train_acc=0.6100 | val_loss=0.9506 | val_acc=0.6187
Epoch 016 | loss=0.8940 | train_acc=0.6098 | val_loss=0.8679 | val_acc=0.6187
Epoch 017 | loss=0.8307 | train_acc=0.6098 | val_loss=0.9224 | val_acc=0.6187
Epoch 018 | loss=0.8605 | train_acc=0.6100 | val_loss=0.8638 | val_acc=0.6187
Epoch 019 | loss=0.8303 | train_acc=0.6099 | val_loss=0.9322 | val_acc=0.6187
Epoch 020 | loss=0.8565 | train_acc=0.6099 | val_loss=0.8773 | val_acc=0.6187
Epoch 021 | loss=0.8298 | train_acc=0.6098 | val_loss=0.8956 | val_acc=0.6187
Epoch 022 | loss=0.8327 | train_acc=0.6098 | val_loss=0.9217 | val_acc=0.6187
Epoch 023 | loss=0.8785 | train_acc=0.6100 | val_loss=0.8559 | val_acc=0.6187
Epoch 024 | loss=1.1767 | train_acc=0.6128 | val_loss=1.1414 | val_acc=0.3480
Epoch 025 | loss=0.9911 | train_acc=0.6448 | val_loss=3.2888 | val_acc=0.2455
Epoch 026 | loss=1.1946 | train_acc=0.6001 | val_loss=1.0105 | val_acc=0.6187
Epoch 027 | loss=1.0157 | train_acc=0.6094 | val_loss=1.0032 | val_acc=0.6187
Epoch 028 | loss=0.9709 | train_acc=0.6099 | val_loss=0.9840 | val_acc=0.6187
Epoch 029 | loss=0.9427 | train_acc=0.6098 | val_loss=0.9680 | val_acc=0.6187
Epoch 030 | loss=0.9293 | train_acc=0.6099 | val_loss=0.9555 | val_acc=0.6187
Epoch 031 | loss=0.9260 | train_acc=0.6098 | val_loss=0.8978 | val_acc=0.6187
Epoch 032 | loss=0.8904 | train_acc=0.6099 | val_loss=0.9185 | val_acc=0.6183
Epoch 033 | loss=0.9135 | train_acc=0.6098 | val_loss=0.8529 | val_acc=0.6187
Epoch 034 | loss=0.8643 | train_acc=0.6095 | val_loss=0.8881 | val_acc=0.5765
Epoch 035 | loss=0.8624 | train_acc=0.6095 | val_loss=0.8852 | val_acc=0.6183
