{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "663caec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/remirand/bind_gps_env/lib64/python3.9/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/users/remirand/bind_gps_env/lib64/python3.9/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mromer_miranda\u001b[0m (\u001b[33mbind-gps\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/oscar/data/larschan/shared_data/BindGPS/model/wandb/run-20251026_193207-8xq0jrxl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bind-gps/basic-gnn/runs/8xq0jrxl' target=\"_blank\">GNN testing</a></strong> to <a href='https://wandb.ai/bind-gps/basic-gnn' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bind-gps/basic-gnn' target=\"_blank\">https://wandb.ai/bind-gps/basic-gnn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bind-gps/basic-gnn/runs/8xq0jrxl' target=\"_blank\">https://wandb.ai/bind-gps/basic-gnn/runs/8xq0jrxl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique train labels: tensor([0, 1, 2])\n",
      "Training with num_classes: 3\n",
      "Train Samples: 20232\n",
      "Val Samples: 2601\n",
      "Test Samples: 6071\n",
      "cpu\n",
      "Epoch 000 | loss=1.1528 | train_acc=0.6096 | val_loss=1.0494 | val_acc=0.6187\n",
      "Epoch 001 | loss=1.0343 | train_acc=0.6097 | val_loss=1.0243 | val_acc=0.6187\n",
      "Epoch 002 | loss=1.0167 | train_acc=0.6100 | val_loss=1.0183 | val_acc=0.6187\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 492\u001b[0m\n\u001b[1;32m    489\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 492\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 489\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    482\u001b[0m cfg \u001b[38;5;241m=\u001b[39m Config(\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;66;03m# toggle this on to log to W&B (requires `wandb login`)\u001b[39;00m\n\u001b[1;32m    484\u001b[0m     use_wandb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m    485\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,  \u001b[38;5;66;03m# Using default seed for reproducibility\u001b[39;00m\n\u001b[1;32m    486\u001b[0m     val_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m  \u001b[38;5;66;03m# Enable batched validation\u001b[39;00m\n\u001b[1;32m    487\u001b[0m )\n\u001b[1;32m    488\u001b[0m trainer \u001b[38;5;241m=\u001b[39m GNNTrainer(cfg)\n\u001b[0;32m--> 489\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 474\u001b[0m, in \u001b[0;36mGNNTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 474\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwandb_run \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    476\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwandb_run\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[0;32mIn[1], line 466\u001b[0m, in \u001b[0;36mGNNTrainer.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m--> 466\u001b[0m     loss, acc, val_loss, val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | train_acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | val_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | val_acc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    469\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, loader\u001b[38;5;241m=\u001b[39mtest_loader)\n",
      "Cell \u001b[0;32mIn[1], line 360\u001b[0m, in \u001b[0;36mGNNTrainer._train_one_epoch\u001b[0;34m(self, train_loader, val_loader, epoch)\u001b[0m\n\u001b[1;32m    358\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(batch\u001b[38;5;241m.\u001b[39mx, batch\u001b[38;5;241m.\u001b[39medge_index, edge_attr\u001b[38;5;241m=\u001b[39medge_attr)  \u001b[38;5;66;03m# [N_batch, C]\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 360\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [N_batch, C]\u001b[39;00m\n\u001b[1;32m    361\u001b[0m mask \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mtrain_mask\u001b[38;5;241m.\u001b[39mbool()\n\u001b[1;32m    362\u001b[0m targets \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mlong)\n",
      "File \u001b[0;32m~/bind_gps_env/lib64/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bind_gps_env/lib64/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/oscar/data/larschan/shared_data/BindGPS/model/src/models.py:186\u001b[0m, in \u001b[0;36mGCN.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m    184\u001b[0m     x \u001b[38;5;241m=\u001b[39m conv(x,edge_index)\n\u001b[1;32m    185\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m--> 186\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# linear layers\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,lin \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear):\n",
      "File \u001b[0;32m~/bind_gps_env/lib64/python3.9/site-packages/torch/nn/functional.py:1422\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1422\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1423\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train_gnn.py\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Project modules\n",
    "from src.data import DatasetLoader, GraphParamBuilder\n",
    "from src.models import GCN, GATModel\n",
    "\n",
    "# PyG\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "# Optional: Weights & Biases\n",
    "try:\n",
    "    import wandb\n",
    "    WANDB_AVAILABLE = True\n",
    "except Exception:\n",
    "    WANDB_AVAILABLE = False\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Paths\n",
    "    # base_path: str = \"/home/jchc/Documents/larschan_laboratory/BindGPS/data/datasets\"\n",
    "    base_path: str = \"/gpfs/data/larschan/shared_data/BindGPS/data/datasets/\"\n",
    "\n",
    "    # Data\n",
    "    p_value: str = \"0_1\"\n",
    "    resolution: str = \"1kb\"\n",
    "    exclude_features: Tuple[str, ...] = (\"clamp\", \"gaf\", \"psq\")\n",
    "    features_of_interest: Tuple[str, ...] = (\n",
    "        \"clamp\",\"gaf\",\"psq\",\"h3k27ac\",\"h3k27me3\",\"h3k36me3\",\n",
    "        \"h3k4me1\",\"h3k4me2\",\"h3k4me3\",\"h3k9me3\",\"h4k16ac\"\n",
    "    )\n",
    "    target_column: str = \"mre_labels\"\n",
    "    train_size: float = 0.7\n",
    "    non_mre_size: float = 0.3\n",
    "    seed: int = 42\n",
    "\n",
    "    # Model\n",
    "    model_type: str = \"gcn\"  # \"gcn\" or \"gat\"\n",
    "    hidden_gnn_size: int = 128\n",
    "    num_gnn_layers: int = 3\n",
    "    hidden_linear_size: int = 128\n",
    "    num_linear_layers: int = 3\n",
    "    dropout: float = 0.5\n",
    "    normalize: bool = True\n",
    "    \n",
    "    # GAT-specific parameters\n",
    "    gat_heads: int = 4\n",
    "    gat_negative_slope: float = 0.2\n",
    "    gat_concat: bool = True\n",
    "    gat_edge_dim: int = 2  # contactCount + loop_size_transformed\n",
    "\n",
    "    # Optimization\n",
    "    lr: float = 5e-4\n",
    "    weight_decay: float = 5e-4\n",
    "    epochs: int = 25\n",
    "\n",
    "    # NeighborLoader\n",
    "    num_neighbors: Tuple[int, int, int] = (20, 20, 20)\n",
    "    batch_size: int = 256\n",
    "    num_workers: int = 1\n",
    "    \n",
    "    # Batch validation (processes all edges)\n",
    "    val_batch_size: Optional[int] = 1024  # If None, uses full graph evaluation\n",
    "\n",
    "    # Device / perf\n",
    "    use_cuda_if_available: bool = True\n",
    "\n",
    "    # Logging\n",
    "    use_wandb: bool = False\n",
    "    wandb_project: str = \"basic-gnn\"\n",
    "    wandb_entity: Optional[str] = None  # or your entity string\n",
    "\n",
    "# ----------------------------\n",
    "# Utilities\n",
    "# ----------------------------\n",
    "def set_global_seed(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # For more determinism (may reduce performance slightly)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def get_device(use_cuda_if_available: bool = True) -> torch.device:\n",
    "    if use_cuda_if_available and torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "# ----------------------------\n",
    "# Trainer\n",
    "# ----------------------------\n",
    "class GNNTrainer:\n",
    "    def __init__(self, cfg: Config):\n",
    "        self.cfg = cfg\n",
    "        set_global_seed(cfg.seed)\n",
    "        self.device = get_device(cfg.use_cuda_if_available)\n",
    "\n",
    "        # Will be populated later\n",
    "        self.node_df: Optional[pd.DataFrame] = None\n",
    "        self.edge_df: Optional[pd.DataFrame] = None\n",
    "        self.data: Optional[Data] = None\n",
    "        self.model: Optional[torch.nn.Module] = None\n",
    "        self.optimizer: Optional[torch.optim.Optimizer] = None\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        # W&B\n",
    "        self.wandb_run = None\n",
    "        if self.cfg.use_wandb and WANDB_AVAILABLE:\n",
    "            self._init_wandb()\n",
    "\n",
    "    # ----- Logging -----\n",
    "    def _init_wandb(self) -> None:\n",
    "        try:\n",
    "            wandb.login()\n",
    "            self.wandb_run = wandb.init(\n",
    "                project=self.cfg.wandb_project,\n",
    "                entity=self.cfg.wandb_entity,\n",
    "                config=asdict(self.cfg),\n",
    "                name=\"GNN testing\",\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] W&B init failed: {e}\")\n",
    "            self.wandb_run = None\n",
    "\n",
    "    def _log(self, metrics: dict, step: Optional[int] = None) -> None:\n",
    "        if self.wandb_run is not None:\n",
    "            wandb.log(metrics, step=step)\n",
    "\n",
    "    # ----- Data -----\n",
    "    def build_dataset(self) -> None:\n",
    "        # Load raw dataframes\n",
    "        loader = DatasetLoader(base_path=self.cfg.base_path)\n",
    "        node_df, edge_df = loader.load(p_value=self.cfg.p_value, resolution=self.cfg.resolution)\n",
    "\n",
    "        self.node_df = node_df.copy()\n",
    "        self.edge_df = edge_df.copy()\n",
    "\n",
    "        # Feature selection\n",
    "        feats = [f for f in self.cfg.features_of_interest if f not in self.cfg.exclude_features]\n",
    "        input_features = self.node_df.loc[:, feats].copy()\n",
    "\n",
    "        # Target and mask (mre > 0 considered labeled)\n",
    "        target = self.node_df[self.cfg.target_column].copy()\n",
    "        self.node_df[\"mre_mask\"] = self.node_df[self.cfg.target_column].apply(lambda x: True if x > 0 else False)\n",
    "        mre_mask = self.node_df[\"mre_mask\"].astype(bool)\n",
    "\n",
    "        total_mre_samples = mre_mask.sum()\n",
    "        self.node_df[\"non_mre_mask\"] = self.node_df[self.cfg.target_column].apply(lambda x: True if x == 0 else False)\n",
    "        non_mre_mask = self.node_df[\"non_mre_mask\"].astype(bool)\n",
    "\n",
    "\n",
    "        # Randomly select non-MRE samples based on defined size. \n",
    "        non_mre_samples = total_mre_samples * self.cfg.non_mre_size\n",
    "        indices = np.arange(len(non_mre_mask))\n",
    "        non_mre_indices = indices[non_mre_mask]\n",
    "        selected_non_mres = np.random.choice(non_mre_indices, size=int(non_mre_samples), replace=False)\n",
    "        non_mre_mask = np.zeros(len(non_mre_mask), dtype=bool)\n",
    "        non_mre_mask[selected_non_mres] = True\n",
    "        self.node_df[\"non_mre_mask\"] = non_mre_mask\n",
    "\n",
    "        # Combine MRE and non-MRE Samples\n",
    "        mask = mre_mask | non_mre_mask\n",
    "\n",
    "        # Stratified split on masked subset\n",
    "        X_train, X_evaluation, y_train, y_evaluation = train_test_split(\n",
    "            input_features[mask],\n",
    "            target[mask],\n",
    "            train_size=self.cfg.train_size,\n",
    "            stratify=target[mask],\n",
    "            random_state=self.cfg.seed,\n",
    "        )\n",
    "\n",
    "        X_val, X_test, y_val, y_test = train_test_split(\n",
    "            X_evaluation,\n",
    "            y_evaluation,\n",
    "            train_size=0.3,\n",
    "            stratify=y_evaluation,\n",
    "            random_state=self.cfg.seed,\n",
    "        )\n",
    "\n",
    "        # Build boolean masks over the FULL index\n",
    "        train_mask = pd.Series(False, index=input_features.index)\n",
    "        val_mask = pd.Series(False, index=input_features.index)\n",
    "        test_mask = pd.Series(False, index=input_features.index)\n",
    "        train_mask.loc[X_train.index] = True\n",
    "        val_mask.loc[X_val.index] = True\n",
    "        test_mask.loc[X_test.index] = True\n",
    "\n",
    "        # Build tensors via your helper\n",
    "        builder = GraphParamBuilder(\n",
    "            node_df=self.node_df,\n",
    "            edge_df=self.edge_df,\n",
    "            target=target,\n",
    "            mask=mask,\n",
    "            input_features=input_features,\n",
    "            seed=self.cfg.seed,\n",
    "        )\n",
    "        tensors = builder.convert_to_tensors()\n",
    "\n",
    "        # Ensure dtypes\n",
    "        X = tensors[\"X\"]                          # [N, F] float\n",
    "        y = tensors[\"y\"].to(torch.long)           # [N] long for CE loss\n",
    "        edge_index = tensors[\"edge_index\"]        # [2, E]\n",
    "        \n",
    "        # Edge weights for NeighborSampler (use p-value transformed)\n",
    "        edge_weight = tensors[\"edge_pvalue_transformed\"]  # [E]\n",
    "        \n",
    "        # Edge features (contact count + loop size transformed)\n",
    "        edge_attr = torch.stack([\n",
    "            tensors[\"edge_contactCount\"],\n",
    "            tensors[\"edge_loop_size_transformed\"]\n",
    "        ], dim=1)  # [E, 2] - 2 edge features\n",
    "\n",
    "        pyg_data = Data(\n",
    "            x=X,\n",
    "            y=y,\n",
    "            edge_index=edge_index,\n",
    "            edge_weight=edge_weight,\n",
    "            edge_attr=edge_attr,\n",
    "            train_mask=torch.tensor(train_mask.to_numpy(), dtype=torch.bool),\n",
    "            val_mask=torch.tensor(val_mask.to_numpy(), dtype=torch.bool),\n",
    "            test_mask=torch.tensor(test_mask.to_numpy(), dtype=torch.bool),\n",
    "        )\n",
    "        self.data = pyg_data\n",
    "\n",
    "    def build_loaders(self) -> Tuple[NeighborLoader, NeighborLoader, NeighborLoader]:\n",
    "        assert self.data is not None, \"Call build_dataset() first.\"\n",
    "        \n",
    "        # Training loader with neighbor sampling\n",
    "        train_loader = NeighborLoader(\n",
    "            self.data,\n",
    "            input_nodes=self.data.train_mask,\n",
    "            num_neighbors=list(self.cfg.num_neighbors),\n",
    "            batch_size=self.cfg.batch_size,\n",
    "            weight_attr=\"edge_weight\",\n",
    "            num_workers=self.cfg.num_workers,\n",
    "            pin_memory=self.device.type == \"cuda\",\n",
    "        )\n",
    "        \n",
    "        # Validation loader with ALL neighbors (-1 means no sampling limit)\n",
    "        val_loader = NeighborLoader(\n",
    "            self.data,\n",
    "            input_nodes=self.data.val_mask,\n",
    "            num_neighbors=[-1] * len(self.cfg.num_neighbors),  # Sample ALL neighbors\n",
    "            batch_size=self.cfg.val_batch_size if self.cfg.val_batch_size else self.cfg.batch_size,\n",
    "            weight_attr=\"edge_weight\",\n",
    "            num_workers=self.cfg.num_workers,\n",
    "            pin_memory=self.device.type == \"cuda\",\n",
    "        )\n",
    "        \n",
    "        # Test loader with ALL neighbors\n",
    "        test_loader = NeighborLoader(\n",
    "            self.data,\n",
    "            input_nodes=self.data.test_mask,\n",
    "            num_neighbors=[-1] * len(self.cfg.num_neighbors),  # Sample ALL neighbors\n",
    "            batch_size=self.cfg.val_batch_size if self.cfg.val_batch_size else self.cfg.batch_size,\n",
    "            weight_attr=\"edge_weight\",\n",
    "            num_workers=self.cfg.num_workers,\n",
    "            pin_memory=self.device.type == \"cuda\",\n",
    "        )\n",
    "        \n",
    "        return train_loader, val_loader, test_loader\n",
    "\n",
    "    # ----- Model / Optim -----\n",
    "    def build_model(self) -> None:\n",
    "        assert self.data is not None, \"Call build_dataset() first.\"\n",
    "\n",
    "        # Safer class count (works even if labels aren't 0..C-1)\n",
    "        train_labels = self.data.y[self.data.train_mask]\n",
    "        unique_labels = torch.unique(train_labels)\n",
    "        num_classes = int(unique_labels.numel())\n",
    "        \n",
    "        print(f\"Unique train labels: {unique_labels}\")\n",
    "        print(f\"Training with num_classes: {num_classes}\")\n",
    "        print(f\"Train Samples: {self.data.train_mask.sum()}\")\n",
    "        print(f\"Val Samples: {self.data.val_mask.sum()}\")\n",
    "        print(f\"Test Samples: {self.data.test_mask.sum()}\")\n",
    "        \n",
    "        # Check if labels need remapping to 0..C-1 range, will happen when we exclude non-mres\n",
    "        if unique_labels.min() != 0 or unique_labels.max() != (num_classes - 1):\n",
    "            print(f\"Warning: Labels not in 0..{num_classes-1} range, remapping...\")\n",
    "            # Create mapping from original labels to 0..C-1\n",
    "            label_mapping = {int(old_label): new_label for new_label, old_label in enumerate(unique_labels)}\n",
    "            print(f\"Label mapping: {label_mapping}\")\n",
    "            \n",
    "            # Remap all labels in the dataset\n",
    "            for old_label, new_label in label_mapping.items():\n",
    "                self.data.y[self.data.y == old_label] = new_label\n",
    "\n",
    "\n",
    "        if self.cfg.model_type.lower() == \"gcn\":\n",
    "            self.model = GCN(\n",
    "                in_channels=self.data.x.size(1),\n",
    "                out_channels=num_classes,\n",
    "                hidden_gnn_size=self.cfg.hidden_gnn_size,\n",
    "                num_gnn_layers=self.cfg.num_gnn_layers,\n",
    "                hidden_linear_size=self.cfg.hidden_linear_size,\n",
    "                num_linear_layers=self.cfg.num_linear_layers,\n",
    "                dropout=self.cfg.dropout,\n",
    "                normalize=self.cfg.normalize,\n",
    "            ).to(self.device)\n",
    "        elif self.cfg.model_type.lower() == \"gat\":\n",
    "            self.model = GATModel(\n",
    "                in_channels=self.data.x.size(1),\n",
    "                out_channels=num_classes,\n",
    "                hidden_gnn_size=self.cfg.hidden_gnn_size,\n",
    "                num_gnn_layers=self.cfg.num_gnn_layers,\n",
    "                hidden_linear_size=self.cfg.hidden_linear_size,\n",
    "                num_linear_layers=self.cfg.num_linear_layers,\n",
    "                heads=self.cfg.gat_heads,\n",
    "                concat=self.cfg.gat_concat,\n",
    "                negative_slope=self.cfg.gat_negative_slope,\n",
    "                dropout=self.cfg.dropout,\n",
    "                edge_dim=self.cfg.gat_edge_dim,\n",
    "            ).to(self.device)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model_type: {self.cfg.model_type}. Use 'gcn' or 'gat'.\")\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            self.model.parameters(),\n",
    "            lr=self.cfg.lr,\n",
    "            weight_decay=self.cfg.weight_decay,\n",
    "        )\n",
    "\n",
    "    # ----- Train / Eval -----\n",
    "    def _train_one_epoch(self, train_loader: NeighborLoader, val_loader: NeighborLoader, epoch: int) -> Tuple[float, float, float, float]:\n",
    "        assert self.model is not None and self.optimizer is not None\n",
    "\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Handle different model types\n",
    "            if self.cfg.model_type.lower() == \"gat\":\n",
    "                # GAT models can use edge attributes if available\n",
    "                edge_attr = getattr(batch, 'edge_attr', None)\n",
    "                out = self.model(batch.x, batch.edge_index, edge_attr=edge_attr)  # [N_batch, C]\n",
    "            else:\n",
    "                out = self.model(batch.x, batch.edge_index)  # [N_batch, C]\n",
    "            mask = batch.train_mask.bool()\n",
    "            targets = batch.y.to(torch.long)\n",
    "\n",
    "            loss = self.criterion(out[mask], targets[mask])\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += float(loss.detach().item())\n",
    "            preds = out[mask].detach().argmax(dim=1)\n",
    "            correct += int((preds == targets[mask]).sum().item())\n",
    "            total += int(mask.sum().item())\n",
    "            \n",
    "            # Clear intermediate variables to free GPU memory\n",
    "            del out, loss, preds\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        epoch_loss = total_loss / max(len(train_loader), 1)\n",
    "        epoch_acc = correct / max(total, 1)\n",
    "\n",
    "        self._log({\"train/loss\": epoch_loss, \"train/acc\": epoch_acc, \"epoch\": epoch}, step=epoch)\n",
    "\n",
    "        # evaluate on validation dataset using validation loader\n",
    "        val_loss, val_acc = self.evaluate(split=\"val\", loader=val_loader)\n",
    "        # self._log({\"val/loss\": val_loss, \"val/acc\": val_acc, \"epoch\": epoch}, step=epoch)\n",
    "        \n",
    "        return epoch_loss, epoch_acc, val_loss, val_acc\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, split=\"test\", loader=None) -> Tuple[float, float]:\n",
    "        \"\"\"Evaluate model using NeighborLoader with all neighbors.\n",
    "        \n",
    "        Args:\n",
    "            split: \"test\" or \"val\" to specify which nodes to evaluate\n",
    "            loader: NeighborLoader to use for evaluation (required)\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (loss, accuracy)\n",
    "        \"\"\"\n",
    "        assert self.model is not None and self.data is not None\n",
    "        assert loader is not None, \"NeighborLoader is required for evaluation\"\n",
    "        \n",
    "        self.model.eval()\n",
    "        return self._evaluate_with_loader(loader, split)\n",
    "    \n",
    "    \n",
    "    def _evaluate_with_loader(self, loader: NeighborLoader, split: str) -> Tuple[float, float]:\n",
    "        \"\"\"Evaluate using NeighborLoader with all neighbors (-1 sampling).\"\"\"\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        all_losses = []\n",
    "        \n",
    "        for batch in loader:\n",
    "            batch = batch.to(self.device)\n",
    "            \n",
    "            if self.cfg.model_type.lower() == \"gat\":\n",
    "                edge_attr = getattr(batch, 'edge_attr', None)\n",
    "                out = self.model(batch.x, batch.edge_index, edge_attr=edge_attr)\n",
    "            else:\n",
    "                out = self.model(batch.x, batch.edge_index)\n",
    "            \n",
    "            if split == \"val\":\n",
    "                mask = batch.val_mask.bool()\n",
    "            else:\n",
    "                mask = batch.test_mask.bool()\n",
    "            \n",
    "            if mask.sum() == 0:\n",
    "                continue\n",
    "            \n",
    "            batch_logits = out[mask]\n",
    "            batch_targets = batch.y[mask].to(torch.long)\n",
    "            \n",
    "            batch_loss = self.criterion(batch_logits, batch_targets)\n",
    "            batch_preds = batch_logits.argmax(dim=1).detach().cpu()\n",
    "            batch_targets_cpu = batch_targets.detach().cpu()\n",
    "            \n",
    "            all_preds.append(batch_preds)\n",
    "            all_targets.append(batch_targets_cpu)\n",
    "            all_losses.append(batch_loss.item())\n",
    "            \n",
    "            # Memory cleanup\n",
    "            del out, batch_logits, batch_targets\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        if not all_preds:\n",
    "            print(f\"Warning: No {split} predictions generated\")\n",
    "            return 0.0, 0.0\n",
    "        \n",
    "        final_preds = torch.cat(all_preds, dim=0)\n",
    "        final_targets = torch.cat(all_targets, dim=0)\n",
    "        avg_loss = sum(all_losses) / len(all_losses)\n",
    "        \n",
    "        acc = accuracy_score(final_targets.numpy(), final_preds.numpy())\n",
    "        self._log({f\"{split}/acc\": acc, f\"{split}/loss\": avg_loss})\n",
    "        \n",
    "        return float(avg_loss), float(acc)\n",
    "\n",
    "    def fit(self) -> None:\n",
    "        self.build_dataset()\n",
    "        train_loader, val_loader, test_loader = self.build_loaders()\n",
    "        self.build_model()\n",
    "\n",
    "        print(self.device)\n",
    "        for epoch in range(self.cfg.epochs):\n",
    "            loss, acc, val_loss, val_acc = self._train_one_epoch(train_loader, val_loader, epoch)\n",
    "            print(f\"Epoch {epoch:03d} | loss={loss:.4f} | train_acc={acc:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc:.4f}\")\n",
    "\n",
    "        test_loss, test_acc = self.evaluate(split=\"test\", loader=test_loader)\n",
    "        print(f\"Final Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "    # Convenience single entry\n",
    "    def run(self) -> None:\n",
    "        self.fit()\n",
    "        if self.wandb_run is not None:\n",
    "            self.wandb_run.finish()\n",
    "\n",
    "# ----------------------------\n",
    "# Script entry\n",
    "# ----------------------------\n",
    "def main():\n",
    "    cfg = Config(\n",
    "        # toggle this on to log to W&B (requires `wandb login`)\n",
    "        use_wandb=True, \n",
    "        seed=42,  # Using default seed for reproducibility\n",
    "        val_batch_size=1024  # Enable batched validation\n",
    "    )\n",
    "    trainer = GNNTrainer(cfg)\n",
    "    trainer.run()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fcca8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SVM sequence vectors from /oscar/data/larschan/shared_data/BindGPS/data/1kb_128_vec_dataset.pt ...\n",
      "SVM vector shape: (137572, 128)\n"
     ]
    }
   ],
   "source": [
    "# train_gnn.py\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Project modules\n",
    "from src.data import DatasetLoader, GraphParamBuilder\n",
    "from src.models import GCN, GATModel\n",
    "\n",
    "# PyG\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "svm_path = \"/oscar/data/larschan/shared_data/BindGPS/data/1kb_128_vec_dataset.pt\"\n",
    "print(f\"Loading SVM sequence vectors from {svm_path} ...\")\n",
    "\n",
    "svm_dataset = torch.load(svm_path, weights_only=False)\n",
    "svm_vecs = svm_dataset.vecs\n",
    "print(f\"SVM vector shape: {svm_vecs.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dd1a1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[110341, 110342, 110343, 110344, 110345, 110346, 110347, 110348, 110349, 110350, 110351, 110352, 110353, 110354, 110355, 110356, 110357, 110358, 110359, 110360]\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "chrM_indices = svm_dataset.metadata.index[svm_dataset.metadata[\"chr\"] == \"chrM\"].tolist()\n",
    "print(chrM_indices)\n",
    "print(len(chrM_indices))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd31b4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_features shape: (137552, 8)\n",
      "Number of NaN rows: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h3k27ac</th>\n",
       "      <th>h3k27me3</th>\n",
       "      <th>h3k36me3</th>\n",
       "      <th>h3k4me1</th>\n",
       "      <th>h3k4me2</th>\n",
       "      <th>h3k4me3</th>\n",
       "      <th>h3k9me3</th>\n",
       "      <th>h4k16ac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   h3k27ac  h3k27me3  h3k36me3  h3k4me1  h3k4me2  h3k4me3  h3k9me3  h4k16ac\n",
       "0      0.0       0.0    0.0000      0.0      0.0      0.0      0.0      0.0\n",
       "1      0.0       0.0    0.0000      0.0      0.0      0.0      0.0      0.0\n",
       "2      0.0       0.0    0.0000      0.0      0.0      0.0      0.0      0.0\n",
       "3      0.0       0.0    0.0000      0.0      0.0      0.0      0.0      0.0\n",
       "4      0.0       0.0   20.0032      0.0      0.0      0.0      0.0      0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from src.data import DatasetLoader\n",
    "\n",
    "# Manually set parameters (same as in Config)\n",
    "base_path = \"/gpfs/data/larschan/shared_data/BindGPS/data/datasets/\"\n",
    "p_value = \"0_1\"\n",
    "resolution = \"1kb\"\n",
    "exclude_features = (\"clamp\", \"gaf\", \"psq\")\n",
    "features_of_interest = (\n",
    "    \"clamp\",\"gaf\",\"psq\",\"h3k27ac\",\"h3k27me3\",\"h3k36me3\",\n",
    "    \"h3k4me1\",\"h3k4me2\",\"h3k4me3\",\"h3k9me3\",\"h4k16ac\"\n",
    ")\n",
    "\n",
    "\n",
    "loader = DatasetLoader(base_path=base_path)\n",
    "node_df, edge_df = loader.load(p_value=p_value, resolution=resolution)\n",
    "\n",
    "\n",
    "feats = [f for f in features_of_interest if f not in exclude_features]\n",
    "\n",
    "\n",
    "input_features = node_df.loc[:, feats].copy()\n",
    "\n",
    "\n",
    "print(\"input_features shape:\", input_features.shape)\n",
    "print(\"Number of NaN rows:\", input_features.isna().any(axis=1).sum())\n",
    "display(input_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe595da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bind_gps_env)",
   "language": "python",
   "name": "bind_gps_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
